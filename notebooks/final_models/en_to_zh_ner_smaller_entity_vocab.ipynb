{"cells":[{"cell_type":"code","source":["pip install huggingface rouge_score bert_score sacrebleu datasets transformers evaluate"],"metadata":{"id":"1QVdkeZ-exoK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733642622162,"user_tz":480,"elapsed":7632,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"0b69e84b-063d-4e2c-f76f-fbddaca9e66a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting huggingface\n","  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n","Collecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n","Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n","Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=bac425ba485ef13af2db5ab847509263d5361150afd522f365057e641872541f\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: huggingface, xxhash, portalocker, fsspec, dill, colorama, sacrebleu, rouge_score, multiprocess, datasets, bert_score, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed bert_score-0.3.13 colorama-0.4.6 datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 huggingface-0.0.1 multiprocess-0.70.16 portalocker-3.0.0 rouge_score-0.1.2 sacrebleu-2.4.3 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount your Google Drive\n","drive.mount('/content/drive')\n","\n","data_dir = \"/content/drive/My Drive/266 Data Project/corpora\""],"metadata":{"id":"_EDs-WNVezGf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733642642551,"user_tz":480,"elapsed":20392,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"78330bc7-5a72-482d-bc70-6a95ccdd9938"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LxX8e9HweXzI"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import transformers\n","import gensim\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","from datasets import Dataset\n","import pandas as pd\n","import os\n","import json\n","from tqdm.autonotebook import trange, tqdm"]},{"cell_type":"code","source":["# import torch\n","# import torch.nn.functional as F\n","# def extract_entities(text, model, tokenizer):\n","#     # Tokenize the input text\n","#     inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n","\n","#     # Perform inference\n","#     with torch.no_grad():\n","#         outputs = model(**inputs).logits\n","\n","#     # Get the predicted token labels\n","#     predictions = torch.argmax(outputs, dim=-1)\n","\n","#     # Convert token IDs back to tokens\n","#     tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n","\n","#     # Get model labels (you can adjust based on the model’s configuration)\n","#     labels = model.config.id2label\n","\n","#     # Extract named entities and their scores\n","#     entities = []\n","#     for token, prediction, logits in zip(tokens, predictions[0], outputs[0]):\n","#         if prediction != 0:  # Assuming '0' is the 'O' tag for non-entities\n","#             # Get the score (probability) for the predicted class\n","#             class_score = F.softmax(logits, dim=-1)[prediction.item()].item()\n","#             entities.append((token, labels[prediction.item()], class_score))\n","\n","#     return entities"],"metadata":{"id":"QM7GrovKlJPm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import AutoTokenizer, AutoModelForTokenClassification\n","# import pandas as pd\n","# import tqdm\n","# import os\n","\n","# # Load the first model: venkatd/BioMed_NER\n","# model1_name = \"venkatd/BioMed_NER\"\n","# tokenizer1 = AutoTokenizer.from_pretrained(model1_name)\n","# model1 = AutoModelForTokenClassification.from_pretrained(model1_name)\n","\n","# # Sample biomedical text in Chinese\n","# df = pd.read_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/nejm_train.parquet\")\n","# texts = df.english.tolist()\n","# # Extract entities\n","# english_entities = []\n","# for text in tqdm.tqdm(texts):\n","#     # print(f\"Text: {text}\")\n","#     entities = extract_entities(text, model1, tokenizer1)\n","#     for i in entities:\n","#         if i[2] >= .20:\n","#             english_entities.append(entities)\n","\n","# df_entities_en = pd.DataFrame({\"entities\": english_entities})\n","# df_entities_en.to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/en-entities.parquet\")"],"metadata":{"id":"tGrbV1b7kLu3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["english_entities = pd.read_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/eng-list.parquet\").entity.unique().tolist()"],"metadata":{"id":"NxyOMzAmpWGH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hoo-SrrUpxsF"},"outputs":[],"source":["class TranslationDataset:\n","    \"\"\"\n","    Prepare tokenized datasets for training and evaluation without relying on DataLoader.\n","    \"\"\"\n","    @staticmethod\n","    def prepare_dataset(english_texts, chinese_texts, tokenizer):\n","        # Tokenize parallel corpus\n","        tokenized_data = {\n","            \"source\": tokenizer(\n","                english_texts,\n","                padding=True,\n","                truncation=True,\n","                max_length=128,\n","                return_tensors='pt'\n","            ),\n","            \"target\": tokenizer(\n","                chinese_texts,\n","                padding=True,\n","                truncation=True,\n","                max_length=128,\n","                return_tensors='pt'\n","            )\n","        }\n","\n","        # Prepare data dictionary for Hugging Face Dataset\n","        dataset_dict = {\n","            \"input_ids\": tokenized_data[\"source\"][\"input_ids\"],\n","            \"attention_mask\": tokenized_data[\"source\"][\"attention_mask\"],\n","            \"labels\": tokenized_data[\"target\"][\"input_ids\"]\n","        }\n","\n","        # Convert to Hugging Face Dataset\n","        return Dataset.from_dict({key: value.tolist() for key, value in dataset_dict.items()})\n","\n","\n","class BiomedicalMarianMTEnhancer(nn.Module):\n","    \"\"\"\n","    Wraps MarianMT with additional medical term embeddings.\n","    \"\"\"\n","    def __init__(self, base_model, tokenizer, biowordvec_path):\n","        super().__init__()\n","        self.base_model = base_model\n","        self.tokenizer = tokenizer\n","\n","        # Load BioWordVec embeddings\n","        self.biowordvec = gensim.models.KeyedVectors.load_word2vec_format(\n","            biowordvec_path,\n","            binary=True\n","        )\n","\n","        # Create a custom embedding layer for medical terms\n","        embedding_dim = self.biowordvec.vector_size\n","        vocab_size = base_model.config.vocab_size\n","\n","        # Create a custom embedding layer\n","        self.medical_embedding_layer = nn.Embedding(\n","            vocab_size,\n","            embedding_dim\n","        )\n","\n","        # Initialize medical embedding layer\n","        self._init_medical_embeddings()\n","\n","        # Additional projection layer to align embeddings\n","        self.projection = nn.Linear(\n","            embedding_dim,\n","            base_model.config.d_model\n","        )\n","\n","    def _init_medical_embeddings(self):\n","        weight = self.medical_embedding_layer.weight.data\n","\n","        for token, idx in self.tokenizer.get_vocab().items():\n","            clean_token = token.replace('▁', '').strip()\n","\n","            try:\n","                # Try to get embedding for the token\n","                vec = self.biowordvec[clean_token]\n","                weight[idx] = torch.tensor(vec)\n","            except KeyError:\n","                # Fallback to default initialization\n","                nn.init.xavier_uniform_(weight[idx].unsqueeze(0))\n","\n","    def forward(self, input_ids, labels=None, attention_mask=None):\n","        # Get base model embeddings\n","        base_embeddings = self.base_model.model.get_input_embeddings()(input_ids)\n","\n","        # Get medical term embeddings\n","        medical_embeddings = self.medical_embedding_layer(input_ids)\n","\n","        # Project medical embeddings\n","        projected_medical_embeddings = self.projection(medical_embeddings)\n","\n","        # Combine base and medical embeddings\n","        combined_embeddings = base_embeddings + projected_medical_embeddings\n","\n","        # Continue with standard MarianMT forward pass\n","        outputs = self.base_model(\n","            inputs_embeds=combined_embeddings,\n","            attention_mask=attention_mask,\n","            labels=labels\n","        )\n","\n","        return outputs\n","\n","    def generate(self, input_ids=None, attention_mask=None, **kwargs):\n","        \"\"\"\n","        Generate translations with custom embeddings and pass them into MarianMT method as input_embeddings\n","        \"\"\"\n","        if input_ids is not None:\n","            # Compute the base embeddings\n","            base_embeddings = self.base_model.model.get_input_embeddings()(input_ids)\n","\n","            # Compute the medical term embeddings\n","            medical_embeddings = self.medical_embedding_layer(input_ids)\n","\n","            # Project medical embeddings\n","            projected_medical_embeddings = self.projection(medical_embeddings)\n","\n","            # Combine base and medical embeddings\n","            combined_embeddings = base_embeddings + projected_medical_embeddings\n","\n","            # Use the combined embeddings for generation\n","            return self.base_model.generate(\n","                inputs_embeds=combined_embeddings,\n","                attention_mask=attention_mask,\n","                **kwargs\n","            )\n","        else:\n","            raise ValueError(\"`input_ids` must be provided for generating embeddings.\")\n","\n","    def save_custom(self, save_directory, tokenizer=None):\n","        \"\"\"\n","        Save the model and custom embeddings.\n","        \"\"\"\n","        os.makedirs(save_directory, exist_ok=True)\n","\n","        # Paths\n","        model_save_path = os.path.join(save_directory, \"model\")\n","        embedding_save_path = os.path.join(model_save_path, \"medical_embeddings.pth\")\n","        projection_save_path = os.path.join(model_save_path, \"projection_layer.pth\")\n","        custom_config_path = os.path.join(model_save_path, \"custom_config.json\")\n","        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n","\n","        os.makedirs(model_save_path, exist_ok=True)\n","\n","        # Save the base model\n","        self.base_model.save_pretrained(model_save_path)\n","\n","        # Save the medical embedding and projection layer\n","        torch.save(self.medical_embedding_layer.state_dict(), embedding_save_path)\n","        torch.save(self.projection.state_dict(), projection_save_path)\n","\n","        # Save custom configuration\n","        custom_config = {\n","            \"embedding_dim\": self.medical_embedding_layer.embedding_dim,\n","            \"vocab_size\": self.medical_embedding_layer.num_embeddings\n","        }\n","        with open(custom_config_path, \"w\") as f:\n","            json.dump(custom_config, f)\n","\n","        # Save tokenizer\n","        if tokenizer is not None:\n","            tokenizer.save_pretrained(tokenizer_save_path)\n","\n","    def from_custom(cls, save_directory):\n","        \"\"\"\n","        Load the model and custom embeddings.\n","        \"\"\"\n","        # Paths\n","        model_save_path = os.path.join(save_directory, \"model\")\n","        embedding_save_path = os.path.join(model_save_path, \"medical_embeddings.pth\")\n","        projection_save_path = os.path.join(model_save_path, \"projection_layer.pth\")\n","        custom_config_path = os.path.join(model_save_path, \"custom_config.json\")\n","        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n","\n","        # Load the base model\n","        base_model = transformers.MarianMTModel.from_pretrained(model_save_path)\n","\n","        # Load custom configuration\n","        with open(custom_config_path, \"r\") as f:\n","            custom_config = json.load(f)\n","\n","        # Extract custom configuration values\n","        embedding_dim = custom_config.get(\"embedding_dim\")\n","        vocab_size = custom_config.get(\"vocab_size\")\n","\n","        # Create an instance of the enhanced model\n","        enhancer = cls(\n","            base_model=base_model,\n","            tokenizer=None,  # Replace with tokenizer if required\n","            biowordvec_path=None  # BioWordVec is not reloaded here\n","        )\n","\n","        # Resize and initialize the medical embedding layer based on the saved config\n","        enhancer.medical_embedding_layer = nn.Embedding(\n","            num_embeddings=vocab_size,\n","            embedding_dim=embedding_dim\n","        )\n","\n","        # Load the medical embedding and projection layer states\n","        medical_embedding_state = torch.load(embedding_save_path)\n","        projection_state = torch.load(projection_save_path)\n","        enhancer.medical_embedding_layer.load_state_dict(medical_embedding_state)\n","        enhancer.projection.load_state_dict(projection_state)\n","\n","        # Load tokenizer\n","        tokenizer = transformers.MarianTokenizer.from_pretrained(tokenizer_save_path)\n","\n","        return enhancer, tokenizer\n","\n","\n","\n","def train_biomedical_translation_model(\n","    base_model,\n","    tokenizer,\n","    english_texts,\n","    chinese_texts,\n","    biowordvec_path,\n","    test_size=0.1,\n","    batch_size=16,\n","    learning_rate=1e-4,\n","    num_train_epochs=3,\n","    output_dir=\"./results\"\n","):\n","    # Prepare datasets\n","    full_dataset = TranslationDataset.prepare_dataset(english_texts, chinese_texts, tokenizer)\n","    split_dataset = full_dataset.train_test_split(test_size=test_size, seed=42)\n","\n","    # Wrap the base model with the enhancer\n","    enhanced_model = BiomedicalMarianMTEnhancer(\n","        base_model,\n","        tokenizer,\n","        biowordvec_path\n","    )\n","\n","    # Define Seq2Seq training arguments\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        evaluation_strategy=\"epoch\",\n","        save_strategy=\"epoch\",\n","        learning_rate=learning_rate,\n","        per_device_train_batch_size=batch_size,\n","        per_device_eval_batch_size=batch_size,\n","        weight_decay=0.01,\n","        save_safetensors=False,\n","        num_train_epochs=num_train_epochs,\n","        logging_dir=\"./logs\",\n","        logging_steps=500,\n","        predict_with_generate=True,  # This is essential for seq2seq tasks like translation\n","        generation_num_beams=3,  # Beam search during generation\n","        # load_best_model_at_end=True\n","    )\n","\n","    # Initialize Seq2SeqTrainer\n","    trainer = Seq2SeqTrainer(\n","        model=enhanced_model,\n","        args=training_args,\n","        train_dataset=split_dataset[\"train\"],\n","        eval_dataset=split_dataset[\"test\"],\n","        tokenizer=tokenizer\n","    )\n","\n","    # Train the model\n","    trainer.train()\n","\n","    return enhanced_model\n"]},{"cell_type":"code","source":["from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n","from tokenizers import normalizers\n","from tokenizers.processors import TemplateProcessing\n","\n","# Example new vocabulary (replace this with your actual vocabulary)\n","new_vocab = english_entities\n","\n","# Example of training data: list of sentences (replace with your dataset)\n","dataset = pd.read_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/nejm_train.parquet\")\n","english_texts = dataset[\"english\"].tolist()\n","corpus = english_texts\n","\n","# Initialize a tokenizer model (e.g., BPE or WordPiece)\n","tokenizer = transformers.MarianTokenizer.from_pretrained(model_name)\n","\n","# Set normalizer to handle case sensitivity (you can modify as needed)\n","tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.StripAccents()])\n","\n","# Pre-tokenizer to split text into words\n","tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n","\n","# Add new vocabulary to tokenizer\n","# Make sure to add <unk> for unknown tokens and any special tokens you want\n","tokenizer.add_special_tokens(new_vocab)\n","\n","# Initialize a trainer for the tokenizer\n","trainer = trainers.BpeTrainer(\n","    vocab_size=65001+5000,  # Set a desired vocab size\n",")\n","\n","# Train the tokenizer using the corpus (you may also use a large dataset here)\n","tokenizer.train_from_iterator(corpus, trainer=trainer)\n","\n","# Save the trained tokenizer\n","tokenizer.save(\"my_tokenizer.json\")\n","\n","# Load and test the tokenizer\n","tokenizer = Tokenizer.from_file(\"my_tokenizer.json\")\n","\n","# Example sentence to tokenize\n","sentence = \"COVID is transforming the field of biomedicine.\"\n","encoded = tokenizer.encode(sentence)\n","print(\"Encoded sentence:\", encoded.tokens)\n","\n","# Decode it back to verify\n","decoded = tokenizer.decode(encoded.ids)\n","print(\"Decoded sentence:\", decoded)\n"],"metadata":{"id":"T5JGkynNrZQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p3SWie5geXzL","outputId":"0bc23e56-a3ec-48f5-b567-440509d406dc","colab":{"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"status":"ok","timestamp":1733644871162,"user_tz":480,"elapsed":664106,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n","The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-9-376d1be46fcc>:256: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["Tracking run with wandb version 0.18.7"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20241208_073547-afs5i4lm</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/afs5i4lm' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/afs5i4lm' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface/runs/afs5i4lm</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='4332' max='10485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 4332/10485 14:16 < 20:17, 5.06 it/s, Epoch 1.24/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.856700</td>\n","      <td>0.824842</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='10485' max='10485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10485/10485 25:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.856700</td>\n","      <td>0.824842</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.496400</td>\n","      <td>0.459455</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.401300</td>\n","      <td>0.405166</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}],"source":["# Main execution\n","# Load pretrained MarianMT model\n","model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n","base_model = transformers.MarianMTModel.from_pretrained(model_name)\n","\n","# Add entities as vocab\n","\n","tokenizer.add_special_tokens({\n","        'additional_special_tokens': list(set(english_entities))\n","    })\n","base_model.resize_token_embeddings(len(tokenizer))\n","\n","\n","# Load your parallel corpus\n","dataset = pd.read_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/nejm_train.parquet\")\n","english_texts = dataset[\"english\"].tolist()\n","chinese_texts = dataset[\"chinese\"].tolist()\n","\n","# Train the biomedical translation model\n","enhanced_model = train_biomedical_translation_model(\n","    base_model,\n","    tokenizer,\n","    english_texts,\n","    chinese_texts,\n","    biowordvec_path='/content/drive/MyDrive/266 Data Project/corpora/nejm/BioWordVec_PubMed_MIMICIII_d200.vec.bin',\n","    num_train_epochs=3\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kvFjYQwaeXzL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733644895305,"user_tz":480,"elapsed":24147,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"eae1f24a-6292-4e47-be29-a831cad9c8a6"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]}],"source":["save_dir = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/word-vec-model-ner\"\n","enhanced_model.save_custom(save_dir, tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5fS1Yk4GpnT_"},"outputs":[],"source":["from evaluate import load\n","\n","def evaluate_model_metrics(predictions, references, save_path=None):\n","    # Load the evaluation metrics\n","    bleu_metric = load(\"bleu\")\n","    rouge_metric = load(\"rouge\")\n","    bertscore_metric = load(\"bertscore\")\n","    ter_metric = load(\"ter\")\n","\n","    # Format references for metric calculation\n","    references = [[ref] for ref in references]\n","\n","    # Evaluate BLEU score\n","    bleu_result = bleu_metric.compute(predictions=predictions, references=references)\n","\n","    # Evaluate ROUGE score\n","    rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n","\n","    # Evaluate BERTScore\n","    bertscore_result = bertscore_metric.compute(predictions=predictions, references=references, lang=\"en\")\n","\n","    # Evaluate TER (Translation Edit Rate)\n","    ter_result = ter_metric.compute(predictions=predictions, references=references)\n","\n","    # Extract summary statistics for BERTScore\n","    bertscore_summary = {\n","        \"mean\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]),\n","        \"median\": sorted(bertscore_result[\"f1\"])[len(bertscore_result[\"f1\"]) // 2],\n","        \"std\": (sum((x - sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]))**2 for x in bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]))**0.5\n","    }\n","\n","    # Consolidate results\n","    results = {\n","        \"BLEU\": bleu_result,\n","        \"ROUGE\": rouge_result,\n","        \"BERTScore\": bertscore_summary,\n","        \"TER\": ter_result,\n","    }\n","\n","    return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dt8Qn-JYeXzM"},"outputs":[],"source":["class BiomedicalTranslationEvaluator:\n","    \"\"\"\n","    Evaluate the performance of a biomedical translation model.\n","    \"\"\"\n","    def __init__(self, model, tokenizer, device=None):\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        # Move model to the specified device\n","        self.model.to(self.device)\n","\n","    def prepare_dataset(self, english_texts, chinese_texts, max_length=512):\n","        \"\"\"\n","        Prepare a dataset for evaluation.\n","        \"\"\"\n","        # Tokenize source (English) texts\n","        source_encodings = self.tokenizer(\n","            english_texts,\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Tokenize target (Chinese) texts for comparison (optional)\n","        target_encodings = self.tokenizer(\n","            chinese_texts,\n","            padding=True,\n","            truncation=True,\n","            max_length=max_length,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Move all tensors to the appropriate device\n","        return {\n","            \"source_input_ids\": source_encodings[\"input_ids\"].to(self.device),\n","            \"source_attention_mask\": source_encodings[\"attention_mask\"].to(self.device),\n","            \"target_input_ids\": target_encodings[\"input_ids\"].to(self.device)\n","        }\n","\n","    def generate_translations(self, dataset, batch_size=16):\n","        translations = []\n","        for i in trange(0, len(dataset[\"source_input_ids\"]), batch_size):\n","            batch_input_ids = dataset[\"source_input_ids\"][i:i + batch_size].to(self.device)\n","            batch_attention_mask = dataset[\"source_attention_mask\"][i:i + batch_size].to(self.device)\n","\n","            # Generate translations for the batch\n","            outputs = self.model.generate(\n","                input_ids=batch_input_ids,\n","                attention_mask=batch_attention_mask,\n","                num_beams=3,\n","                max_length=128,  # Adjust if needed\n","            )\n","            translations.extend(self.tokenizer.batch_decode(outputs, skip_special_tokens=True))\n","\n","        return translations\n","\n","\n","    def run_evaluation(self, english_texts, chinese_texts):\n","        \"\"\"\n","        Run the evaluation process.\n","        \"\"\"\n","        # Prepare dataset\n","        dataset = self.prepare_dataset(english_texts, chinese_texts)\n","\n","        # Generate translations\n","        translations = self.generate_translations(dataset)\n","\n","        # Decode target inputs for human-readable comparison\n","        # target_texts = self.tokenizer.batch_decode(\n","        #     dataset[\"target_input_ids\"].to(\"cpu\"), skip_special_tokens=True\n","        # )\n","\n","        return {\n","            \"translations\": translations,\n","            \"targets\": chinese_texts\n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YZJLXpSeXzN","colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["853ed081a11b415da7af9139ddc30bab","6de946be53364c8f808c28a1c69c772d","9ad74ca2881d4fdfbfcbf81d0c3d8584","8cc3186cbda04139b435b1344574f12d","ae8822469d5243e6ae5e77b0e50f6c73","fe94a5d1a5c5413fab9b690f7966d948","019889e785264c16a06b48a2e3920f15","779bef96323846768227556d2662c262","f1a320ebb90344ab8d50018f2b21152e","8d12e685f92c48d98aa2d57039ac070b","cfeaf470a5654cd4a2bc1003d549903e"]},"executionInfo":{"status":"ok","timestamp":1733649223342,"user_tz":480,"elapsed":4328036,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"c3028fe0-7bdb-4d41-f3f0-582733016c62"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/132 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"853ed081a11b415da7af9139ddc30bab"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Translations:\n","['是 一种    它 在', '和   的', '目前 尚 不 知晓     患者 的  和', '在 这项 为期 年 的 剂量  研究 中 我们 纳入 了 例 慢性  患者 和 例    患者 这些 患者 有 至少  之前 接受 过     治疗 (   ) 的  或 不可 接受 的', '主要 目的 是 确定 最大  剂量 或 推荐 的  剂量 ( 或 两者 )']\n","\n","Targets:\n","['asciminib 是 与 BCR - ABL1 蛋白 的 豆蔻 酰 位点 相结合 的 别构抑制 剂 , 它 可 通过 不同于 所有 其他 ABL 激酶 抑制剂 的 机制 将 BCR - ABL1 锁定 在 非 活性 构象 .', 'asciminib 同时 靶向 作用 于 天然 和 突变 的 BCR - ABL1 , 包括 看门 基因 ( gatekeeper ) T315I 突变体 .', 'asciminib 用于 费城 染色体 阳性 白血病 患者 的 安全性 和 抗 白血病 活性 尚未 明确 .', '在 这项 1 期 剂量 递增 研究 中 , 我们 纳入 了 141 例 慢性期 和 9 例 加速 期 慢性 髓系 白血病 ( CML ) 患者 , 这些 患者 既往 对 至少 两种 ATP 竞争性 酪氨酸 激酶 抑制剂 ( TKI ) 耐药 或 发生 不可 接受 的 副作用 .', '本 试验 的 主要 目的 是 确定 asciminib 的 最大 耐受 剂量 或 推荐 剂量 ( 或 这 两者 ) .']\n"]}],"source":["# Initialize the evaluator\n","evaluator = BiomedicalTranslationEvaluator(\n","    enhanced_model,\n","    tokenizer\n",")\n","\n","test_dataset = pd.read_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/nejm_test.parquet\")\n","english_test_texts = test_dataset[\"english\"].tolist()\n","chinese_test_texts = test_dataset[\"chinese\"].tolist()\n","\n","torch.cuda.empty_cache()\n","# Run evaluation\n","results = evaluator.run_evaluation(\n","    english_test_texts,  # List of English sentences\n","    chinese_test_texts   # List of Chinese reference translations\n",")\n","\n","# Print results\n","print(\"Translations:\")\n","print(results[\"translations\"][0:5])\n","print(\"\\nTargets:\")\n","print(results[\"targets\"][0:5])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dl3-0nNNeXzO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733649461754,"user_tz":480,"elapsed":49635,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"e9a9568f-94db-4610-9422-36b753ff679b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'BLEU': {'bleu': 0.08483225657980646,\n","  'precisions': [0.6239937748202211,\n","   0.24910455398260276,\n","   0.11569124841456786,\n","   0.0553249010331177],\n","  'brevity_penalty': 0.4776560140919169,\n","  'length_ratio': 0.5750879575334856,\n","  'translation_length': 37268,\n","  'reference_length': 64804},\n"," 'ROUGE': {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0, 'rougeLsum': 0.0},\n"," 'BERTScore': {'mean': 0.8898050120396347,\n","  'median': 0.8951306939125061,\n","  'std': 0.07368598900462432},\n"," 'TER': {'score': 70.41777084957131,\n","  'num_edits': 45173,\n","  'ref_length': 64150.0}}"]},"metadata":{},"execution_count":16}],"source":["evaluate_model_metrics(results[\"translations\"], results[\"targets\"])"]},{"cell_type":"code","source":[],"metadata":{"id":"x3HEA1fq7wbt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame({\"predicted_chinese\": results[\"translations\"], \"chinese\": results[\"targets\"]}).to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/en_to_zh_cleaned_embeddings_output.parquet\")"],"metadata":{"id":"NtNkHnksKk0y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vFp44XDsjta6"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[{"file_id":"1-ojEjj1bNOO2UuZzh_JtFKWE1MS6UV5U","timestamp":1733641763939},{"file_id":"1jM71vv6FHV7LmmYOdxnzgFJU8qBjpupu","timestamp":1733598841097}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"853ed081a11b415da7af9139ddc30bab":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6de946be53364c8f808c28a1c69c772d","IPY_MODEL_9ad74ca2881d4fdfbfcbf81d0c3d8584","IPY_MODEL_8cc3186cbda04139b435b1344574f12d"],"layout":"IPY_MODEL_ae8822469d5243e6ae5e77b0e50f6c73"}},"6de946be53364c8f808c28a1c69c772d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe94a5d1a5c5413fab9b690f7966d948","placeholder":"​","style":"IPY_MODEL_019889e785264c16a06b48a2e3920f15","value":"100%"}},"9ad74ca2881d4fdfbfcbf81d0c3d8584":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_779bef96323846768227556d2662c262","max":132,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1a320ebb90344ab8d50018f2b21152e","value":132}},"8cc3186cbda04139b435b1344574f12d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8d12e685f92c48d98aa2d57039ac070b","placeholder":"​","style":"IPY_MODEL_cfeaf470a5654cd4a2bc1003d549903e","value":" 132/132 [1:12:05&lt;00:00, 25.98s/it]"}},"ae8822469d5243e6ae5e77b0e50f6c73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe94a5d1a5c5413fab9b690f7966d948":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"019889e785264c16a06b48a2e3920f15":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"779bef96323846768227556d2662c262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1a320ebb90344ab8d50018f2b21152e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8d12e685f92c48d98aa2d57039ac070b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfeaf470a5654cd4a2bc1003d549903e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}