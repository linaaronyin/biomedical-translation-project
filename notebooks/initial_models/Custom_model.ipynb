{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"65560451e77148ab9cb75c56b991dbb7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_23053a02a446455b9af116b237324e19","IPY_MODEL_2d6a4fd0c3fc440f87fa14c0b1803183","IPY_MODEL_9dae01c9360a450dacc6382f8e063e78"],"layout":"IPY_MODEL_698768444c6949c49d9acb2dda3ca236"}},"23053a02a446455b9af116b237324e19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff1a2157044e4d7d834319f46b6a4814","placeholder":"​","style":"IPY_MODEL_241df10426d741f19a5aaf4451ea02b9","value":"Generating train split: "}},"2d6a4fd0c3fc440f87fa14c0b1803183":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6086b926eb7e459580063ae87272eea2","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec0c0482beb048899a5b9367408825c4","value":1}},"9dae01c9360a450dacc6382f8e063e78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83a8aae9de5c43bbad2dc8ff6daa26f0","placeholder":"​","style":"IPY_MODEL_22973aaecd4f468a93203366c4515318","value":" 62127/0 [00:01&lt;00:00, 52370.63 examples/s]"}},"698768444c6949c49d9acb2dda3ca236":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff1a2157044e4d7d834319f46b6a4814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"241df10426d741f19a5aaf4451ea02b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6086b926eb7e459580063ae87272eea2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"ec0c0482beb048899a5b9367408825c4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83a8aae9de5c43bbad2dc8ff6daa26f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22973aaecd4f468a93203366c4515318":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"94daa39f3828453b832bf48a268713c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38d6cdecb6f34b44aa6155f1f313b1fd","IPY_MODEL_445ee10a5b6d416797caa0a7645a2075","IPY_MODEL_6d108856e0db4eadad57a3347768fb97"],"layout":"IPY_MODEL_9b413611ac2f468eb64869857208f196"}},"38d6cdecb6f34b44aa6155f1f313b1fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5f6d2f2905654757984387b466117ff4","placeholder":"​","style":"IPY_MODEL_efaefb54f481411b8aa8cd2dbb0330f5","value":"Map: 100%"}},"445ee10a5b6d416797caa0a7645a2075":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a59582dc14a1464b8115fdf68756ff00","max":2102,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6db16fdb2d494a8d9cf3c8a408bd4bf7","value":2102}},"6d108856e0db4eadad57a3347768fb97":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1debe358dd964e8388f5a135924232c5","placeholder":"​","style":"IPY_MODEL_fde160c61af4465e90923cfd265081c5","value":" 2102/2102 [12:24&lt;00:00,  2.83 examples/s]"}},"9b413611ac2f468eb64869857208f196":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f6d2f2905654757984387b466117ff4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"efaefb54f481411b8aa8cd2dbb0330f5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a59582dc14a1464b8115fdf68756ff00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6db16fdb2d494a8d9cf3c8a408bd4bf7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1debe358dd964e8388f5a135924232c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fde160c61af4465e90923cfd265081c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpfUJY42OHfc","executionInfo":{"status":"ok","timestamp":1733459753345,"user_tz":480,"elapsed":7320,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"8f63b35d-cdc9-48fb-9274-df8c2c5317ee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting huggingface\n","  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n","Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n","Installing collected packages: huggingface\n","Successfully installed huggingface-0.0.1\n"]}],"source":["pip install huggingface"]},{"cell_type":"code","source":["pip install datasets transformers evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPNcQ2SBQYFp","executionInfo":{"status":"ok","timestamp":1733459836135,"user_tz":480,"elapsed":9998,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"66931b8d-0d81-451c-c050-5c903557c019"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount your Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the data directory inside your Google Drive\n","# data_dir = \"/content/drive/My Drive/Colab Notebooks/corpora\"\n","data_dir = \"/content/drive/My Drive/266 Data Project/corpora\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zdAKxG1Qwyf","executionInfo":{"status":"ok","timestamp":1733459946866,"user_tz":480,"elapsed":39070,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"33fee2cf-08fd-49c6-ecf5-9482176e1d6d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n","import pandas as pd\n","from datasets import Dataset, load_dataset\n","import torch.nn.functional as F\n","import os\n","import json\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\n","\n","def load_marian_with_biomedical_layer(model_name, hidden_size, special_tokens):\n","    # Load tokenizer and add special tokens\n","    tokenizer = MarianTokenizer.from_pretrained(model_name)\n","    tokenizer.add_special_tokens({\n","        'additional_special_tokens': list(set(special_tokens))\n","    })\n","\n","    # Load base model\n","    model = MarianMTModel.from_pretrained(model_name)\n","\n","    # Create custom model, CustomMarianMTModel will create a BiomedicalEncoder object in init()\n","    custom_model = CustomMarianMTModel(\n","        config=model.config,\n","        hidden_size=hidden_size,\n","        special_token_size=len(special_tokens),\n","    )\n","\n","\n","    # Resize token embeddings\n","    custom_model.resize_token_embeddings(len(tokenizer))\n","\n","    return tokenizer, custom_model\n","\n","class BiomedicalEncoder(nn.Module):\n","    def __init__(self, hidden_size, special_token_size):\n","        super(BiomedicalEncoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.special_token_size = special_token_size\n","\n","        # Adjust the linear layer to match input dimensions\n","        self.linear = nn.Linear(special_token_size, hidden_size)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, entity_embeddings):\n","        # Reshape entity embeddings if necessary\n","        original_shape = entity_embeddings.shape\n","\n","        # Flatten the tensor if it has more than 2 dimensions\n","        if len(original_shape) > 2:\n","            entity_embeddings = entity_embeddings.view(-1, original_shape[-1])\n","\n","        # Ensure the input matches the expected dimension\n","        if entity_embeddings.size(1) != self.special_token_size:\n","            # If the input doesn't match, pad or truncate\n","            if entity_embeddings.size(1) < self.special_token_size:\n","                # Pad with zeros\n","                padding = torch.zeros(\n","                    entity_embeddings.size(0),\n","                    self.special_token_size - entity_embeddings.size(1),\n","                    device=entity_embeddings.device\n","                )\n","                entity_embeddings = torch.cat([entity_embeddings, padding], dim=1)\n","            else:\n","                # Truncate\n","                entity_embeddings = entity_embeddings[:, :self.special_token_size]\n","\n","        # Apply linear transformation and activation\n","        encoded = self.linear(entity_embeddings)\n","        return self.activation(encoded)\n","\n","class CustomMarianMTModel(MarianMTModel):\n","    def __init__(self, config, hidden_size=512, special_token_size=206573, biomedicalEncoder=None):\n","        super().__init__(config)\n","        self.hidden_size = hidden_size\n","        self.special_token_size = special_token_size\n","\n","        # Initialize biomedical encoder within the model\n","        if biomedicalEncoder == None:\n","            self.biomedical_encoder = BiomedicalEncoder(hidden_size, special_token_size)\n","        else:\n","            self.biomedical_encoder = biomedicalEncoder\n","\n","        # Entity embedding for special tokens\n","        self.entity_embedding = nn.Embedding(special_token_size + 1, hidden_size)  # +1 for padding token\n","\n","        # Projection layer to match vocabulary size\n","        self.entity_projection = nn.Linear(hidden_size, config.vocab_size)\n","\n","    def save_custom(self, save_directory):\n","        # Create save directory if it doesn't exist\n","        os.makedirs(save_directory, exist_ok=True)\n","\n","        model_save_path = os.path.join(save_directory, \"model\")\n","        print(model_save_path)\n","        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n","\n","        os.makedirs(model_save_path, exist_ok=True)\n","        os.makedirs(tokenizer_save_path, exist_ok=True)\n","\n","        # Save the model and its configuration\n","        self.save_pretrained(model_save_path)\n","\n","        # Save the biomedical encoder's state_dict\n","        torch.save(self.biomedical_encoder.state_dict(), os.path.join(model_save_path, \"biomedical_encoder.pth\"))\n","\n","        # Save custom attributes in a JSON file\n","        custom_config = {\n","            \"hidden_size\": self.hidden_size,\n","            \"special_token_size\": self.special_token_size,\n","        }\n","        with open(os.path.join(model_save_path, \"custom_config.json\"), \"w\") as f:\n","            json.dump(custom_config, f)\n","\n","        if tokenizer is not None:\n","            tokenizer.save_pretrained(tokenizer_save_path)\n","\n","    @classmethod\n","    def from_custom(cls, save_directory):\n","        model_save_path = os.path.join(save_directory, \"model\")\n","        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n","\n","        # Load custom attributes from JSON\n","        custom_config_path = os.path.join(model_save_path, \"custom_config.json\")\n","        with open(custom_config_path, \"r\") as f:\n","            custom_config = json.load(f)\n","\n","        # Load base model configuration\n","        model = MarianMTModel.from_pretrained(model_save_path)\n","\n","        # Create a new CustomMarianMTModel with the loaded configuration\n","        new_model = cls(\n","            config=model.config,\n","            hidden_size=custom_config[\"hidden_size\"],\n","            special_token_size=custom_config[\"special_token_size\"]\n","        )\n","\n","        # Load the biomedical encoder state dict\n","        biomedical_encoder_path = os.path.join(model_save_path, \"biomedical_encoder.pth\")\n","        biomedical_encoder_state_dict = torch.load(biomedical_encoder_path)\n","        new_model.biomedical_encoder.load_state_dict(biomedical_encoder_state_dict)\n","\n","        # Load the main model weights\n","        state_dict = model.state_dict()\n","        new_model_state_dict = new_model.state_dict()\n","\n","        # Update the state dictionary, keeping the biomedical encoder weights\n","        for key, value in state_dict.items():\n","            if key in new_model_state_dict:\n","                new_model_state_dict[key] = value\n","\n","        new_model.load_state_dict(new_model_state_dict, strict=False)\n","\n","        # Load tokenizer\n","        tokenizer = MarianTokenizer.from_pretrained(tokenizer_save_path)\n","\n","        return new_model, tokenizer\n","\n","    def forward(self, input_ids=None, attention_mask=None, labels=None, entity_ids=None, **kwargs):\n","        # Perform base MarianMT forward pass\n","        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n","\n","        # Process entity information if provided\n","        if entity_ids is not None:\n","            try:\n","                # Ensure entity_ids is a tensor with 2 dimensions\n","                if len(entity_ids.shape) == 1:\n","                    entity_ids = entity_ids.unsqueeze(0)\n","\n","                # Get batch size, sequence length, and vocab size from outputs\n","                batch_size = outputs.logits.size(0)\n","                sequence_length = outputs.logits.size(1)\n","                vocab_size = outputs.logits.size(2)\n","\n","                # Ensure entity_ids is on the same device as outputs.logits\n","                entity_ids = entity_ids.to(outputs.logits.device)\n","\n","                # Limit entity_ids to current batch size\n","                entity_ids = entity_ids[:batch_size]\n","\n","                if torch.any(entity_ids >= self.entity_embedding.num_embeddings):\n","                    print(f\"Invalid entity IDs detected: {entity_ids}\")\n","                    raise ValueError(\"Entity IDs are out of bounds for the embedding layer\")\n","\n","\n","                # Get embeddings for entity special tokens\n","                entity_embeddings = self.entity_embedding(entity_ids)\n","\n","                # Ensure embeddings are on the correct device\n","                entity_embeddings = entity_embeddings.to(outputs.logits.device)\n","\n","                # Process through biomedical encoder\n","                original_shape = entity_embeddings.shape\n","                entity_features = self.biomedical_encoder(entity_embeddings.view(-1, original_shape[-1]))\n","\n","                # Reshape back to original batch and entity dimension\n","                entity_features = entity_features.view(original_shape[0], original_shape[1], -1)\n","\n","                # Project entity features to match logits dimensionality\n","                entity_logits = self.entity_projection(entity_features)\n","\n","                # Ensure logits are on the correct device\n","                entity_logits = entity_logits.to(outputs.logits.device)\n","\n","                # Create a tensor of zeros with the same shape as outputs.logits\n","                expanded_entity_logits = torch.zeros_like(outputs.logits)\n","\n","                # Adjust logits shape to match the entity length\n","                min_entities = min(entity_logits.size(1), expanded_entity_logits.size(1))\n","                min_vocab = min(entity_logits.size(2), expanded_entity_logits.size(2))\n","\n","                expanded_entity_logits[:, :min_entities, :min_vocab] = entity_logits[:, :min_entities, :min_vocab]\n","\n","                # Add entity-based logits to original logits\n","                outputs.logits = outputs.logits + expanded_entity_logits\n","\n","            except Exception as e:\n","                print(f\"Error in forward method: {e}\")\n","                raise\n","        torch.cuda.synchronize()\n","        return outputs\n","\n","\n","\n","def prepare_dataset(dataset, tokenizer, src_lang=\"chinese\", tgt_lang=\"english\", max_entities=5):\n","    def preprocess_function(examples):\n","        # Ensure inputs are lists\n","        src_sentences = examples[src_lang]\n","        tgt_sentences = examples[tgt_lang]\n","        entities_list = examples.get(\"entities\", [[] for _ in src_sentences])\n","\n","        processed_src = []\n","        processed_entities = []\n","\n","        for sentence, entities in zip(src_sentences, entities_list):\n","            # Ensure sentence is a string and remove existing spaces\n","            sentence = str(sentence).replace(\" \", \"\")\n","\n","            # Add special tokens for entities\n","            for entity in entities:\n","                sentence = sentence.replace(entity, f\"<<{entity}>>\")\n","\n","            processed_src.append(sentence)\n","\n","            # Convert entities to token IDs\n","            entity_ids = [\n","                tokenizer.convert_tokens_to_ids(f\"<<{entity}>>\")\n","                for entity in entities\n","            ]\n","\n","            # Pad or truncate entity_ids\n","            entity_ids = entity_ids[:max_entities]\n","            entity_ids += [0] * (max_entities - len(entity_ids))\n","\n","            # Debugging: Log entity ids and padding\n","            # print(f\"Entity IDs (after padding/truncation): {entity_ids}\")\n","\n","            processed_entities.append(entity_ids)\n","\n","        # Tokenize source sentences\n","        model_inputs = tokenizer(\n","            processed_src,\n","            max_length=128,\n","            truncation=True,\n","            padding=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Tokenize target sentences\n","        labels = tokenizer(\n","            tgt_sentences,\n","            max_length=128,\n","            truncation=True,\n","            padding=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Add labels to model inputs\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","\n","        # Convert entity_ids to tensor\n","        model_inputs[\"entity_ids\"] = torch.tensor(processed_entities, dtype=torch.long)\n","\n","        return model_inputs\n","\n","    # Apply preprocessing to the dataset\n","    processed_dataset = dataset.map(\n","        preprocess_function,\n","        batched=True,\n","        remove_columns=dataset.column_names\n","    )\n","\n","    return processed_dataset\n","\n","def fine_tune_custom_model(custom_model, tokenizer, tokenized_dataset, output_dir):\n","    # Split dataset\n","    dataset = tokenized_dataset.train_test_split(test_size=0.1)\n","\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        evaluation_strategy=\"epoch\",\n","        learning_rate=5e-5,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=3,\n","        save_strategy=\"epoch\",\n","        save_safetensors=False,\n","        logging_dir=f\"{output_dir}/logs\",\n","        logging_steps=100,\n","        predict_with_generate=True,\n","        push_to_hub=False,\n","        fp16=False  # Disable mixed precision\n","    )\n","\n","    # Create trainer\n","    trainer = Seq2SeqTrainer(\n","        model=custom_model,\n","        args=training_args,\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"test\"],\n","        tokenizer=tokenizer,\n","    )\n","\n","    # Start training\n","    trainer.train()\n"],"metadata":{"id":"NWQmaXceO-HR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_save_path = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training\"\n","# Loading custom model\n","custom_model, tokenizer = CustomMarianMTModel.from_custom(model_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lvM1GxwYPinv","executionInfo":{"status":"ok","timestamp":1733460064919,"user_tz":480,"elapsed":63884,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"a0103ae1-cc31-4d75-b2ba-e00d42319e3b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-4-122b811ad29c>:140: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  biomedical_encoder_state_dict = torch.load(biomedical_encoder_path)\n","/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]}]},{"cell_type":"code","source":["tokenized_dataset = load_dataset(\"parquet\", data_files=\"/content/drive/MyDrive/266 Data Project/corpora/nejm/zh-en-tokenized-train-working-model.parquet\")[\"train\"]\n","\n","output_dir = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/custom_models/\"\n","\n","# Before training\n","# Move all components to GPU before training\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","custom_model = custom_model.to(device)\n","# tokenizer = tokenizer.to(device)\n","\n","# Add explicit error checking\n","torch.cuda.empty_cache()  # Clear GPU memory before training\n","\n","# Fine-tune the model\n","fine_tune_custom_model(\n","    custom_model,\n","    tokenizer,\n","    tokenized_dataset,\n","    output_dir\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479,"referenced_widgets":["65560451e77148ab9cb75c56b991dbb7","23053a02a446455b9af116b237324e19","2d6a4fd0c3fc440f87fa14c0b1803183","9dae01c9360a450dacc6382f8e063e78","698768444c6949c49d9acb2dda3ca236","ff1a2157044e4d7d834319f46b6a4814","241df10426d741f19a5aaf4451ea02b9","6086b926eb7e459580063ae87272eea2","ec0c0482beb048899a5b9367408825c4","83a8aae9de5c43bbad2dc8ff6daa26f0","22973aaecd4f468a93203366c4515318"]},"id":"3x0suBQSPvFP","executionInfo":{"status":"ok","timestamp":1733466416805,"user_tz":480,"elapsed":6328199,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"41434b19-2d1d-4968-f584-249d1825aab5"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65560451e77148ab9cb75c56b991dbb7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-4-122b811ad29c>:316: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.18.7"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20241206_044221-qa4czn0n</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/qa4czn0n' target=\"_blank\">/content/drive/MyDrive/266 Data Project/corpora/nejm/custom_models/</a></strong> to <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/qa4czn0n' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface/runs/qa4czn0n</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='10485' max='10485' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [10485/10485 1:44:30, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.431200</td>\n","      <td>1.321974</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.162800</td>\n","      <td>1.096767</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.102500</td>\n","      <td>1.036248</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}}]},{"cell_type":"code","source":["# Saving custom model\n","model_save_path = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/after-training\"\n","custom_model.save_custom(model_save_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wQx4-HR_RYTP","executionInfo":{"status":"ok","timestamp":1733466446007,"user_tz":480,"elapsed":25049,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"3e8f4ff2-e98c-45ba-82a4-738281b249bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/266 Data Project/corpora/nejm/after-training/model\n"]}]},{"cell_type":"code","source":["from evaluate import load\n","import torch\n","\n","def add_special_tokens(tokenizer, entities):\n","    \"\"\"\n","    Adds new entity tokens to the tokenizer if they are not already present.\n","\n","    Args:\n","        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to update.\n","        entities (list of str): List of entity names to add as special tokens.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    special_tokens = [f\"<<{entity}>>\" for entity in entities]\n","    added_tokens = [token for token in special_tokens if token not in tokenizer.get_vocab()]\n","    if added_tokens:\n","        tokenizer.add_special_tokens({'additional_special_tokens': added_tokens})\n","        print(f\"Added new special tokens: {added_tokens}\")\n","\n","\n","def translate_tokenized_dataset(model, tokenizer, tokenized_dataset, batch_size=32):\n","    translations = []\n","\n","    model.eval()\n","\n","    for i in range(0, len(tokenized_dataset), batch_size):\n","        # Extract batch data\n","        input_ids = tokenized_dataset[\"input_ids\"][i:i + batch_size]\n","        attention_mask = tokenized_dataset[\"attention_mask\"][i:i + batch_size]\n","        entity_ids = tokenized_dataset[\"entity_ids\"][i:i + batch_size]\n","\n","        # Convert to tensors with explicit type and device handling\n","        input_ids = torch.tensor(input_ids, dtype=torch.long).to(model.device)\n","        attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(model.device)\n","        entity_ids = torch.tensor(entity_ids, dtype=torch.long).to(model.device)\n","\n","        # Debug print statements\n","        print(f\"Batch {i//batch_size + 1}:\")\n","        print(f\"Input IDs shape: {input_ids.shape}\")\n","        print(f\"Attention Mask shape: {attention_mask.shape}\")\n","        print(f\"Entity IDs shape: {entity_ids.shape}\")\n","        print(f\"Entity IDs min: {entity_ids.min()}, max: {entity_ids.max()}\")\n","        print(f\"Model entity embedding size: {model.entity_embedding.num_embeddings}\")\n","\n","        # Validate entity_ids before generation\n","        try:\n","            # Check if all entity IDs are within the valid range\n","            assert torch.all(entity_ids >= 0), \"Negative entity IDs found\"\n","            assert torch.all(entity_ids < model.entity_embedding.num_embeddings), \"Out-of-bound entity IDs\"\n","        except AssertionError as e:\n","            print(f\"Entity ID validation error: {e}\")\n","            # Skip this batch or handle the error as needed\n","            continue\n","\n","        # Generate translations\n","        try:\n","            with torch.no_grad():\n","                outputs = model.generate(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    entity_ids=entity_ids\n","                )\n","        except Exception as e:\n","            print(f\"Generation error in batch {i//batch_size + 1}: {e}\")\n","            continue\n","\n","        # Decode translations\n","        translated_batch = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n","        translations.extend(translated_batch)\n","\n","    return translations\n","\n","\n","# Define evaluation metrics\n","def evaluate_model_metrics(predictions, references, save_path=None):\n","    # Load the evaluation metrics\n","    bleu_metric = load(\"bleu\")\n","    rouge_metric = load(\"rouge\")\n","    bertscore_metric = load(\"bertscore\")\n","    ter_metric = load(\"ter\")\n","\n","    # Format references for metric calculation\n","    references = [[ref] for ref in references]\n","\n","    # Evaluate BLEU score\n","    bleu_result = bleu_metric.compute(predictions=predictions, references=references)\n","\n","    # Evaluate ROUGE score\n","    rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n","\n","    # Evaluate BERTScore\n","    bertscore_result = bertscore_metric.compute(predictions=predictions, references=references, lang=\"en\")\n","\n","    # Evaluate TER (Translation Edit Rate)\n","    ter_result = ter_metric.compute(predictions=predictions, references=references)\n","\n","    # Extract summary statistics for BERTScore\n","    bertscore_summary = {\n","        \"mean\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]),\n","        \"median\": sorted(bertscore_result[\"f1\"])[len(bertscore_result[\"f1\"]) // 2],\n","        \"std\": (sum((x - sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]))**2 for x in bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]))**0.5\n","    }\n","\n","    # Consolidate results\n","    results = {\n","        \"BLEU\": bleu_result,\n","        \"ROUGE\": rouge_result,\n","        \"BERTScore\": bertscore_summary,\n","        \"TER\": ter_result,\n","    }\n","\n","    return results"],"metadata":{"id":"5fS1Yk4GpnT_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples, tokenizer, src_lang=\"chinese\", max_entities=5):\n","    src_sentences = examples[src_lang]\n","    entities_list = examples.get(\"entities\", [[] for _ in src_sentences])\n","\n","    processed_src = []\n","    processed_entities = []\n","\n","    for sentence, entities in zip(src_sentences, entities_list):\n","        # Process source sentence (add markers for entities in vocabulary)\n","        for entity in entities:\n","            if f\"<<{entity}>>\" in tokenizer.get_vocab():\n","                sentence = sentence.replace(entity, f\"<<{entity}>>\")\n","        processed_src.append(sentence)\n","\n","        # Convert entities to token IDs (if in vocab)\n","        entity_ids = [\n","            tokenizer.convert_tokens_to_ids(f\"<<{entity}>>\")\n","            if f\"<<{entity}>>\" in tokenizer.get_vocab() else 0\n","            for entity in entities\n","        ]\n","\n","        # Pad or truncate entity_ids\n","        entity_ids = entity_ids[:max_entities]\n","        entity_ids += [0] * (max_entities - len(entity_ids))  # Pad with zeros\n","        processed_entities.append(entity_ids)\n","\n","    # Tokenize the processed source sentences\n","    model_inputs = tokenizer(\n","        processed_src,\n","        max_length=128,\n","        truncation=True,\n","        padding=True,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Add entity_ids as a tensor to the inputs\n","    model_inputs[\"entity_ids\"] = torch.tensor(processed_entities, dtype=torch.long)\n","\n","    return model_inputs\n","\n","def preprocess_test_data(test_dataset, tokenizer, src_lang=\"chinese\", max_entities=5):\n","    \"\"\"\n","    Preprocess test data to tokenize inputs and add entity_ids for entity-based embeddings.\n","    \"\"\"\n","    # Wrap preprocess_function with fixed arguments\n","    def wrapped_preprocess_function(examples):\n","        return preprocess_function(\n","            examples, tokenizer=tokenizer, src_lang=src_lang, max_entities=max_entities\n","        )\n","\n","    # Apply the preprocessing function to the test dataset\n","    processed_test_dataset = test_dataset.map(\n","        wrapped_preprocess_function,\n","        batched=True,\n","        remove_columns=test_dataset.column_names,\n","    )\n","\n","    return processed_test_dataset\n","\n","\n","def evaluate_model(model, tokenized_test_dataset, tokenizer, batch_size=16):\n","    \"\"\"\n","    Evaluate the model on the tokenized test dataset.\n","    \"\"\"\n","    # Prepare DataLoader for test data\n","    test_loader = torch.utils.data.DataLoader(\n","        tokenized_test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        collate_fn=lambda batch: tokenizer.pad(batch, return_tensors=\"pt\")\n","    )\n","\n","    model.eval()\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            # Move inputs to GPU if available\n","            inputs = {key: val.to(model.device) for key, val in batch.items() if key != \"labels\"}\n","\n","            # Generate predictions\n","            outputs = model.generate(**inputs)\n","            predictions.extend(outputs)\n","\n","    return predictions\n"],"metadata":{"id":"mDdTQpxEpyuj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load your test dataset\n","test_dataset = load_dataset(\"parquet\", data_files={\"test\": \"/content/drive/MyDrive/266 Data Project/corpora/nejm/nejm_test_entities.parquet\"})[\"test\"]\n","\n","# Preprocess and tokenize test data\n","tokenized_test_dataset = preprocess_test_data(test_dataset, tokenizer)\n","\n","# Evaluate model on test data\n","predictions = evaluate_model(custom_model, tokenized_test_dataset, tokenizer)\n","predictions[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156,"referenced_widgets":["94daa39f3828453b832bf48a268713c7","38d6cdecb6f34b44aa6155f1f313b1fd","445ee10a5b6d416797caa0a7645a2075","6d108856e0db4eadad57a3347768fb97","9b413611ac2f468eb64869857208f196","5f6d2f2905654757984387b466117ff4","efaefb54f481411b8aa8cd2dbb0330f5","a59582dc14a1464b8115fdf68756ff00","6db16fdb2d494a8d9cf3c8a408bd4bf7","1debe358dd964e8388f5a135924232c5","fde160c61af4465e90923cfd265081c5"]},"id":"BLXoJhm3p1Qb","executionInfo":{"status":"ok","timestamp":1733467738526,"user_tz":480,"elapsed":815320,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"2cc46bf1-c880-4b64-b882-ac120a6d0def"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/2102 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94daa39f3828453b832bf48a268713c7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor([65000,     7,  1813,     7, 20212,    86, 15398,     4,     7,  8528,\n","         1552,   748,     7,     2,     7,  1813,     7,  4435, 21845,     0],\n","       device='cuda:0')"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["pd.DataFrame(data={\"predicted_english_tokens\": [x.cpu().numpy() for x in predictions]}).to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/post_training_predictions.parquet\")"],"metadata":{"id":"p1IrWQLhvEc_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decode predictions to text\n","decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n","print(\"Predictions:\", decoded_predictions[0:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gMynXYNnvZ7b","executionInfo":{"status":"ok","timestamp":1733471437066,"user_tz":480,"elapsed":434658,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"3bfa4173-2706-4359-9dc9-b16a5bdc7355"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions: ['an interaction of protein , an immu', 'efficacy antibody therapy with an', 'safety profile of antibody @-@', 'in this phase 1 trial , phase 1', 'the primary end point was determined by the']\n"]}]},{"cell_type":"code","source":["pd.DataFrame(data={\"predicted_english\": decoded_predictions}).to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/post_training_predictions_detokenized.parquet\")"],"metadata":{"id":"3oiPqLqz5Egx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = evaluate_model_metrics(decoded_predictions, test_dataset[\"english\"])\n","results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lmg_vpqf51fy","executionInfo":{"status":"ok","timestamp":1733472096644,"user_tz":480,"elapsed":37232,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"65eb657a-9273-42d8-bacc-56a42617510e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["{'BLEU': {'bleu': 0.002118818585985407,\n","  'precisions': [0.4381300219138057,\n","   0.16612012426648257,\n","   0.0851063829787234,\n","   0.045742814734853594],\n","  'brevity_penalty': 0.0163311288042505,\n","  'length_ratio': 0.19551556698086262,\n","  'translation_length': 13690,\n","  'reference_length': 70020},\n"," 'ROUGE': {'rouge1': 0.1560821290834139,\n","  'rouge2': 0.04536898385243797,\n","  'rougeL': 0.14204262472165466,\n","  'rougeLsum': 0.14213386198079658},\n"," 'BERTScore': {'mean': 0.8180702171305267,\n","  'median': 0.8156791925430298,\n","  'std': 0.027420137574809416},\n"," 'TER': {'score': 92.98824531257128,\n","  'num_edits': 61150,\n","  'ref_length': 65761.0}}"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["pip install sacrebleu"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tYlee_5G9plO","executionInfo":{"status":"ok","timestamp":1733471689328,"user_tz":480,"elapsed":7972,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"c076769e-0e4d-47b9-9b68-6f53e7abcde0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sacrebleu\n","  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n","Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n","Installing collected packages: portalocker, colorama, sacrebleu\n","Successfully installed colorama-0.4.6 portalocker-3.0.0 sacrebleu-2.4.3\n"]}]},{"cell_type":"code","source":["pip install bert_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ley_w9279UdV","executionInfo":{"status":"ok","timestamp":1733471600407,"user_tz":480,"elapsed":8445,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"cb4a2a55-f97f-4958-d8db-4d4d1f69e543"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.46.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.9.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.26.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bert_score\n","Successfully installed bert_score-0.3.13\n"]}]},{"cell_type":"code","source":["pip install rouge_score"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"92s6Gabj9JB8","executionInfo":{"status":"ok","timestamp":1733471556684,"user_tz":480,"elapsed":10656,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"9156eca7-1a8d-49d0-a022-4c6ea32e0c8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.6)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=3a821853eb6d5d97cda169be67a5529f46a6c4f0f2a7881fa49a73598be9274f\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: rouge_score\n","Successfully installed rouge_score-0.1.2\n"]}]},{"cell_type":"code","source":["temp = tokenizer.convert_ids_to_tokens(predictions[0])\n","temp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DBP8JLTfu1Wr","executionInfo":{"status":"ok","timestamp":1733467817617,"user_tz":480,"elapsed":6,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"6c5bc0e3-b2ab-4588-b78c-805c4beadd10"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<pad>',\n"," '▁',\n"," 'an',\n"," '▁',\n"," 'inter',\n"," 'a',\n"," 'ction',\n"," '▁of',\n"," '▁',\n"," 'pro',\n"," 'te',\n"," 'in',\n"," '▁',\n"," ',',\n"," '▁',\n"," 'an',\n"," '▁',\n"," 'im',\n"," 'mu',\n"," '</s>']"]},"metadata":{},"execution_count":20}]}]}