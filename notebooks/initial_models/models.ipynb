{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuned models with tokenized entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define custom Entity-Aware Marian Model class\n",
    "class EntityAwareMarianModel(MarianMTModel):\n",
    "    def __init__(self, config, max_entities=100):\n",
    "        super().__init__(config)\n",
    "        # Entity embedding layer\n",
    "        self.entity_embedding = nn.Embedding(max_entities, config.d_model)\n",
    "        \n",
    "        # Projection layer to integrate entity information\n",
    "        self.entity_projection = nn.Linear(config.d_model, config.d_model)\n",
    "        \n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, entity_ids=None, **kwargs):\n",
    "    # Standard translation model forward pass\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "        \n",
    "        if entity_ids is not None:\n",
    "            # Embed entities\n",
    "            entity_embeddings = self.entity_embedding(entity_ids)  # Shape: [batch_size, max_entities, hidden_dim]\n",
    "            \n",
    "            # Pad or truncate the entity embeddings to match the input sequence length\n",
    "            entity_embeddings_padded = entity_embeddings.unsqueeze(1).repeat(1, 512, 1)  # Repeat across sequence length\n",
    "            \n",
    "            # Project entity embeddings\n",
    "            entity_features = self.entity_projection(entity_embeddings_padded)\n",
    "            \n",
    "            # Add entity features to encoder hidden states\n",
    "            if hasattr(outputs, 'encoder_last_hidden_state'):\n",
    "                outputs.encoder_last_hidden_state += entity_features\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "# Load and prepare the dataset function\n",
    "def prepare_dataset(parquet_file, tokenizer, src_lang=\"chinese\", tgt_lang=\"english\", max_entity_length=10):\n",
    "    # Load the Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Create an entity mapping\n",
    "    unique_entities = set()\n",
    "    for entities in df[\"entities\"]:\n",
    "        unique_entities.update(entities)\n",
    "    \n",
    "    # Create entity to ID mapping\n",
    "    entity_to_id = {entity: idx for idx, entity in enumerate(unique_entities)}\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        processed_inputs = []\n",
    "        processed_entity_ids = []\n",
    "        \n",
    "        for sentence, entities in zip(examples[src_lang], examples[\"entities\"]):\n",
    "            # Preprocess sentence (remove spaces)\n",
    "            sentence = str(sentence).replace(\" \", \"\")\n",
    "            processed_inputs.append(sentence)\n",
    "            \n",
    "            # Map entities to their IDs, padding or truncating as needed\n",
    "            sent_entity_ids = [\n",
    "                entity_to_id.get(entity, 0)  # 0 as default/unknown entity\n",
    "                for entity in entities\n",
    "            ]\n",
    "            # Pad or truncate to a fixed length (e.g., 10)\n",
    "            sent_entity_ids = (sent_entity_ids + [0] * max_entity_length)[:max_entity_length]\n",
    "            processed_entity_ids.append(sent_entity_ids)\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        model_inputs = tokenizer(\n",
    "            processed_inputs,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        \n",
    "        # Tokenize labels\n",
    "        labels = tokenizer(\n",
    "            examples[tgt_lang],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        \n",
    "        # Add entity IDs and labels to the model inputs\n",
    "        model_inputs[\"entity_ids\"] = processed_entity_ids\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    # Convert to Hugging Face Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    # Remove the \"entities\" column as it is no longer needed\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([\"entities\"])\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns([src_lang, tgt_lang])\n",
    "    \n",
    "    return tokenized_dataset, entity_to_id\n",
    "\n",
    "# Fine-tuning the custom model\n",
    "def fine_tune_custom_model(model, tokenizer, tokenized_dataset, output_dir):\n",
    "    # Split dataset\n",
    "    dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        predict_with_generate=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "\n",
    "# Load entity-aware model function\n",
    "def load_entity_aware_model(model_name, unique_entities, max_entities=100):\n",
    "    # Load original model\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Create custom model with entity awareness\n",
    "    config = model.config\n",
    "    entity_aware_model = EntityAwareMarianModel(config, max_entities)\n",
    "    \n",
    "    # Copy weights from original model\n",
    "    entity_aware_model.load_state_dict(model.state_dict(), strict=False)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    return tokenizer, entity_aware_model\n",
    "\n",
    "# Main execution\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "parquet_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_train_entities.parquet\"\n",
    "output_dir = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\embeddings-1\"\n",
    "path_to_entities = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\"\n",
    "\n",
    "# Load entities first\n",
    "named_entities_df = pd.read_parquet(path_to_entities)\n",
    "unique_entities = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n",
    "\n",
    "# Load model with entity mapping\n",
    "tokenizer, model = load_entity_aware_model(model_name, unique_entities)\n",
    "\n",
    "# Prepare dataset using entity mapping\n",
    "tokenized_dataset, entity_to_id = prepare_dataset(parquet_file, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import json\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "def load_marian_with_biomedical_layer(model_name, hidden_size, special_tokens):\n",
    "    # Load tokenizer and add special tokens\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.add_special_tokens({\n",
    "        'additional_special_tokens': list(set(special_tokens))\n",
    "    })\n",
    "\n",
    "    # Load base model\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    \n",
    "    # Create custom model, CustomMarianMTModel will create a BiomedicalEncoder object in init()\n",
    "    custom_model = CustomMarianMTModel(\n",
    "        config=model.config,\n",
    "        hidden_size=hidden_size,\n",
    "        special_token_size=len(special_tokens), \n",
    "    )\n",
    "\n",
    "\n",
    "    # Resize token embeddings\n",
    "    custom_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    return tokenizer, custom_model\n",
    "\n",
    "class BiomedicalEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, special_token_size):\n",
    "        super(BiomedicalEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.special_token_size = special_token_size\n",
    "        \n",
    "        # Adjust the linear layer to match input dimensions\n",
    "        self.linear = nn.Linear(special_token_size, hidden_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, entity_embeddings):\n",
    "        # Reshape entity embeddings if necessary\n",
    "        original_shape = entity_embeddings.shape\n",
    "        \n",
    "        # Flatten the tensor if it has more than 2 dimensions\n",
    "        if len(original_shape) > 2:\n",
    "            entity_embeddings = entity_embeddings.view(-1, original_shape[-1])\n",
    "        \n",
    "        # Ensure the input matches the expected dimension\n",
    "        if entity_embeddings.size(1) != self.special_token_size:\n",
    "            # If the input doesn't match, pad or truncate\n",
    "            if entity_embeddings.size(1) < self.special_token_size:\n",
    "                # Pad with zeros\n",
    "                padding = torch.zeros(\n",
    "                    entity_embeddings.size(0), \n",
    "                    self.special_token_size - entity_embeddings.size(1),\n",
    "                    device=entity_embeddings.device\n",
    "                )\n",
    "                entity_embeddings = torch.cat([entity_embeddings, padding], dim=1)\n",
    "            else:\n",
    "                # Truncate\n",
    "                entity_embeddings = entity_embeddings[:, :self.special_token_size]\n",
    "        \n",
    "        # Apply linear transformation and activation\n",
    "        encoded = self.linear(entity_embeddings)\n",
    "        return self.activation(encoded)\n",
    "\n",
    "class CustomMarianMTModel(MarianMTModel):\n",
    "    def __init__(self, config, hidden_size=512, special_token_size=206573, biomedicalEncoder=None):\n",
    "        super().__init__(config)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.special_token_size = special_token_size\n",
    "\n",
    "        # Initialize biomedical encoder within the model\n",
    "        if biomedicalEncoder == None:\n",
    "            self.biomedical_encoder = BiomedicalEncoder(hidden_size, special_token_size)\n",
    "        else:\n",
    "            self.biomedical_encoder = biomedicalEncoder\n",
    "\n",
    "        # Entity embedding for special tokens\n",
    "        self.entity_embedding = nn.Embedding(special_token_size + 1, hidden_size)  # +1 for padding token\n",
    "\n",
    "        # Projection layer to match vocabulary size\n",
    "        self.entity_projection = nn.Linear(hidden_size, config.vocab_size)\n",
    "\n",
    "    def save_custom(self, save_directory):\n",
    "        # Create save directory if it doesn't exist\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        model_save_path = os.path.join(save_directory, \"model\")\n",
    "        print(model_save_path)\n",
    "        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n",
    "\n",
    "        os.makedirs(model_save_path, exist_ok=True)\n",
    "        os.makedirs(tokenizer_save_path, exist_ok=True)\n",
    "\n",
    "        # Save the model and its configuration\n",
    "        self.save_pretrained(model_save_path)\n",
    "\n",
    "        # Save the biomedical encoder's state_dict\n",
    "        torch.save(self.biomedical_encoder.state_dict(), os.path.join(model_save_path, \"biomedical_encoder.pth\"))\n",
    "\n",
    "        # Save custom attributes in a JSON file\n",
    "        custom_config = {\n",
    "            \"hidden_size\": self.hidden_size,\n",
    "            \"special_token_size\": self.special_token_size,\n",
    "        }\n",
    "        with open(os.path.join(model_save_path, \"custom_config.json\"), \"w\") as f:\n",
    "            json.dump(custom_config, f)\n",
    "\n",
    "        if tokenizer is not None:\n",
    "            tokenizer.save_pretrained(tokenizer_save_path)\n",
    "\n",
    "    @classmethod\n",
    "    def from_custom(cls, save_directory):\n",
    "        model_save_path = os.path.join(save_directory, \"model\")\n",
    "        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n",
    "\n",
    "        # Load custom attributes from JSON\n",
    "        custom_config_path = os.path.join(model_save_path, \"custom_config.json\")\n",
    "        with open(custom_config_path, \"r\") as f:\n",
    "            custom_config = json.load(f)\n",
    "\n",
    "        # Load base model configuration\n",
    "        model = MarianMTModel.from_pretrained(model_save_path)\n",
    "\n",
    "        # Create a new CustomMarianMTModel with the loaded configuration\n",
    "        new_model = cls(\n",
    "            config=model.config, \n",
    "            hidden_size=custom_config[\"hidden_size\"],\n",
    "            special_token_size=custom_config[\"special_token_size\"]\n",
    "        )\n",
    "\n",
    "        # Load the biomedical encoder state dict\n",
    "        biomedical_encoder_path = os.path.join(model_save_path, \"biomedical_encoder.pth\")\n",
    "        biomedical_encoder_state_dict = torch.load(biomedical_encoder_path)\n",
    "        new_model.biomedical_encoder.load_state_dict(biomedical_encoder_state_dict)\n",
    "\n",
    "        # Load the main model weights\n",
    "        state_dict = model.state_dict()\n",
    "        new_model_state_dict = new_model.state_dict()\n",
    "        \n",
    "        # Update the state dictionary, keeping the biomedical encoder weights\n",
    "        for key, value in state_dict.items():\n",
    "            if key in new_model_state_dict:\n",
    "                new_model_state_dict[key] = value\n",
    "        \n",
    "        new_model.load_state_dict(new_model_state_dict, strict=False)\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = MarianTokenizer.from_pretrained(tokenizer_save_path)\n",
    "\n",
    "        return new_model, tokenizer\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, entity_ids=None, **kwargs):\n",
    "        # Perform base MarianMT forward pass\n",
    "        outputs = super().forward(input_ids=input_ids, attention_mask=attention_mask, labels=labels, **kwargs)\n",
    "\n",
    "        # Process entity information if provided\n",
    "        if entity_ids is not None:\n",
    "            try:\n",
    "                # Ensure entity_ids is a tensor with 2 dimensions\n",
    "                if len(entity_ids.shape) == 1:\n",
    "                    entity_ids = entity_ids.unsqueeze(0)\n",
    "\n",
    "                # Get batch size, sequence length, and vocab size from outputs\n",
    "                batch_size = outputs.logits.size(0)\n",
    "                sequence_length = outputs.logits.size(1)\n",
    "                vocab_size = outputs.logits.size(2)\n",
    "\n",
    "                # Ensure entity_ids is on the same device as outputs.logits\n",
    "                entity_ids = entity_ids.to(outputs.logits.device)\n",
    "\n",
    "                # Limit entity_ids to current batch size\n",
    "                entity_ids = entity_ids[:batch_size]\n",
    "\n",
    "                if torch.any(entity_ids >= self.entity_embedding.num_embeddings):\n",
    "                    print(f\"Invalid entity IDs detected: {entity_ids}\")\n",
    "                    raise ValueError(\"Entity IDs are out of bounds for the embedding layer\")\n",
    "\n",
    "\n",
    "                # Get embeddings for entity special tokens\n",
    "                entity_embeddings = self.entity_embedding(entity_ids)\n",
    "\n",
    "                # Ensure embeddings are on the correct device\n",
    "                entity_embeddings = entity_embeddings.to(outputs.logits.device)\n",
    "\n",
    "                # Process through biomedical encoder\n",
    "                original_shape = entity_embeddings.shape\n",
    "                entity_features = self.biomedical_encoder(entity_embeddings.view(-1, original_shape[-1]))\n",
    "\n",
    "                # Reshape back to original batch and entity dimension\n",
    "                entity_features = entity_features.view(original_shape[0], original_shape[1], -1)\n",
    "\n",
    "                # Project entity features to match logits dimensionality\n",
    "                entity_logits = self.entity_projection(entity_features)\n",
    "\n",
    "                # Ensure logits are on the correct device\n",
    "                entity_logits = entity_logits.to(outputs.logits.device)\n",
    "\n",
    "                # Create a tensor of zeros with the same shape as outputs.logits\n",
    "                expanded_entity_logits = torch.zeros_like(outputs.logits)\n",
    "\n",
    "                # Adjust logits shape to match the entity length\n",
    "                min_entities = min(entity_logits.size(1), expanded_entity_logits.size(1))\n",
    "                min_vocab = min(entity_logits.size(2), expanded_entity_logits.size(2))\n",
    "\n",
    "                expanded_entity_logits[:, :min_entities, :min_vocab] = entity_logits[:, :min_entities, :min_vocab]\n",
    "\n",
    "                # Add entity-based logits to original logits\n",
    "                outputs.logits = outputs.logits + expanded_entity_logits\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in forward method: {e}\")\n",
    "                raise\n",
    "        torch.cuda.synchronize()\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "def prepare_dataset(dataset, tokenizer, src_lang=\"chinese\", tgt_lang=\"english\", max_entities=5):\n",
    "    def preprocess_function(examples):\n",
    "        # Ensure inputs are lists\n",
    "        src_sentences = examples[src_lang]\n",
    "        tgt_sentences = examples[tgt_lang]\n",
    "        entities_list = examples.get(\"entities\", [[] for _ in src_sentences])\n",
    "\n",
    "        processed_src = []\n",
    "        processed_entities = []\n",
    "        \n",
    "        for sentence, entities in zip(src_sentences, entities_list):\n",
    "            # Ensure sentence is a string and remove existing spaces\n",
    "            sentence = str(sentence).replace(\" \", \"\")\n",
    "            \n",
    "            # Add special tokens for entities\n",
    "            for entity in entities:\n",
    "                sentence = sentence.replace(entity, f\"<<{entity}>>\")\n",
    "            \n",
    "            processed_src.append(sentence)\n",
    "            \n",
    "            # Convert entities to token IDs\n",
    "            entity_ids = [\n",
    "                tokenizer.convert_tokens_to_ids(f\"<<{entity}>>\") \n",
    "                for entity in entities\n",
    "            ]\n",
    "            \n",
    "            # Pad or truncate entity_ids\n",
    "            entity_ids = entity_ids[:max_entities]\n",
    "            entity_ids += [0] * (max_entities - len(entity_ids))\n",
    "            \n",
    "            # Debugging: Log entity ids and padding\n",
    "            # print(f\"Entity IDs (after padding/truncation): {entity_ids}\")\n",
    "\n",
    "            processed_entities.append(entity_ids)\n",
    "\n",
    "        # Tokenize source sentences\n",
    "        model_inputs = tokenizer(\n",
    "            processed_src,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Tokenize target sentences\n",
    "        labels = tokenizer(\n",
    "            tgt_sentences,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Add labels to model inputs\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        \n",
    "        # Convert entity_ids to tensor\n",
    "        model_inputs[\"entity_ids\"] = torch.tensor(processed_entities, dtype=torch.long)\n",
    "        \n",
    "        return model_inputs\n",
    "\n",
    "    # Apply preprocessing to the dataset\n",
    "    processed_dataset = dataset.map(\n",
    "        preprocess_function, \n",
    "        batched=True, \n",
    "        remove_columns=dataset.column_names\n",
    "    )\n",
    "\n",
    "    return processed_dataset\n",
    "\n",
    "def fine_tune_custom_model(custom_model, tokenizer, tokenized_dataset, output_dir):\n",
    "    # Split dataset\n",
    "    dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_safetensors=False,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=100,\n",
    "        predict_with_generate=True,\n",
    "        push_to_hub=False,\n",
    "        fp16=False  # Disable mixed precision\n",
    "    )\n",
    "\n",
    "    # Create trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=custom_model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 62127/62127 [00:21<00:00, 2886.20 examples/s]\n",
      "Creating parquet from Arrow format: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:00<00:00, 520.05ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25052770"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "output_dir = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\embeddings-2\"\n",
    "hidden_size = 512\n",
    "\n",
    "# Load named entities\n",
    "named_entities_df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")\n",
    "named_entities = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n",
    "special_tokens = [f\"<<{entity}>>\" for entity in named_entities]\n",
    "special_token_size = len(special_tokens)\n",
    "\n",
    "# Load tokenizer and custom model\n",
    "tokenizer, custom_model = load_marian_with_biomedical_layer(\n",
    "    model_name, \n",
    "    hidden_size, \n",
    "    special_tokens\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"parquet\", data_files=\"nejm/nejm_train_entities.parquet\")[\"train\"]\n",
    "\n",
    "# Prepare dataset\n",
    "tokenized_dataset = prepare_dataset(\n",
    "    dataset, \n",
    "    tokenizer, \n",
    "    src_lang=\"chinese\", \n",
    "    tgt_lang=\"english\"\n",
    ")\n",
    "\n",
    "dataset.to_parquet(\"nejm/512_tokenized_nejm_train_entities.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "output_dir = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\embeddings-2\"\n",
    "hidden_size = 512\n",
    "\n",
    "# Load named entities\n",
    "named_entities_df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")\n",
    "named_entities = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n",
    "special_tokens = [f\"<<{entity}>>\" for entity in named_entities]\n",
    "special_token_size = len(special_tokens)\n",
    "\n",
    "# Load tokenizer and custom model\n",
    "tokenizer, custom_model = load_marian_with_biomedical_layer(\n",
    "    model_name, \n",
    "    hidden_size, \n",
    "    special_tokens\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"parquet\", data_files=\"nejm/nejm_train_entities.parquet\")[\"train\"]\n",
    "\n",
    "# Prepare dataset\n",
    "tokenized_dataset = prepare_dataset(\n",
    "    dataset, \n",
    "    tokenizer, \n",
    "    src_lang=\"chinese\", \n",
    "    tgt_lang=\"english\"\n",
    ")\n",
    "\n",
    "# Before training\n",
    "# Move all components to GPU before training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model = custom_model.to(device)\n",
    "# tokenizer = tokenizer.to(device)\n",
    "\n",
    "# Add explicit error checking\n",
    "torch.cuda.empty_cache()  # Clear GPU memory before training\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_custom_model(\n",
    "    custom_model, \n",
    "    tokenizer, \n",
    "    tokenized_dataset, \n",
    "    output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "c:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Gaming\\AppData\\Local\\Temp\\ipykernel_21196\\1013327680.py:316: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maalin\u001b[0m (\u001b[33maalin-uc-berkeley\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Gaming\\Documents\\GitHub\\MIE2\\2024-fall-assignment-linaaron88\\project\\wandb\\run-20241205_201844-k5jrcj2a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/k5jrcj2a' target=\"_blank\">C:\\Users\\Gaming\\Documents\\GitHub\\MIE2\\2024-fall-assignment-linaaron88\\project\\custom_fine_tuned_marianmt\\embeddings-2</a></strong> to <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/k5jrcj2a' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface/runs/k5jrcj2a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 100/10485 [04:29<6:29:10,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.9977, 'grad_norm': 10.107197761535645, 'learning_rate': 4.9523128278493086e-05, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–         | 200/10485 [10:01<5:33:17,  1.94s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.7475, 'grad_norm': 8.768858909606934, 'learning_rate': 4.904625655698618e-05, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 300/10485 [13:08<5:18:11,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.0411, 'grad_norm': 5.347640514373779, 'learning_rate': 4.856938483547926e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–         | 400/10485 [16:11<5:03:02,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9677, 'grad_norm': 1.1317561864852905, 'learning_rate': 4.8092513113972345e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|â–         | 500/10485 [19:17<5:00:08,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4141, 'grad_norm': 0.8909512758255005, 'learning_rate': 4.761564139246543e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 600/10485 [22:27<5:02:38,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2436, 'grad_norm': 0.7377718687057495, 'learning_rate': 4.713876967095851e-05, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 700/10485 [25:38<5:09:50,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1422, 'grad_norm': 0.7214361429214478, 'learning_rate': 4.66618979494516e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|â–Š         | 800/10485 [28:48<5:07:24,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0997, 'grad_norm': 0.6726440787315369, 'learning_rate': 4.618502622794468e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|â–Š         | 900/10485 [31:59<5:03:05,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9765, 'grad_norm': 0.6875271201133728, 'learning_rate': 4.570815450643777e-05, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 1000/10485 [35:10<5:00:57,  1.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9157, 'grad_norm': 0.732824444770813, 'learning_rate': 4.5231282784930856e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–ˆ         | 1100/10485 [38:17<4:41:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9045, 'grad_norm': 1.0072081089019775, 'learning_rate': 4.475441106342394e-05, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆâ–        | 1200/10485 [41:22<4:42:25,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8615, 'grad_norm': 0.8328938484191895, 'learning_rate': 4.4277539341917024e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|â–ˆâ–        | 1300/10485 [44:24<4:43:56,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8697, 'grad_norm': 0.889186680316925, 'learning_rate': 4.3800667620410114e-05, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|â–ˆâ–Ž        | 1400/10485 [47:28<4:31:58,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7539, 'grad_norm': 0.7987344861030579, 'learning_rate': 4.33237958989032e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|â–ˆâ–        | 1500/10485 [50:40<4:49:51,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7864, 'grad_norm': 0.8306804895401001, 'learning_rate': 4.284692417739628e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|â–ˆâ–Œ        | 1600/10485 [53:44<4:28:11,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6972, 'grad_norm': 0.7653444409370422, 'learning_rate': 4.237005245588937e-05, 'epoch': 0.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 1700/10485 [56:45<4:29:15,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6908, 'grad_norm': 0.8932445645332336, 'learning_rate': 4.189318073438245e-05, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|â–ˆâ–‹        | 1800/10485 [59:57<4:39:40,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6665, 'grad_norm': 0.9426655769348145, 'learning_rate': 4.1416309012875534e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|â–ˆâ–Š        | 1900/10485 [1:03:09<4:35:12,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.66, 'grad_norm': 0.8182628750801086, 'learning_rate': 4.0939437291368625e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|â–ˆâ–‰        | 2000/10485 [1:06:22<4:37:18,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6528, 'grad_norm': 0.7854791283607483, 'learning_rate': 4.046256556986171e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|â–ˆâ–ˆ        | 2100/10485 [1:09:35<4:28:59,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6229, 'grad_norm': 0.9765541553497314, 'learning_rate': 3.998569384835479e-05, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 2200/10485 [1:12:46<4:01:17,  1.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5948, 'grad_norm': 0.8807556629180908, 'learning_rate': 3.950882212684788e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|â–ˆâ–ˆâ–       | 2300/10485 [1:15:45<4:13:32,  1.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5437, 'grad_norm': 0.9429752230644226, 'learning_rate': 3.903195040534097e-05, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|â–ˆâ–ˆâ–Ž       | 2400/10485 [1:18:47<4:06:02,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5176, 'grad_norm': 0.9150956273078918, 'learning_rate': 3.855507868383405e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|â–ˆâ–ˆâ–       | 2500/10485 [1:21:50<4:03:22,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.532, 'grad_norm': 0.8026453256607056, 'learning_rate': 3.8078206962327136e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|â–ˆâ–ˆâ–       | 2600/10485 [1:24:53<4:06:03,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4984, 'grad_norm': 0.7272871136665344, 'learning_rate': 3.7601335240820226e-05, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 2700/10485 [1:28:00<4:02:52,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5027, 'grad_norm': 0.8717427253723145, 'learning_rate': 3.712446351931331e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|â–ˆâ–ˆâ–‹       | 2800/10485 [1:31:06<3:38:27,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.493, 'grad_norm': 0.9254312515258789, 'learning_rate': 3.6647591797806394e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|â–ˆâ–ˆâ–Š       | 2900/10485 [1:34:13<3:56:50,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4484, 'grad_norm': 0.8104346990585327, 'learning_rate': 3.617072007629947e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|â–ˆâ–ˆâ–Š       | 3000/10485 [1:37:17<3:47:39,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4717, 'grad_norm': 0.8786535859107971, 'learning_rate': 3.569384835479256e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–‰       | 3100/10485 [1:40:19<3:43:58,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4213, 'grad_norm': 0.9241299033164978, 'learning_rate': 3.5216976633285646e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆ       | 3200/10485 [1:43:25<3:46:51,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4814, 'grad_norm': 1.0273315906524658, 'learning_rate': 3.474010491177873e-05, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|â–ˆâ–ˆâ–ˆâ–      | 3300/10485 [1:46:33<3:45:28,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4167, 'grad_norm': 0.8897446990013123, 'learning_rate': 3.426323319027182e-05, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|â–ˆâ–ˆâ–ˆâ–      | 3400/10485 [1:49:42<3:42:31,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4294, 'grad_norm': 0.934633731842041, 'learning_rate': 3.3786361468764905e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3495/10485 [1:55:18<3:04:16,  1.58s/it]c:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3367047309875488, 'eval_runtime': 170.7677, 'eval_samples_per_second': 36.383, 'eval_steps_per_second': 2.278, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 3500/10485 [1:55:32<27:58:21, 14.42s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4109, 'grad_norm': 0.8515102863311768, 'learning_rate': 3.330948974725799e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|â–ˆâ–ˆâ–ˆâ–      | 3600/10485 [1:58:39<3:23:04,  1.77s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3737, 'grad_norm': 1.0019389390945435, 'learning_rate': 3.283261802575107e-05, 'epoch': 1.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 3700/10485 [2:01:50<3:39:17,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3692, 'grad_norm': 0.9518450498580933, 'learning_rate': 3.2355746304244164e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 3800/10485 [2:05:03<3:36:55,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3468, 'grad_norm': 1.0818392038345337, 'learning_rate': 3.187887458273725e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 3900/10485 [2:08:16<3:26:17,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3488, 'grad_norm': 0.8986314535140991, 'learning_rate': 3.140200286123033e-05, 'epoch': 1.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 4000/10485 [2:11:21<3:18:11,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3325, 'grad_norm': 0.8874548077583313, 'learning_rate': 3.0925131139723415e-05, 'epoch': 1.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|â–ˆâ–ˆâ–ˆâ–‰      | 4100/10485 [2:14:25<3:11:03,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.324, 'grad_norm': 0.9662432074546814, 'learning_rate': 3.0448259418216503e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4200/10485 [2:17:22<3:15:48,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3186, 'grad_norm': 1.0675803422927856, 'learning_rate': 2.9971387696709587e-05, 'epoch': 1.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 4300/10485 [2:20:28<3:09:04,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3183, 'grad_norm': 0.9741929173469543, 'learning_rate': 2.9494515975202674e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4400/10485 [2:23:31<3:07:05,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.283, 'grad_norm': 0.8321027159690857, 'learning_rate': 2.9017644253695758e-05, 'epoch': 1.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 4500/10485 [2:26:37<3:12:29,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2938, 'grad_norm': 1.0482348203659058, 'learning_rate': 2.8540772532188842e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4600/10485 [2:29:43<2:58:20,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2951, 'grad_norm': 1.1143715381622314, 'learning_rate': 2.8063900810681926e-05, 'epoch': 1.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 4700/10485 [2:32:45<2:55:24,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2522, 'grad_norm': 1.0040342807769775, 'learning_rate': 2.7587029089175013e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 4800/10485 [2:35:51<3:01:35,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.266, 'grad_norm': 0.9950077533721924, 'learning_rate': 2.7110157367668097e-05, 'epoch': 1.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 4900/10485 [2:39:06<2:52:22,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2771, 'grad_norm': 1.0714131593704224, 'learning_rate': 2.663328564616118e-05, 'epoch': 1.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 5000/10485 [2:42:17<2:55:36,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2503, 'grad_norm': 0.9988529682159424, 'learning_rate': 2.6156413924654272e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 5100/10485 [2:45:29<2:52:10,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.244, 'grad_norm': 1.1736501455307007, 'learning_rate': 2.5679542203147356e-05, 'epoch': 1.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 5200/10485 [2:48:23<2:36:44,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.272, 'grad_norm': 1.1071336269378662, 'learning_rate': 2.520267048164044e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 5300/10485 [2:51:23<2:35:10,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2405, 'grad_norm': 1.0398554801940918, 'learning_rate': 2.4725798760133524e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5400/10485 [2:54:18<2:27:51,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2181, 'grad_norm': 1.097158670425415, 'learning_rate': 2.4248927038626608e-05, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5500/10485 [2:57:17<2:28:35,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2454, 'grad_norm': 0.9179044365882874, 'learning_rate': 2.3772055317119695e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 5600/10485 [3:00:10<2:19:03,  1.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1903, 'grad_norm': 0.9687941670417786, 'learning_rate': 2.3295183595612783e-05, 'epoch': 1.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 5700/10485 [3:03:02<2:17:45,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2328, 'grad_norm': 1.1316378116607666, 'learning_rate': 2.2818311874105867e-05, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 5800/10485 [3:05:56<2:12:29,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1988, 'grad_norm': 1.1029574871063232, 'learning_rate': 2.2341440152598954e-05, 'epoch': 1.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 5900/10485 [3:08:49<2:14:29,  1.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2245, 'grad_norm': 0.9594400525093079, 'learning_rate': 2.1864568431092035e-05, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 6000/10485 [3:11:52<2:14:07,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2186, 'grad_norm': 0.8487704396247864, 'learning_rate': 2.1387696709585122e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 6100/10485 [3:14:51<2:09:13,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2073, 'grad_norm': 1.195090651512146, 'learning_rate': 2.091082498807821e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 6200/10485 [3:18:41<2:44:35,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2201, 'grad_norm': 1.038196325302124, 'learning_rate': 2.0433953266571293e-05, 'epoch': 1.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6300/10485 [3:24:33<6:47:27,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2099, 'grad_norm': 1.0752381086349487, 'learning_rate': 1.995708154506438e-05, 'epoch': 1.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 6400/10485 [3:32:01<5:17:57,  4.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2126, 'grad_norm': 1.008446455001831, 'learning_rate': 1.9480209823557465e-05, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 6500/10485 [3:39:32<4:32:11,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1781, 'grad_norm': 1.0756773948669434, 'learning_rate': 1.900333810205055e-05, 'epoch': 1.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6600/10485 [3:48:24<7:49:45,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.179, 'grad_norm': 0.9816827178001404, 'learning_rate': 1.8526466380543633e-05, 'epoch': 1.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 6641/10485 [3:52:25<6:16:10,  5.87s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clear GPU memory before training\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[43mfine_tune_custom_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 325\u001b[0m, in \u001b[0;36mfine_tune_custom_model\u001b[1;34m(custom_model, tokenizer, tokenized_dataset, output_dir)\u001b[0m\n\u001b[0;32m    316\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    317\u001b[0m     model\u001b[38;5;241m=\u001b[39mcustom_model,\n\u001b[0;32m    318\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    321\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m    322\u001b[0m )\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 325\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 194\u001b[0m, in \u001b[0;36mCustomMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, labels, entity_ids, **kwargs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Process through biomedical encoder\u001b[39;00m\n\u001b[0;32m    193\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m entity_embeddings\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 194\u001b[0m entity_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbiomedical_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# Reshape back to original batch and entity dimension\u001b[39;00m\n\u001b[0;32m    197\u001b[0m entity_features \u001b[38;5;241m=\u001b[39m entity_features\u001b[38;5;241m.\u001b[39mview(original_shape[\u001b[38;5;241m0\u001b[39m], original_shape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 63\u001b[0m, in \u001b[0;36mBiomedicalEncoder.forward\u001b[1;34m(self, entity_embeddings)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m entity_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_token_size:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;66;03m# Pad with zeros\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     padding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[0;32m     59\u001b[0m         entity_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_token_size \u001b[38;5;241m-\u001b[39m entity_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m),\n\u001b[0;32m     61\u001b[0m         device\u001b[38;5;241m=\u001b[39mentity_embeddings\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m     62\u001b[0m     )\n\u001b[1;32m---> 63\u001b[0m     entity_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mentity_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Truncate\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     entity_embeddings \u001b[38;5;241m=\u001b[39m entity_embeddings[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_token_size]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "output_dir = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\embeddings-2\"\n",
    "hidden_size = 512\n",
    "\n",
    "# Load named entities\n",
    "named_entities_df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")\n",
    "named_entities = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n",
    "special_tokens = [f\"<<{entity}>>\" for entity in named_entities]\n",
    "special_token_size = len(special_tokens)\n",
    "\n",
    "# Load tokenizer and custom model\n",
    "tokenizer, custom_model = load_marian_with_biomedical_layer(\n",
    "    model_name, \n",
    "    hidden_size, \n",
    "    special_tokens\n",
    ")\n",
    "\n",
    "tokenized_dataset = load_dataset(\"parquet\", data_files=\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\tokenized\\\\zh-en-tokenized-train-working-model.parquet\")[\"train\"]\n",
    "\n",
    "# Before training\n",
    "# Move all components to GPU before training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model = custom_model.to(device)\n",
    "# tokenizer = tokenizer.to(device)\n",
    "\n",
    "# Add explicit error checking\n",
    "torch.cuda.empty_cache()  # Clear GPU memory before training\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_custom_model(\n",
    "    custom_model, \n",
    "    tokenizer, \n",
    "    tokenized_dataset, \n",
    "    output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving custom model\n",
    "model_save_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\custom_data_saver-2\"\n",
    "custom_model.save_custom(model_save_path)\n",
    "\n",
    "# Loading custom model\n",
    "custom_model_2 = CustomMarianMTModel.from_custom(model_save_path, config=custom_model.config, biomedical_encoder=custom_model.biomedical_encoder, hidden_size=512, special_token_size=special_token_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaming\\AppData\\Local\\Temp\\ipykernel_27716\\2116545771.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  biomedicalEncoder = BiomedicalEncoder(custom_config[\"hidden_size\"], custom_config[\"special_token_size\"]).load_state_dict(torch.load(biomedical_encoder_path))\n",
      "c:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Gaming\\AppData\\Local\\Temp\\ipykernel_27716\\2116545771.py:309: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maalin\u001b[0m (\u001b[33maalin-uc-berkeley\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Gaming\\Documents\\GitHub\\MIE2\\2024-fall-assignment-linaaron88\\project\\wandb\\run-20241205_193916-yn3bbugh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/yn3bbugh' target=\"_blank\">C:\\Users\\Gaming\\Documents\\GitHub\\MIE2\\2024-fall-assignment-linaaron88\\project\\custom_fine_tuned_marianmt\\embeddings-2</a></strong> to <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aalin-uc-berkeley/huggingface/runs/yn3bbugh' target=\"_blank\">https://wandb.ai/aalin-uc-berkeley/huggingface/runs/yn3bbugh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10485 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in forward method: '_IncompatibleKeys' object is not callable\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_IncompatibleKeys' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clear GPU memory before training\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[43mfine_tune_custom_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 318\u001b[0m, in \u001b[0;36mfine_tune_custom_model\u001b[1;34m(custom_model, tokenizer, tokenized_dataset, output_dir)\u001b[0m\n\u001b[0;32m    309\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    310\u001b[0m     model\u001b[38;5;241m=\u001b[39mcustom_model,\n\u001b[0;32m    311\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m    315\u001b[0m )\n\u001b[0;32m    317\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m--> 318\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3585\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[1;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 187\u001b[0m, in \u001b[0;36mCustomMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, labels, entity_ids, **kwargs)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;66;03m# Process through biomedical encoder\u001b[39;00m\n\u001b[0;32m    186\u001b[0m original_shape \u001b[38;5;241m=\u001b[39m entity_embeddings\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 187\u001b[0m entity_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbiomedical_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_embeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;66;03m# Reshape back to original batch and entity dimension\u001b[39;00m\n\u001b[0;32m    190\u001b[0m entity_features \u001b[38;5;241m=\u001b[39m entity_features\u001b[38;5;241m.\u001b[39mview(original_shape[\u001b[38;5;241m0\u001b[39m], original_shape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: '_IncompatibleKeys' object is not callable"
     ]
    }
   ],
   "source": [
    "model_save_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\NER-model\\\\before-training\"\n",
    "\n",
    "custom_model, tokenizer = CustomMarianMTModel.from_custom(model_save_path)\n",
    "\n",
    "tokenized_dataset = load_dataset(\"parquet\", data_files=\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\tokenized\\\\zh-en-tokenized-train-working-model.parquet\")[\"train\"]\n",
    "\n",
    "# Before training\n",
    "# Move all components to GPU before training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model = custom_model.to(device)\n",
    "# tokenizer = tokenizer.to(device)\n",
    "\n",
    "# Add explicit error checking\n",
    "torch.cuda.empty_cache()  # Clear GPU memory before training\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_custom_model(\n",
    "    custom_model, \n",
    "    tokenizer, \n",
    "    tokenized_dataset, \n",
    "    output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels', 'entity_ids'],\n",
       "    num_rows: 62127\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"parquet\", data_files=\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\tokenized\\\\zh-en-tokenized-train-working-model.parquet\")[\"train\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaming\\Documents\\GitHub\\MIE2\\2024-fall-assignment-linaaron88\\project\\NER-model\\before-training\\model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Gaming\\AppData\\Local\\Temp\\ipykernel_27716\\2116545771.py:133: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  biomedicalEncoder = BiomedicalEncoder(custom_config[\"hidden_size\"], custom_config[\"special_token_size\"]).load_state_dict(torch.load(biomedical_encoder_path))\n"
     ]
    }
   ],
   "source": [
    "# Saving custom model\n",
    "model_save_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\NER-model\\\\before-training\"\n",
    "custom_model.save_custom(model_save_path)\n",
    "\n",
    "# Loading custom model\n",
    "custom_model_base, tokenizer_base = CustomMarianMTModel.from_custom(model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving custom model\n",
    "model_save_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\latest-trained\"\n",
    "custom_model.save_custom(model_save_path)\n",
    "\n",
    "# Loading custom model\n",
    "custom_model_2, tokenizer_2 = CustomMarianMTModel.from_custom(model_save_path, config=custom_model.config, biomedical_encoder=custom_model.biomedical_encoder, hidden_size=512, special_token_size=special_token_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'custom_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Before training\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Move all components to GPU before training\u001b[39;00m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_model\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# tokenizer = tokenizer.to(device)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Add explicit error checking\u001b[39;00m\n\u001b[0;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()  \u001b[38;5;66;03m# Clear GPU memory before training\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'custom_model' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = load_dataset(\"parquet\",data_files=\"nejm/tokenized/zh-en-tokenized-train-working-model.parquet\")[\"train\"]\n",
    "# Before training\n",
    "# Move all components to GPU before training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model = custom_model.to(device)\n",
    "# tokenizer = tokenizer.to(device)\n",
    "\n",
    "# Add explicit error checking\n",
    "torch.cuda.empty_cache()  # Clear GPU memory before training\n",
    "\n",
    "# Fine-tune the model\n",
    "fine_tune_custom_model(\n",
    "    custom_model, \n",
    "    tokenizer, \n",
    "    tokenized_dataset, \n",
    "    output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\tokenized\\\\zh-en-tokenized-train-working-model.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer with special tokens\n",
    "tokenizer.save_pretrained(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\NER-model\\\\pre-training-tokenizer\")\n",
    "\n",
    "# Save the model (with added modifications like entity embeddings)\n",
    "custom_model.save_pretrained(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\NER-model\\\\pre-training-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_input = tokenized_dataset['input_ids'][0]\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import pandas as pd\n",
    "\n",
    "# fine_tuned_model_path = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\fine_tuned_marian_model-zh-en/checkpoint-10485\"\n",
    "# fine_tuned_model_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\custom_fine_tuned_marianmt\\\\first_run\\\\checkpoint-10485\"\n",
    "\n",
    "# tokenizer = MarianTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "# model = MarianMTModel.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "tokenizer = None\n",
    "model = None\n",
    "\n",
    "# Hyperparameter tuning objective function\n",
    "def objective(trial):\n",
    "    global model, tokenizer\n",
    "    # Suggest hyperparameters to tune\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-6, 1e-3)\n",
    "    batch_size = trial.suggest_int('batch_size', 8, 32, step=8)\n",
    "    num_train_epochs = trial.suggest_int('num_train_epochs', 2, 5)\n",
    "\n",
    "    # Load preprocessed entities\n",
    "    named_entities_df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")\n",
    "    named_entities = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n",
    "    special_tokens = [f\"<{entity}>\" for entity in named_entities]\n",
    "\n",
    "    # Load tokenizer and custom model\n",
    "    hidden_size = 512\n",
    "    \n",
    "    # Initialize model and tokenizer\n",
    "    model_name = \"Helsinki-NLP/opus-mt-zh-en\"  # Change as needed\n",
    "    tokenizer, model = load_marian_with_biomedical_layer(model_name, hidden_size, special_tokens)\n",
    "\n",
    "    # Prepare dataset\n",
    "    tokenized_dataset = prepare_dataset(\"your_dataset.parquet\", tokenizer)\n",
    "\n",
    "    # Split dataset\n",
    "    dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    output_dir = f\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\lr_{learning_rate}_batch-size_{batch_size}_epochs_{num_train_epochs}\"\n",
    "\n",
    "\n",
    "    # Training arguments with suggested hyperparameters\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=learning_rate,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_safetensors = False,\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=100,\n",
    "        predict_with_generate=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    # Trainer setup\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the validation set (using BLEU as an example)\n",
    "    eval_results = trainer.evaluate()\n",
    "    bleu_score = eval_results[\"eval_bleu\"]\n",
    "\n",
    "    # Return the negative BLEU score because Optuna minimizes the objective\n",
    "    return -bleu_score\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize the negative BLEU score\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print best trial\n",
    "print(f\"Best trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities_df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "def load_trained_model_and_tokenizer(model_dir, hidden_size, special_tokens):\n",
    "    \"\"\"\n",
    "    Load the trained model and tokenizer.\n",
    "\n",
    "    :param model_dir: Directory where the model checkpoint is stored.\n",
    "    :param hidden_size: The hidden size used for the biomedical encoder.\n",
    "    :param special_tokens: A list of special tokens used during training.\n",
    "    :return: The tokenizer and custom trained model.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer and add special tokens\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_dir)\n",
    "    # tokenizer.add_special_tokens({\n",
    "    #     'additional_special_tokens': list(set(special_tokens))\n",
    "    # })\n",
    "\n",
    "    # Load the model architecture and the trained weights\n",
    "    model = MarianMTModel.from_pretrained(model_dir)\n",
    "    \n",
    "    # Create biomedical encoder (same as during training)\n",
    "    special_token_size = len(special_tokens)\n",
    "    biomedical_encoder = BiomedicalEncoder(hidden_size, special_token_size)\n",
    "    \n",
    "    # Create custom model\n",
    "    custom_model = CustomMarianMTModel(\n",
    "        model.config, \n",
    "        hidden_size, \n",
    "        special_token_size, \n",
    "        biomedical_encoder\n",
    "    )\n",
    "\n",
    "    # Resize token embeddings based on the tokenizer size\n",
    "    custom_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # Load the trained weights into the model\n",
    "    checkpoint_path = os.path.join(model_dir, \"pytorch_model.bin\")\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Load state dict with strict=False in case there are any mismatches\n",
    "    custom_model.load_state_dict(checkpoint, strict=False)\n",
    "    \n",
    "    # Return the tokenizer and the trained model\n",
    "    return tokenizer, custom_model\n",
    "\n",
    "# Example usage\n",
    "model_directory = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\embeddings-2\\\\checkpoint-10485\"\n",
    "hidden_size = 512  # Use the hidden size from training\n",
    "named_entities_df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")\n",
    "named_entities = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n",
    "special_tokens = [f\"<<{entity}>>\" for entity in named_entities]\n",
    "\n",
    "tokenizer, custom_model = load_trained_model_and_tokenizer(model_directory, hidden_size, special_tokens)\n",
    "\n",
    "# Optionally, move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "custom_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "special_token_size = len(special_tokens)\n",
    "biomedical_encoder = BiomedicalEncoder(hidden_size, special_token_size)\n",
    "model = MarianMTModel.from_pretrained(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\embeddings-2\\\\checkpoint-10485\")\n",
    "model = CustomMarianMTModel(config=model.config, hidden_size=512, special_token_size=special_token_size, biomedical_encoder=biomedical_encoder).from_pretrained(model_directory,config=model.config, hidden_size=512, special_token_size=special_token_size, biomedical_encoder=biomedical_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import MarianConfig, MarianMTModel\n",
    "import pandas as pd\n",
    "\n",
    "# Recreate the model architecture\n",
    "hidden_size = 512  # Adjust this to match the original hidden size\n",
    "named_entities_df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")\n",
    "named_entities = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n",
    "special_tokens = [f\"<<{entity}>>\" for entity in named_entities]\n",
    "special_token_size = len(special_tokens)  # Ensure you have the special tokens list\n",
    "biomedical_encoder = BiomedicalEncoder(hidden_size, special_token_size)\n",
    "\n",
    "base_model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "config = MarianConfig.from_pretrained(base_model_name)\n",
    "\n",
    "custom_model = CustomMarianMTModel(\n",
    "    config,  # Ensure you have access to the original config\n",
    "    hidden_size, \n",
    "    special_token_size, \n",
    "    biomedical_encoder\n",
    ")\n",
    "custom_model.resize_token_embeddings(118379)\n",
    "\n",
    "# Load the state dict\n",
    "model_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\custom_fine_tuned_marianmt\\\\embeddings-2\\\\checkpoint-10485\"\n",
    "checkpoint = torch.load(model_path + \"\\\\pytorch_model.bin\")\n",
    "custom_model.load_state_dict(checkpoint)\n",
    "\n",
    "# Move to GPU if needed\n",
    "custom_model = custom_model.to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "import torch\n",
    "\n",
    "def add_special_tokens(tokenizer, entities):\n",
    "    \"\"\"\n",
    "    Adds new entity tokens to the tokenizer if they are not already present.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to update.\n",
    "        entities (list of str): List of entity names to add as special tokens.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    special_tokens = [f\"<<{entity}>>\" for entity in entities]\n",
    "    added_tokens = [token for token in special_tokens if token not in tokenizer.get_vocab()]\n",
    "    if added_tokens:\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': added_tokens})\n",
    "        print(f\"Added new special tokens: {added_tokens}\")\n",
    "\n",
    "\n",
    "def translate_tokenized_dataset(model, tokenizer, tokenized_dataset, batch_size=32):\n",
    "    translations = []\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for i in range(0, len(tokenized_dataset), batch_size):\n",
    "        # Extract batch data\n",
    "        input_ids = tokenized_dataset[\"input_ids\"][i:i + batch_size]\n",
    "        attention_mask = tokenized_dataset[\"attention_mask\"][i:i + batch_size]\n",
    "        entity_ids = tokenized_dataset[\"entity_ids\"][i:i + batch_size]\n",
    "\n",
    "        # Convert to tensors with explicit type and device handling\n",
    "        input_ids = torch.tensor(input_ids, dtype=torch.long).to(model.device)\n",
    "        attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(model.device)\n",
    "        entity_ids = torch.tensor(entity_ids, dtype=torch.long).to(model.device)\n",
    "\n",
    "        # Debug print statements\n",
    "        print(f\"Batch {i//batch_size + 1}:\")\n",
    "        print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "        print(f\"Attention Mask shape: {attention_mask.shape}\")\n",
    "        print(f\"Entity IDs shape: {entity_ids.shape}\")\n",
    "        print(f\"Entity IDs min: {entity_ids.min()}, max: {entity_ids.max()}\")\n",
    "        print(f\"Model entity embedding size: {model.entity_embedding.num_embeddings}\")\n",
    "\n",
    "        # Validate entity_ids before generation\n",
    "        try:\n",
    "            # Check if all entity IDs are within the valid range\n",
    "            assert torch.all(entity_ids >= 0), \"Negative entity IDs found\"\n",
    "            assert torch.all(entity_ids < model.entity_embedding.num_embeddings), \"Out-of-bound entity IDs\"\n",
    "        except AssertionError as e:\n",
    "            print(f\"Entity ID validation error: {e}\")\n",
    "            # Skip this batch or handle the error as needed\n",
    "            continue\n",
    "\n",
    "        # Generate translations\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids, \n",
    "                    attention_mask=attention_mask, \n",
    "                    entity_ids=entity_ids\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Generation error in batch {i//batch_size + 1}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Decode translations\n",
    "        translated_batch = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n",
    "        translations.extend(translated_batch)\n",
    "\n",
    "    return translations\n",
    "\n",
    "\n",
    "# Define evaluation metrics\n",
    "def evaluate_model(predictions, references):\n",
    "    # Load the evaluation metrics\n",
    "    bleu_metric = load(\"bleu\")\n",
    "    rouge_metric = load(\"rouge\")\n",
    "    bertscore_metric = load(\"bertscore\")\n",
    "    ter_metric = load(\"ter\")\n",
    "\n",
    "    # Format references for metric calculation\n",
    "    # The expected format for BLEU, ROUGE, and BERTScore is a list of lists of strings\n",
    "    references = [[ref] for ref in references]\n",
    "\n",
    "    # Evaluate BLEU score\n",
    "    bleu_result = bleu_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    # Evaluate ROUGE score\n",
    "    rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    # Evaluate BERTScore\n",
    "    bertscore_result = bertscore_metric.compute(predictions=predictions, references=references, lang=\"en\")\n",
    "    \n",
    "    # Evaluate TER (Translation Edit Rate)\n",
    "    ter_result = ter_metric.compute(predictions=predictions, references=references)\n",
    "    \n",
    "    return {\n",
    "        \"BLEU\": bleu_result,\n",
    "        \"ROUGE\": rouge_result,\n",
    "        \"BERTScore\": bertscore_result,\n",
    "        \"TER\": ter_result,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_data(test_dataset, tokenizer, src_lang=\"chinese\", max_entities=5):\n",
    "    \"\"\"\n",
    "    Preprocess test data to tokenize inputs and add entity_ids for entity-based embeddings.\n",
    "    \"\"\"\n",
    "    def preprocess_function(examples):\n",
    "        src_sentences = examples[src_lang]\n",
    "        entities_list = examples.get(\"entities\", [[] for _ in src_sentences])\n",
    "        \n",
    "        processed_src = []\n",
    "        processed_entities = []\n",
    "        \n",
    "        for sentence, entities in zip(src_sentences, entities_list):\n",
    "            # Process source sentence (add markers for entities in vocabulary)\n",
    "            for entity in entities:\n",
    "                if f\"<<{entity}>>\" in tokenizer.get_vocab():\n",
    "                    sentence = sentence.replace(entity, f\"<<{entity}>>\")\n",
    "            processed_src.append(sentence)\n",
    "            \n",
    "            # Convert entities to token IDs (if in vocab)\n",
    "            entity_ids = [\n",
    "                tokenizer.convert_tokens_to_ids(f\"<<{entity}>>\") \n",
    "                if f\"<<{entity}>>\" in tokenizer.get_vocab() else 0\n",
    "                for entity in entities\n",
    "            ]\n",
    "            \n",
    "            # Pad or truncate entity_ids\n",
    "            entity_ids = entity_ids[:max_entities]\n",
    "            entity_ids += [0] * (max_entities - len(entity_ids))  # Pad with zeros\n",
    "            processed_entities.append(entity_ids)\n",
    "        \n",
    "        # Tokenize the processed source sentences\n",
    "        model_inputs = tokenizer(\n",
    "            processed_src,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Add entity_ids as a tensor to the inputs\n",
    "        model_inputs[\"entity_ids\"] = torch.tensor(processed_entities, dtype=torch.long)\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    # Apply the preprocessing function to the test dataset\n",
    "    processed_test_dataset = test_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        remove_columns=test_dataset.column_names,\n",
    "    )\n",
    "    \n",
    "    return processed_test_dataset\n",
    "\n",
    "\n",
    "def evaluate_model(model, tokenized_test_dataset, tokenizer, batch_size=16):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the tokenized test dataset.\n",
    "    \"\"\"\n",
    "    # Prepare DataLoader for test data\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        tokenized_test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: tokenizer.pad(batch, return_tensors=\"pt\")\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move inputs to GPU if available\n",
    "            inputs = {key: val.to(model.device) for key, val in batch.items() if key != \"labels\"}\n",
    "            \n",
    "            # Generate predictions\n",
    "            outputs = model.generate(**inputs)\n",
    "            predictions.extend(outputs)\n",
    "    \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2102/2102 [00:00<00:00, 7016.02 examples/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m evaluate_model(custom_model, tokenized_test_dataset, tokenizer)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Decode predictions to text\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m decoded_predictions \u001b[38;5;241m=\u001b[39m [\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_predictions)\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:247\u001b[0m, in \u001b[0;36mMarianTokenizer.decode\u001b[1;34m(self, token_ids, **kwargs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, token_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    224\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;124;03m    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m        `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:4002\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[1;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[0;32m   3981\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3982\u001b[0m \u001b[38;5;124;03mConverts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\u001b[39;00m\n\u001b[0;32m   3983\u001b[0m \u001b[38;5;124;03mtokens and clean up tokenization spaces.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m    `str`: The decoded sentence.\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m-> 4002\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4004\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(\n\u001b[0;32m   4005\u001b[0m     token_ids\u001b[38;5;241m=\u001b[39mtoken_ids,\n\u001b[0;32m   4006\u001b[0m     skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[0;32m   4007\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m   4008\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4009\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\utils\\generic.py:275\u001b[0m, in \u001b[0;36mto_py_obj\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n\u001b[1;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mframework_to_py_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mnumber):\n",
      "File \u001b[1;32mc:\\Users\\Gaming\\anaconda3\\envs\\266-env\\Lib\\site-packages\\transformers\\utils\\generic.py:260\u001b[0m, in \u001b[0;36mto_py_obj.<locals>.<lambda>\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_py_obj\u001b[39m(obj):\n\u001b[0;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     framework_to_py_obj \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: np\u001b[38;5;241m.\u001b[39masarray(obj)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m    264\u001b[0m     }\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mdict\u001b[39m, UserDict)):\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Load your test dataset\n",
    "test_dataset = load_dataset(\"parquet\", data_files={\"test\": \"nejm/nejm_test_entities.parquet\"})[\"test\"]\n",
    "\n",
    "# Preprocess and tokenize test data\n",
    "tokenized_test_dataset = preprocess_test_data(test_dataset, tokenizer)\n",
    "\n",
    "# Evaluate model on test data\n",
    "predictions = evaluate_model(custom_model, tokenized_test_dataset, tokenizer)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode predictions to text\n",
    "decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n",
    "print(\"Predictions:\", decoded_predictions[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(predictions, test_dataset[\"test\"][\"english\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 2102\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Decode predictions safely\u001b[39;00m\n\u001b[0;32m      2\u001b[0m decoded_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 3\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdecode(\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy(), skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions\n\u001b[0;32m      5\u001b[0m ]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(\"Predictions:\", decoded_predictions)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unknown error\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Decode predictions safely\n",
    "decoded_predictions = [\n",
    "    tokenizer.decode(pred.cpu().numpy(), skip_special_tokens=True)\n",
    "    for pred in predictions\n",
    "]\n",
    "# print(\"Predictions:\", decoded_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pd.DataFrame(data={\"english\": test_dataset[\"test\"][\"english\"], \"predicted_english\": predictions})\n",
    "outputs.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\predictions\\\\zh-en-test-working-model.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences for inference\n",
    "test_sentences = [\n",
    "    \"ATP is a molecule important for energy transfer.\",\n",
    "    \"The enzyme DNA polymerase synthesizes DNA molecules.\"\n",
    "]\n",
    "\n",
    "# Tokenize the test data\n",
    "tokenized_test_data = tokenize_data(tokenizer, test_sentences, named_entities)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(special_token_dataset)\n",
    "\n",
    "# Decode predictions\n",
    "decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(decoded_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset, load_dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "# fine_tuned_model_path = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\fine_tuned_marian_model-zh-en/checkpoint-10485\"\n",
    "# fine_tuned_model_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\custom_fine_tuned_marianmt\\\\first_run\\\\checkpoint-10485\"\n",
    "# custom_model_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\custom_fine_tuned_marianmt\\\\fixed_entities\\\\checkpoint-10485\"\n",
    "\n",
    "# tokenizer = MarianTokenizer.from_pretrained(custom_model_path)\n",
    "# custom_model = MarianMTModel.from_pretrained(custom_model_path)\n",
    "\n",
    "test_entities = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-test-entities.parquet\")\n",
    "test_named_entities = [\"\".join(x) for x in test_entities[\"tokens\"].tolist()]\n",
    "special_token_size = len(test_named_entities)\n",
    "\n",
    "add_special_tokens(tokenizer=tokenizer_2, entities=test_named_entities)\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(\"parquet\", data_files={\"test\": \"nejm/nejm_test_entities.parquet\"})[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate Chinese sentences to English\n",
    "# tokenizer, custom_model\n",
    "special_token_dataset = prepare_dataset(test_dataset, tokenizer_2, src_lang=\"chinese\", tgt_lang=\"english\", max_entities=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_english = translate_tokenized_dataset(model.to(\"cuda\"), tokenizer_2, special_token_dataset)\n",
    "\n",
    "predictions = pd.DataFrame(data={\"english\": test_dataset[\"test\"][\"english\"], \"predicted_english\": predicted_english})\n",
    "predictions.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\predictions\\\\zh-en-test-working-model.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace these file paths with your actual text files\n",
    "# parquet_file = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\nejm_test.parquet\"\n",
    "test_entities_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-test-entities.parquet\"\n",
    "test_entities = pd.read_parquet(test_entities_file)\n",
    "\n",
    "add_special_tokens(tokenizer=None)\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(\"parquet\", data_files={\"test\": \"nejm/nejm_train_entities.parquet\"})[\"test\"]\n",
    "\n",
    "# Translate Chinese sentences to English\n",
    "df[\"predicted_english\"] = translate_sentences(tokenizer, custom_model.to(\"cuda\"), df[\"chinese\"].tolist())\n",
    "# df.to_parquet(\"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\predictions/fine-tuned.parquet\")\n",
    "df.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\predictions/fine-tuned_zh-en.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the evaluation function\n",
    "evaluation_results = evaluate_model(df[\"predicted_english\"], df[\"english\"])\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzoqrYR7GLbf"
   },
   "source": [
    "# Fine-tuned models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYDt5yjDGMDi"
   },
   "source": [
    "## zh-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4iqskN2HRVL"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "# Load the pretrained MarianNMT model and tokenizer\n",
    "def load_marian_model_and_tokenizer(model_name):\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Prepare dataset\n",
    "def prepare_dataset(parquet_file, tokenizer, src_lang=\"chinese\", tgt_lang=\"english\"):\n",
    "    # Load dataset from Parquet\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "\n",
    "    # Tokenization function\n",
    "    def preprocess_function(examples):\n",
    "        model_inputs = tokenizer(\n",
    "            examples[src_lang],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            examples[tgt_lang],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    # Convert pandas DataFrame to Hugging Face Dataset\n",
    "    hf_dataset = Dataset.from_pandas(df)\n",
    "    tokenized_dataset = hf_dataset.map(preprocess_function, batched=True)\n",
    "    return tokenized_dataset\n",
    "\n",
    "# Train the model\n",
    "def fine_tune_model(model, tokenizer, tokenized_dataset, output_dir):\n",
    "    # Split dataset into train and validation\n",
    "    dataset = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "    # Training arguments\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=100,\n",
    "        predict_with_generate=True,\n",
    "        push_to_hub=False,\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    # Trainer setup\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423,
     "referenced_widgets": [
      "8e43d373ea134eab904d34a1ce9b4250",
      "16615c82c9a24594a6e6c2f1a4f6607b",
      "e88e154e6a584d8c9d9479fbd9c5e5d3",
      "e6f0712640e746dfb683989dea1b3e94",
      "952a003defdc41ac8bcac503dd6f8276",
      "8f1bf5a49f454b13b89b5bfc24cf6e54",
      "6886877033984300ba85c8cc9cf80a36",
      "16bea5c39ea24cb989e69e7c2d6066c3",
      "83b4e3a41f5f4430ac859258a6291e6c",
      "408cabacdcf14384b7bdbbc674493139",
      "16d2fdc3436a4344a936e8493850b220"
     ]
    },
    "id": "IxbxrAlyTHiV",
    "outputId": "ff885ebc-b017-4959-8a6a-eb50e5d73698"
   },
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define model and file paths\n",
    "    model_name = \"Helsinki-NLP/opus-mt-zh-en\"  # Adjust for en-zh if needed\n",
    "    # parquet_file = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\nejm_train.parquet\"\n",
    "    # output_dir = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\fine_tuned_marian_model-zh-en\"\n",
    "    parquet_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_train.parquet\"\n",
    "    output_dir = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\fine_tuned_marian_model-zh-en\"\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer, model = load_marian_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    tokenized_dataset = prepare_dataset(parquet_file, tokenizer)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    fine_tune_model(model, tokenizer, tokenized_dataset, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "d67DmjiGwh2q",
    "outputId": "d5e9308b-8270-4531-f46c-13f895e88fc3"
   },
   "outputs": [],
   "source": [
    "pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1L1lzzo3GPfB"
   },
   "source": [
    "## en-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576,
     "referenced_widgets": [
      "8c414892a5aa471dbd4a35dc9145b304",
      "6158858af342430383786e6cd636fbe3",
      "8594e9dd2cd34c4a8c31f072df490669",
      "48d92342685b4651a5935bc2d2e368b4",
      "8d98b199711a46f08bbbb0796cfd2a2c",
      "78ff9f405bc941988926e75e213eafc9",
      "a0520bda70e44cc3b88aebf070c2922b",
      "abbad1e8e7414c349d078b94ba06ab68",
      "7e8bd7fc18ce4fda8c656f813decfade",
      "fd975f2d4a3945c99cc296b4b15fad58",
      "c3af7379713f40f08c95a46b731f384b"
     ]
    },
    "id": "_OzvwFNmk60y",
    "outputId": "8cdb864c-36f1-4105-b9c7-f6e89a3fed95"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define model and file paths\n",
    "    model_name = \"Helsinki-NLP/opus-mt-en-zh\"  # Adjust for en-zh if needed\n",
    "    # parquet_file = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\nejm_train.parquet\"\n",
    "    # output_dir = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\fine_tuned_marian_model-en-zh\"\n",
    "    parquet_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_train.parquet\"\n",
    "    output_dir = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\fine_tuned_marian_model-en-zh\"\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    tokenizer, model = load_marian_model_and_tokenizer(model_name)\n",
    "\n",
    "    # Prepare the dataset\n",
    "    tokenized_dataset = prepare_dataset(parquet_file, tokenizer, src_lang=\"english\", tgt_lang=\"chinese\")\n",
    "\n",
    "    # Fine-tune the model\n",
    "    fine_tune_model(model, tokenizer, tokenized_dataset, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dx41ABTGUh6d"
   },
   "source": [
    "# Evaluate BLEU Score on fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUpGhsDfYVbV"
   },
   "source": [
    "## zh-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69J7ZiHaQNwY",
    "outputId": "f2cda922-d298-434b-edab-1fc7850f99bb"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "# fine_tuned_model_path = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\fine_tuned_marian_model-zh-en/checkpoint-10485\"\n",
    "fine_tuned_model_path = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\fine_tuned_marian_model-zh-en/checkpoint-10485\"\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "model = MarianMTModel.from_pretrained(fine_tuned_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G27vnTRUSmU"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "\n",
    "def load_marian_model(model_name):\n",
    "    # Load the MarianMT model and tokenizer\n",
    "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "    model = MarianMTModel.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "def add_special_tokens(tokenizer, entities):\n",
    "    \"\"\"\n",
    "    Adds new entity tokens to the tokenizer if they are not already present.\n",
    "\n",
    "    Args:\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer to update.\n",
    "        entities (list of str): List of entity names to add as special tokens.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    special_tokens = [f\"<<{entity}>>\" for entity in entities]\n",
    "    added_tokens = [token for token in special_tokens if token not in tokenizer.get_vocab()]\n",
    "    if added_tokens:\n",
    "        tokenizer.add_special_tokens({'additional_special_tokens': added_tokens})\n",
    "        print(f\"Added new special tokens: {added_tokens}\")\n",
    "\n",
    "\n",
    "def translate_sentences(tokenizer, model, sentences, batch_size=32):\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n",
    "        outputs = model.generate(**inputs)\n",
    "        translated_batch = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n",
    "        translations.extend(translated_batch)\n",
    "    return translations\n",
    "\n",
    "def calculate_bleu(predictions, references):\n",
    "    # Use sacrebleu to calculate the BLEU score\n",
    "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
    "    return bleu.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EUEyKMhYUZM1"
   },
   "outputs": [],
   "source": [
    "# Replace these file paths with your actual text files\n",
    "# parquet_file = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\nejm_test.parquet\"\n",
    "parquet_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_test.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Translate Chinese sentences to English\n",
    "df[\"predicted_english\"] = translate_sentences(tokenizer, model.to(\"cuda\"), df[\"chinese\"].tolist())\n",
    "# df.to_parquet(\"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\predictions/fine-tuned.parquet\")\n",
    "df.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\predictions/fine-tuned_zh-en.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnbauO6nPvtD",
    "outputId": "f2bb02dc-7f53-46a8-f4f4-dbc391648bfd"
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu(df[\"predicted_english\"].tolist(), df[\"english\"].tolist())\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "PkMTfqpTuS14",
    "outputId": "143b0224-9890-4724-8053-b84367c9098c"
   },
   "outputs": [],
   "source": [
    "test_dataset = load_dataset(\"parquet\", data_files={\"test\": \"nejm/nejm_train_entities.parquet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL-UbVc2YYQl"
   },
   "source": [
    "## en-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1ykSkhWYZhZ"
   },
   "outputs": [],
   "source": [
    "# Replace these file paths with your actual text files\n",
    "# parquet_file = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\nejm_test.parquet\"\n",
    "parquet_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_test.parquet\"\n",
    "\n",
    "df = pd.read_parquet(\n",
    "    parquet_file)\n",
    "\n",
    "# Translate Chinese sentences to English\n",
    "df[\"predicted_chinese\"] = translate_sentences(tokenizer, model.to(\"cuda\"), df[\"english\"].tolist())\n",
    "# df.to_parquet(\"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\predictions/fine-tuned.parquet\")\n",
    "df.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\predictions/fine-tuned_en-zh.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1mU7Be-WYgTP",
    "outputId": "64b01462-0f1d-4c55-fc05-8d92bc10eea5"
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu(df[\"predicted_chinese\"].tolist(), df[\"chinese\"].tolist())\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 276
    },
    "id": "vereahfQdjbb",
    "outputId": "5683a7c9-a2b8-45b9-d8c3-d95222732db5"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKtvhuZt4Qm-"
   },
   "source": [
    "# Evaluate BLEU Score on Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urNnm1BO6Lgc"
   },
   "source": [
    "## zh-en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404,
     "referenced_widgets": [
      "52d849131b0d4f1ba5ac6205f15f900c",
      "72631d13f3f64ce0a6986e47a658639a",
      "8ffe3f3740b64302b13d81dd43d6556b",
      "93772eacec864960baf965dd294c8950",
      "857723b3a7bb4c66add7e9337ce0fb35",
      "d007579803b946c086b8e28c991f78a5",
      "01984d65864948d4b5781f1875bc3244",
      "6e3231cced70478ba374b03aeed1593c",
      "3cdcca994e07414fae64f8d0f0f72e2f",
      "d475dc6dca8d4ae1aac02ada6d4db047",
      "152fdef31ed24c8999a47da9c59f29c0",
      "c3fef1be4ee046fdbae4af568134826a",
      "0d7ea464cfd44e42b3e1fefb117b7c26",
      "ff668eb8873745cf8fd398bf96634bab",
      "30c32c353ef143fcaed4a0f4735d8a0c",
      "77fb294192f74b379d85d3e03535e04c",
      "d91fd827015d484cad80a56efda1caba",
      "3a8ca44209314aebb22c5a240ae584cb",
      "88f9eed6736d47dcac2c451c348b044a",
      "802201dfcb1b406483ff0ee25dc48077",
      "ee41e199eb4c4fb0b531ff5204897027",
      "08af99cc79244c10b98dc1e3fa85b592",
      "735a7fbffae44fb598b5018da962b69d",
      "401cceb11b65497d883218572a1d5c2f",
      "767719c2b4254361b800a91e76d4b4ec",
      "c1acf167c2cd42f5aa6f0200bfb57611",
      "5a7174c9e9f641a39d8f6c35a332ba6a",
      "c436dcfb3e00413eafb14476b181acf8",
      "caee605432454911958820c92284189f",
      "d85e875ff7264026be65b1bee81e82c1",
      "38a8aa8b5d1942e28269ac3548b1cf9b",
      "8d9cf65834c54890acf2b7bb8740e94c",
      "037f94547f06499385df8c8b164621f7",
      "29419a45ca244422aaf356de314e80f8",
      "024b4d91fe4f4167b6ffb4f7e154f09a",
      "652527ee92284905b74ff7a3598e96f5",
      "105b154f868b4fe68b78a000f2d4eb7e",
      "acdc71ee6b3543ad8adeaa0e98acbdef",
      "261b29342339435f8aeb478ec7da3ba7",
      "ede7604fe1974c22b440419faeeebb76",
      "120b8879445642cc96088a69434a0d6f",
      "61f0a00a29da408e899a4951dda9eadf",
      "a17c733e4a9a4f36b333bd516e6079b2",
      "9dd2263ce2214add88ba9ab67697ede7",
      "f7ffdefb657b4a9ea36d05d1b1e8a125",
      "09bdde5f48974a0d98e909bf69d045f6",
      "3928bdd3a1e24b7bbbdad923aa63b08d",
      "4851647895334f498e8c54b01892b95d",
      "108bfcf9372a4d9ca362529c2781a895",
      "f4774bdc3fe742b4b15aab882aeec53f",
      "ab3af3f0d47b468fb0f56c06f20c2fb7",
      "5ecc0f50150c45b5ba1199ed56dc469c",
      "cfb8bb86bb404556a51703afeecded59",
      "2250d430ee41483b8846b9ce21d991f3",
      "bd93adebc80045c19104e9d54dd75e03",
      "15ef888aa1084ff99e5543d704bae9ae",
      "e845ba792d1942a49b9073099793b9ae",
      "60eaafe01559461f8510097927cb4ece",
      "0a511fc4d66c459ab867c50bb1807f2f",
      "83e7a4ef31ee4d6790f36df22c2d63c3",
      "35fa7eb0c57443d9ac549e63575a303c",
      "63fd6f14d7464054b9e9a7373d704e09",
      "497e1ed177664cd0bddc76195ace65b3",
      "fde1feff2d58464ca349ed4ae7b3b321",
      "247e387289424d9b956c8368d96d0abe",
      "d6e9e2ac999f44e29c6ade68e880197f",
      "d91d27dced67459c864d8f7e61882cca",
      "2e579baccd5d41ff8047169045e0e792",
      "307705e08963408f9829446ac1ca6a51",
      "3dd95a8045704c0699df8d0590e2fe2d",
      "7a580bab682140ccb95d14989ee36656",
      "9dcd0da6d499448780f40554d01e7fa5",
      "818a654426b64d30b84615b82706a0a1",
      "8c61b2f275164c52abbf489f8d15f5f9",
      "a37c52f3b6bf45e68c439112107c9f34",
      "4a118779336b48d48bb76fc9cb76641a",
      "c43d985d8c84423a9c289a10fc6f57bf"
     ]
    },
    "id": "auyVGPF44QIL",
    "outputId": "1eac910d-a032-4e02-a473-da144c4f0ad3"
   },
   "outputs": [],
   "source": [
    "# Replace these file paths with your actual text files\n",
    "# parquet_file = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\nejm_test.parquet\"\n",
    "parquet_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_test.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Load MarianNMT pretrained model for zh-en translation\n",
    "baseline_model_name = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "baseline_tokenizer, baseline_model = load_marian_model(baseline_model_name)\n",
    "\n",
    "# Translate Chinese sentences to English\n",
    "df[\"predicted_english\"] = translate_sentences(baseline_tokenizer, baseline_model.to(\"cuda\"), df[\"chinese\"].tolist())\n",
    "# df.to_parquet(\"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\predictions/fine-tuned.parquet\")\n",
    "df.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\predictions\\\\baseline_zh-en.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vegUoJM343LK",
    "outputId": "62727aa0-1bfd-468d-8767-0c876a3ccd3d"
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu(df[\"predicted_english\"].tolist(), df[\"english\"].tolist())\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "6B-DKddw44i-",
    "outputId": "788699b9-89a1-45b9-b2e9-4369fb0d0b24"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sa4dHZWI6SD7"
   },
   "source": [
    "## en-zh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22XTzOiB6UHS",
    "outputId": "e1efb9d9-a3e9-4bb3-b6c6-538b19727543"
   },
   "outputs": [],
   "source": [
    "# Replace these file paths with your actual text files\n",
    "# parquet_file = \"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\nejm_test.parquet\"\n",
    "parquet_file = \"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_test.parquet\"\n",
    "\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Load MarianNMT pretrained model for en-zh translation\n",
    "baseline_model_name = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "baseline_tokenizer, baseline_model = load_marian_model(baseline_model_name)\n",
    "\n",
    "# Translate Chinese sentences to English\n",
    "df[\"predicted_chinese\"] = translate_sentences(baseline_tokenizer, baseline_model.to(\"cuda\"), df[\"english\"].tolist())\n",
    "# df.to_parquet(\"/content/drive/MyDrive/Colab Notebooks/corpora/nejm\\\\predictions/fine-tuned.parquet\")\n",
    "df.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\predictions/baseline_en-zh.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-WIma5X6gpm",
    "outputId": "d27d898e-cd89-406d-d627-f67edc77af66"
   },
   "outputs": [],
   "source": [
    "# Calculate BLEU score\n",
    "bleu_score = calculate_bleu(df[\"predicted_chinese\"].tolist(), df[\"chinese\"].tolist())\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "Z2BA9tMX6hFb",
    "outputId": "90ef5abc-eac5-46e6-ca53-ea0586ec98a8"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnPrcqE39_W0"
   },
   "source": [
    "# NER using pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOp5HVm9FXie"
   },
   "outputs": [],
   "source": [
    "# https://huggingface.co/lixin12345/chinese-medical-ner\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "class NER:\n",
    "    \"\"\"\n",
    "    å®žä½“å‘½åå®žä½“è¯†åˆ«\n",
    "    \"\"\"\n",
    "    def __init__(self,model_path) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_path:æ¨¡åž‹åœ°å€\n",
    "        \"\"\"\n",
    "\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "    def ner(self,sentence:str) -> list:\n",
    "        \"\"\"\n",
    "        å‘½åå®žä½“è¯†åˆ«\n",
    "        Args:\n",
    "            sentence:è¦è¯†åˆ«çš„å¥å­\n",
    "        Return:\n",
    "            å®žä½“åˆ—è¡¨:[{'type':'LOC','tokens':[...]},...]\n",
    "        \"\"\"\n",
    "        ans = []\n",
    "        for i in range(0,len(sentence),500):\n",
    "            ans = ans + self._ner(sentence[i:i+500])\n",
    "        return ans\n",
    "    \n",
    "    def _ner(self,sentence:str) -> list:\n",
    "        if len(sentence) == 0: return []\n",
    "        inputs = self.tokenizer(\n",
    "            sentence, add_special_tokens=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.model = self.model.to(torch.device('cuda:0'))\n",
    "            for key in inputs:\n",
    "                inputs[key] = inputs[key].to(torch.device('cuda:0'))\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits\n",
    "        predicted_token_class_ids = logits.argmax(-1)\n",
    "        predicted_tokens_classes = [self.model.config.id2label[t.item()] for t in predicted_token_class_ids[0]]\n",
    "        entities = []\n",
    "        entity = {}\n",
    "        for idx, token in enumerate(self.tokenizer.tokenize(sentence,add_special_tokens=True)):\n",
    "            if 'B-' in predicted_tokens_classes[idx] or 'S-' in predicted_tokens_classes[idx]:\n",
    "                if len(entity) != 0:\n",
    "                    entities.append(entity)\n",
    "                entity = {}\n",
    "                entity['type'] = predicted_tokens_classes[idx].replace('B-','').replace('S-','')\n",
    "                entity['tokens'] = [token]\n",
    "            elif 'I-' in predicted_tokens_classes[idx] or 'E-' in predicted_tokens_classes[idx] or 'M-' in predicted_tokens_classes[idx]:\n",
    "                if len(entity) == 0:\n",
    "                    entity['type'] = predicted_tokens_classes[idx].replace('I-','').replace('E-','').replace('M-','')\n",
    "                    entity['tokens'] = []\n",
    "                entity['tokens'].append(token)\n",
    "            else:\n",
    "                if len(entity) != 0:\n",
    "                    entities.append(entity)\n",
    "                    entity = {}\n",
    "        if len(entity) > 0:\n",
    "            entities.append(entity)\n",
    "        return entities\n",
    "\n",
    "ner_model = NER('lixin12345/chinese-medical-ner')\n",
    "text = \"\"\"\n",
    "æ‚£è€…æ—¢å¾€æ…¢é˜»è‚ºå¤šå¹´;å† å¿ƒç—…å²6å¹´ï¼Œå¹³ç´ è§„å¾‹æœç”¨å¿ƒå¯èˆ’ã€ä¿å¿ƒä¸¸ç­‰æŽ§åˆ¶å¯;åŒä¸‹è‚¢é™è„‰è¡€æ “3å¹´ï¼Œä¿å®ˆæ²»ç–—æ•ˆæžœå¯;å·¦ä¾§è…¹è‚¡æ²Ÿæ–œç–æ— å¼ åŠ›ä¿®è¡¥æœ¯åŽ2å¹´ã€‚å¦è®¤\"é«˜è¡€åŽ‹ã€ç³–å°¿ç—…\"ç­‰æ…¢æ€§ç—…ç—…å²ï¼Œå¦è®¤\"è‚ç‚Žã€ç»“æ ¸\"ç­‰ä¼ æŸ“ç—…ç—…å²åŠå…¶å¯†åˆ‡æŽ¥è§¦å²ï¼Œå¦è®¤å…¶ä»–æ‰‹æœ¯ã€é‡å¤§å¤–ä¼¤ã€è¾“è¡€å²ï¼Œå¦è®¤\"é£Ÿç‰©ã€è¯ç‰©ã€å…¶ä»–\"ç­‰è¿‡æ•å²ï¼Œé¢„é˜²æŽ¥ç§å²éšç¤¾ä¼šã€‚\n",
    "\"\"\"\n",
    "ans = ner_model.ner(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "# directory = os.path.join()\n",
    "\n",
    "# Sample biomedical text in Chinese\n",
    "df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_train.parquet\")\n",
    "texts = df.chinese.tolist()\n",
    "# Extract entities\n",
    "chinese_entities = []\n",
    "for text in tqdm.tqdm(texts):\n",
    "    # print(f\"Text: {text}\")\n",
    "    entities = ner_model.ner(text)\n",
    "    for entity in entities:\n",
    "        # print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "        chinese_entities.append(entity)\n",
    "df_entities = pd.DataFrame(chinese_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\zh-entities.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities.type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\") # pass device=0 if using gpu\n",
    "pipe(\"\"\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model\n",
    "model_path = \"venkatd/BIOMed_NER\"\n",
    "pipe = pipeline(\n",
    "    task=\"token-classification\",\n",
    "    model=model_path,\n",
    "    tokenizer=model_path,\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Test the pipeline\n",
    "text = (\"A 48-year-old female presented with vaginal bleeding and abnormal Pap smears. \"\n",
    "        \"Upon diagnosis of invasive non-keratinizing SCC of the cervix, she underwent a radical \"\n",
    "        \"hysterectomy with salpingo-oophorectomy which demonstrated positive spread to the pelvic \"\n",
    "        \"lymph nodes and the parametrium.\")\n",
    "result = pipe(text)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "\n",
    "# directory = os.path.join()\n",
    "\n",
    "# Sample biomedical text in Chinese\n",
    "df = pd.read_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\nejm_train.parquet\")\n",
    "texts = df.english.tolist()\n",
    "# Extract entities\n",
    "english_entities = []\n",
    "for text in tqdm.tqdm(texts):\n",
    "    # print(f\"Text: {text}\")\n",
    "    entities = pipe(text)\n",
    "    for entity in entities:\n",
    "        # print(f\"Entity: {entity['word']}, Label: {entity['entity_group']}, Score: {entity['score']:.4f}\")\n",
    "        english_entities.append(entity)\n",
    "df_entities_en = pd.DataFrame(english_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entities_en.to_parquet(\"C:\\\\Users\\\\Gaming\\\\Documents\\\\GitHub\\\\MIE2\\\\2024-fall-assignment-linaaron88\\\\project\\\\nejm\\\\en-entities.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe(\"\"\"The patient reported no recurrence of palpitations at follow-up 6 months after the ablation.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge graphs (Unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4t5qX6BFXST"
   },
   "outputs": [],
   "source": [
    "def construct_translation_kg(entity_pairs):\n",
    "    translation_kg = []\n",
    "    for source_entity, target_entity in entity_pairs:\n",
    "        triple = (source_entity, \"<t>\", target_entity)\n",
    "        translation_kg.append(triple)\n",
    "    return translation_kg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_knowledge_graph(Ks, Kt, Kp):\n",
    "    return {\"Ks\": Ks, \"Kt\": Kt, \"Kp\": Kp}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_sentence_to_kg(sentence, kg):\n",
    "    # Extract local KG for a sentence\n",
    "    local_kg = []\n",
    "    for entity in extract_entities(sentence, language=\"source\"):\n",
    "        for triple in kg:\n",
    "            if entity in triple:  # Match based on entity presence\n",
    "                local_kg.append(triple)\n",
    "    return local_kg\n",
    "\n",
    "def extract_local_knowledge_graphs(sentence, unified_kg):\n",
    "    Ks_local = match_sentence_to_kg(sentence, unified_kg[\"Ks\"])\n",
    "    Kp_local = match_sentence_to_kg(sentence, unified_kg[\"Kp\"])\n",
    "    \n",
    "    # Match tail entities in Kp_local to Kt\n",
    "    tail_entities = [triple[2] for triple in Kp_local]  # Extract tail entities\n",
    "    Kt_local = [triple for triple in unified_kg[\"Kt\"] if triple[0] in tail_entities]\n",
    "    \n",
    "    return {\"Ks_local\": Ks_local, \"Kp_local\": Kp_local, \"Kt_local\": Kt_local}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ks = [...]  # Triples from source KG (e.g., YAGO)\n",
    "Kt = [...]  # Triples from target KG (e.g., CN-DBpedia)\n",
    "Kp = construct_translation_kg(entity_pairs)  # Entity pairs (from IBM model)\n",
    "\n",
    "unified_kg = unify_knowledge_graph(Ks, Kt, Kp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"å’–å•¡å› å¯èƒ½å¯¼è‡´å¿ƒæ‚¸å’Œå‡é‡\"\n",
    "local_kg = extract_local_knowledge_graphs(sentence, unified_kg)\n",
    "\n",
    "print(local_kg[\"Ks_local\"])  # Local Source KG\n",
    "print(local_kg[\"Kp_local\"])  # Local Translation KG\n",
    "print(local_kg[\"Kt_local\"])  # Local Target KG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hq6k-RoTBxUT"
   },
   "source": [
    "## Generate translation knowledge graph Kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_3W_KaY9-xQ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Load the Parquet file with the parallel corpus directly into Pandas\n",
    "df_pandas = pd.read_parquet('path_to_your_parquet_file')\n",
    "\n",
    "# Example of your dataframe: columns \"source\" (Chinese) and \"target\" (English)\n",
    "# Let's inspect the first few rows to see the data format\n",
    "print(df_pandas.head())\n",
    "\n",
    "# Tokenize the sentences in the source (Chinese) and target (English)\n",
    "nltk.download('punkt')  # If you don't have the Punkt tokenizer models already\n",
    "df_pandas['source_tokens'] = df_pandas['source'].apply(lambda x: word_tokenize(x))\n",
    "df_pandas['target_tokens'] = df_pandas['target'].apply(lambda x: word_tokenize(x))\n",
    "\n",
    "# Create Translation Knowledge Graph (Kp) from word-level alignments\n",
    "def create_translation_triples(row):\n",
    "    source_tokens = row['source_tokens']\n",
    "    target_tokens = row['target_tokens']\n",
    "\n",
    "    translation_triples = []\n",
    "    for s, t in zip(source_tokens, target_tokens):\n",
    "        # Assuming word alignment: (source_word, \"<t>\", target_word)\n",
    "        translation_triples.append((s, \"<t>\", t))\n",
    "\n",
    "    return translation_triples\n",
    "\n",
    "# Apply the function to create translation triples for each sentence pair\n",
    "df_pandas['translation_triples'] = df_pandas.apply(create_translation_triples, axis=1)\n",
    "\n",
    "# Flatten the translation triples and convert to a list of (source, relation, target)\n",
    "all_triples = []\n",
    "for triples in df_pandas['translation_triples']:\n",
    "    all_triples.extend(triples)\n",
    "\n",
    "# Convert the list of triples into a pandas DataFrame for easier storage/analysis\n",
    "triples_df = pd.DataFrame(all_triples, columns=['source_word', 'relation', 'target_word'])\n",
    "\n",
    "# Example: show the first few translation triples\n",
    "print(triples_df.head())\n",
    "\n",
    "# Now you have the translation knowledge graph Kp\n",
    "# You can save this knowledge graph as a CSV or other formats for use in your model\n",
    "triples_df.to_csv('translation_knowledge_graph.csv', index=False)\n",
    "\n",
    "# Or, you can store it in a more structured format like a dictionary or a custom KG\n",
    "# Example: saving the knowledge graph as a list of dictionaries\n",
    "kp_dict = triples_df.to_dict(orient='records')\n",
    "print(kp_dict[:5])  # Print first 5 entries of the dictionary format\n",
    "\n",
    "# The Kp can now be passed into the model for further processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IfQt-9_H9-bO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz-3lzms_PWS"
   },
   "source": [
    "# Knowledge Guided Transformer for NMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdEAznQp_VGm"
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class KnowledgeEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This custom class encodes knowledge from KGs to be passed into a transformer model alongside\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, num_layers):\n",
    "        super(KnowledgeEncoder, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=8), num_layers=num_layers\n",
    "        )\n",
    "\n",
    "    def forward(self, knowledge_triples):\n",
    "        \"\"\"\n",
    "        knowledge_triples: The encoded knowledge triples (Ksel) passed as input\n",
    "        to the knowledge encoder.\n",
    "        \"\"\"\n",
    "        return self.transformer_encoder(knowledge_triples)\n",
    "\n",
    "class KnowledgeAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(KnowledgeAttention, self).__init__()\n",
    "        self.query_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        \"\"\"\n",
    "        query, key, value: hidden states from the source encoder and knowledge encoder.\n",
    "        \"\"\"\n",
    "        q = self.query_linear(query)\n",
    "        k = self.key_linear(key)\n",
    "        v = self.value_linear(value)\n",
    "\n",
    "        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / (key.size(-1) ** 0.5)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        return torch.matmul(attention_weights, v)\n",
    "\n",
    "class KnowledgeGuidedMTModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_size=512, num_layers=6):\n",
    "        super(KnowledgeGuidedMTModel, self).__init__()\n",
    "        self.transformer = MarianMTModel.from_pretrained(pretrained_model_name)\n",
    "        self.tokenizer = MarianTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "        # Knowledge Encoder\n",
    "        self.knowledge_encoder = KnowledgeEncoder(hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        # Knowledge Attention mechanism\n",
    "        self.knowledge_attention = KnowledgeAttention(hidden_size=hidden_size)\n",
    "\n",
    "    def forward(self, input_ids, knowledge_triples, decoder_input_ids=None, attention_mask=None):\n",
    "        \"\"\"\n",
    "        input_ids: Tokenized source sentence.\n",
    "        knowledge_triples: Knowledge triples (Ksel) in the form of token embeddings.\n",
    "        decoder_input_ids: Target sentence (used in training with teacher forcing).\n",
    "        attention_mask: Attention mask for the encoder-decoder.\n",
    "        \"\"\"\n",
    "        # Source sentence encoding using the pretrained MarianMT model\n",
    "        source_encoder_output = self.transformer.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        source_hidden_states = source_encoder_output.last_hidden_state  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Knowledge encoding\n",
    "        knowledge_hidden_states = self.knowledge_encoder(knowledge_triples)  # (batch_size, num_triples, hidden_size)\n",
    "\n",
    "        # Decoder input - Knowledge attention mechanism\n",
    "        if decoder_input_ids is not None:\n",
    "            decoder_input = self.transformer.decoder(\n",
    "                input_ids=decoder_input_ids,\n",
    "                encoder_hidden_states=source_hidden_states,\n",
    "                encoder_attention_mask=attention_mask\n",
    "            )\n",
    "            decoder_hidden_states = decoder_input[0]\n",
    "            query = decoder_hidden_states[-1]  # Last token's hidden state as the query\n",
    "\n",
    "            # Apply knowledge attention layer\n",
    "            knowledge_attended = self.knowledge_attention(query, knowledge_hidden_states, knowledge_hidden_states)\n",
    "            final_hidden_state = decoder_hidden_states + knowledge_attended\n",
    "        else:\n",
    "            final_hidden_state = source_hidden_states\n",
    "\n",
    "        # Generate translation using the final hidden state\n",
    "        output = self.transformer.lm_head(final_hidden_state)\n",
    "        return output\n",
    "\n",
    "    def generate(self, input_text, knowledge_triples):\n",
    "        input_ids = self.tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "        knowledge_triples = torch.tensor(knowledge_triples)  # Assuming this is tokenized\n",
    "        generated_ids = self.generate_from_input(input_ids, knowledge_triples)\n",
    "        return self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    def generate_from_input(self, input_ids, knowledge_triples):\n",
    "        # Generate output for the given input, integrating the knowledge graph\n",
    "        decoder_input_ids = torch.tensor([self.tokenizer.bos_token_id]).unsqueeze(0)  # Start token for the decoder\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "\n",
    "        output = self(input_ids, knowledge_triples, decoder_input_ids=decoder_input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        return predicted_ids\n",
    "\n",
    "# Example usage:\n",
    "pretrained_model_name = 'Helsinki-NLP/opus-mt-en-de'  # Example pretrained MarianMT model\n",
    "model = KnowledgeGuidedMTModel(pretrained_model_name)\n",
    "\n",
    "# Example knowledge triples (source and target language tokens)\n",
    "knowledge_triples = torch.tensor([[1, 2, 3], [4, 5, 6]])  # These should be tokenized triples (example)\n",
    "\n",
    "# Input sentence\n",
    "input_sentence = \"This is a test sentence.\"\n",
    "\n",
    "# Generate translation\n",
    "translated = model.generate(input_sentence, knowledge_triples)\n",
    "print(f\"Translated: {translated}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7kNezJBBtwY"
   },
   "source": [
    "# Unused WMT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XD7tQHPG0lBb"
   },
   "outputs": [],
   "source": [
    "\"\"\"ONLY RUN ONCE: LOAD DATASET AND SAVE TO PARQUET\"\"\"\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import glob\n",
    "\n",
    "# # Function to load and convert text files into a DataFrame\n",
    "# def convert_to_parquet(data_dir, suffix_en, suffix_zh, output_file):\n",
    "#     all_files = glob.glob(os.path.join(data_dir, \"*.txt\"))\n",
    "#     data = []\n",
    "\n",
    "#     for file in all_files:\n",
    "#         if file.endswith(suffix_en):\n",
    "#             base_name = file[:-len(suffix_en)]\n",
    "#             zh_file = base_name + suffix_zh\n",
    "#             if zh_file in all_files:\n",
    "#                 # Read file contents\n",
    "#                 with open(file, 'r', encoding='utf-8') as en_f, open(zh_file, 'r', encoding='utf-8') as zh_f:\n",
    "#                     en_text = en_f.read().strip()\n",
    "#                     zh_text = zh_f.read().strip()\n",
    "#                     data.append({\"english\": en_text, \"chinese\": zh_text})\n",
    "\n",
    "#     # Convert to DataFrame\n",
    "#     df = pd.DataFrame(data)\n",
    "\n",
    "#     # Save as Parquet file\n",
    "#     df.to_parquet(output_file, engine=\"pyarrow\", compression=\"snappy\")\n",
    "#     print(f\"Data saved to {output_file}\")\n",
    "\n",
    "# # Parameters\n",
    "# data_dir = \"data/\"\n",
    "# suffix_en = \"_en.txt\"\n",
    "# suffix_zh = \"_zh-cn.txt\"\n",
    "# output_file = os.path.join(data_dir, \"wmt22_dataset.parquet\")\n",
    "\n",
    "# convert_to_parquet(data_dir, suffix_en, suffix_zh, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LF24B4X20wrl",
    "outputId": "556203d0-47c8-47f9-b59f-1500c86f4943"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load Parquet file into Pandas\n",
    "parquet_file = os.path.join(data_dir, \"wmt22_dataset.parquet\")\n",
    "df = pd.read_parquet(parquet_file)\n",
    "\n",
    "# Convert DataFrame to TensorFlow Dataset\n",
    "def pandas_to_tf_dataset(df):\n",
    "    return tf.data.Dataset.from_tensor_slices((df[\"english\"].values, df[\"chinese\"].values))\n",
    "\n",
    "dataset = pandas_to_tf_dataset(df)\n",
    "\n",
    "# Preview the dataset\n",
    "for en_text, zh_text in dataset.take(5):\n",
    "    print(f\"English: {en_text.numpy().decode('utf-8')}\")\n",
    "    print(f\"Chinese: {zh_text.numpy().decode('utf-8')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uM0JOu1hNyHR"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-5XpRqTNyd8"
   },
   "outputs": [],
   "source": [
    "# # Split the data into train (80%), val (10%), test (10%)\n",
    "# splits = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "# train_val_split = splits[\"train\"].train_test_split(test_size=0.125, seed=42)  # 10% of original for validation\n",
    "\n",
    "# # Assign splits\n",
    "# train_data = train_val_split[\"train\"]\n",
    "# val_data = train_val_split[\"test\"]\n",
    "# test_data = splits[\"test\"]\n",
    "\n",
    "# print(f\"Training size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "266-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01984d65864948d4b5781f1875bc3244": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "024b4d91fe4f4167b6ffb4f7e154f09a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_261b29342339435f8aeb478ec7da3ba7",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ede7604fe1974c22b440419faeeebb76",
      "value": "vocab.json:â€‡100%"
     }
    },
    "037f94547f06499385df8c8b164621f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "068a25f097614e3a9de43a046ac862bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "087dd94f47e34ac58a3472c78d451ddd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08af99cc79244c10b98dc1e3fa85b592": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09bdde5f48974a0d98e909bf69d045f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4774bdc3fe742b4b15aab882aeec53f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ab3af3f0d47b468fb0f56c06f20c2fb7",
      "value": "config.json:â€‡100%"
     }
    },
    "0a511fc4d66c459ab867c50bb1807f2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_247e387289424d9b956c8368d96d0abe",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_d6e9e2ac999f44e29c6ade68e880197f",
      "value": "â€‡312M/312Mâ€‡[00:01&lt;00:00,â€‡189MB/s]"
     }
    },
    "0d7ea464cfd44e42b3e1fefb117b7c26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d91fd827015d484cad80a56efda1caba",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3a8ca44209314aebb22c5a240ae584cb",
      "value": "source.spm:â€‡100%"
     }
    },
    "0ff1e2eb9ce6461693bad31e61b13961": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "105b154f868b4fe68b78a000f2d4eb7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a17c733e4a9a4f36b333bd516e6079b2",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_9dd2263ce2214add88ba9ab67697ede7",
      "value": "â€‡1.62M/1.62Mâ€‡[00:00&lt;00:00,â€‡6.16MB/s]"
     }
    },
    "108bfcf9372a4d9ca362529c2781a895": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "120b8879445642cc96088a69434a0d6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1357a8e92e074d39bf7c4c5f56cec350": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "152fdef31ed24c8999a47da9c59f29c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "15ef888aa1084ff99e5543d704bae9ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e845ba792d1942a49b9073099793b9ae",
       "IPY_MODEL_60eaafe01559461f8510097927cb4ece",
       "IPY_MODEL_0a511fc4d66c459ab867c50bb1807f2f"
      ],
      "layout": "IPY_MODEL_83e7a4ef31ee4d6790f36df22c2d63c3"
     }
    },
    "16615c82c9a24594a6e6c2f1a4f6607b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f1bf5a49f454b13b89b5bfc24cf6e54",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_6886877033984300ba85c8cc9cf80a36",
      "value": "Map:â€‡100%"
     }
    },
    "16bea5c39ea24cb989e69e7c2d6066c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16d2fdc3436a4344a936e8493850b220": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2250d430ee41483b8846b9ce21d991f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "247e387289424d9b956c8368d96d0abe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "261b29342339435f8aeb478ec7da3ba7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "29419a45ca244422aaf356de314e80f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_024b4d91fe4f4167b6ffb4f7e154f09a",
       "IPY_MODEL_652527ee92284905b74ff7a3598e96f5",
       "IPY_MODEL_105b154f868b4fe68b78a000f2d4eb7e"
      ],
      "layout": "IPY_MODEL_acdc71ee6b3543ad8adeaa0e98acbdef"
     }
    },
    "2bf1a0ea3d0147e2abc18d728e3355f7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e579baccd5d41ff8047169045e0e792": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9dcd0da6d499448780f40554d01e7fa5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_818a654426b64d30b84615b82706a0a1",
      "value": "generation_config.json:â€‡100%"
     }
    },
    "307705e08963408f9829446ac1ca6a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c61b2f275164c52abbf489f8d15f5f9",
      "max": 293,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a37c52f3b6bf45e68c439112107c9f34",
      "value": 293
     }
    },
    "30c32c353ef143fcaed4a0f4735d8a0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee41e199eb4c4fb0b531ff5204897027",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_08af99cc79244c10b98dc1e3fa85b592",
      "value": "â€‡805k/805kâ€‡[00:00&lt;00:00,â€‡3.11MB/s]"
     }
    },
    "31a2149d084c401a942284aa074ce9f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "35fa7eb0c57443d9ac549e63575a303c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38a8aa8b5d1942e28269ac3548b1cf9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3928bdd3a1e24b7bbbdad923aa63b08d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ecc0f50150c45b5ba1199ed56dc469c",
      "max": 1394,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cfb8bb86bb404556a51703afeecded59",
      "value": 1394
     }
    },
    "3a87c9a7ae41446b8b8838ba779bf91d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f89f703aa3e4b1ea8fa85a8d355dc81",
      "max": 213450,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0895dbce7ac4bca8456c97826b1e6f6",
      "value": 213450
     }
    },
    "3a8ca44209314aebb22c5a240ae584cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c1b7a7407fb400c91a40650d530be60": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3cdcca994e07414fae64f8d0f0f72e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3dd95a8045704c0699df8d0590e2fe2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a118779336b48d48bb76fc9cb76641a",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c43d985d8c84423a9c289a10fc6f57bf",
      "value": "â€‡293/293â€‡[00:00&lt;00:00,â€‡17.8kB/s]"
     }
    },
    "401cceb11b65497d883218572a1d5c2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c436dcfb3e00413eafb14476b181acf8",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_caee605432454911958820c92284189f",
      "value": "target.spm:â€‡100%"
     }
    },
    "408cabacdcf14384b7bdbbc674493139": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43868398b2f44529bfd41f873e8cb171": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4851647895334f498e8c54b01892b95d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2250d430ee41483b8846b9ce21d991f3",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_bd93adebc80045c19104e9d54dd75e03",
      "value": "â€‡1.39k/1.39kâ€‡[00:00&lt;00:00,â€‡103kB/s]"
     }
    },
    "48d92342685b4651a5935bc2d2e368b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd975f2d4a3945c99cc296b4b15fad58",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_c3af7379713f40f08c95a46b731f384b",
      "value": "â€‡62127/62127â€‡[00:26&lt;00:00,â€‡2698.26â€‡examples/s]"
     }
    },
    "497e1ed177664cd0bddc76195ace65b3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a118779336b48d48bb76fc9cb76641a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52d849131b0d4f1ba5ac6205f15f900c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72631d13f3f64ce0a6986e47a658639a",
       "IPY_MODEL_8ffe3f3740b64302b13d81dd43d6556b",
       "IPY_MODEL_93772eacec864960baf965dd294c8950"
      ],
      "layout": "IPY_MODEL_857723b3a7bb4c66add7e9337ce0fb35"
     }
    },
    "5a7174c9e9f641a39d8f6c35a332ba6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d6b657822a84ed48348b9cdf43b8b1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43868398b2f44529bfd41f873e8cb171",
      "max": 435780550,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_31a2149d084c401a942284aa074ce9f1",
      "value": 435780550
     }
    },
    "5e3a27ac4f9f486da839aae8021fd882": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5ecc0f50150c45b5ba1199ed56dc469c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5fe98b6ebe014d9b938734756e653454": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e39679e6c0a1498da70bee68f8911a73",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_068a25f097614e3a9de43a046ac862bc",
      "value": "â€‡213k/213kâ€‡[00:00&lt;00:00,â€‡2.82MB/s]"
     }
    },
    "60eaafe01559461f8510097927cb4ece": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_497e1ed177664cd0bddc76195ace65b3",
      "max": 312087009,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fde1feff2d58464ca349ed4ae7b3b321",
      "value": 312087009
     }
    },
    "6158858af342430383786e6cd636fbe3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78ff9f405bc941988926e75e213eafc9",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_a0520bda70e44cc3b88aebf070c2922b",
      "value": "Map:â€‡100%"
     }
    },
    "61f0a00a29da408e899a4951dda9eadf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "635d5f4e415d43f2a65f0a097375539c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6ed58b1c767e4fe286dbd1f9fc2582af",
       "IPY_MODEL_5d6b657822a84ed48348b9cdf43b8b1b",
       "IPY_MODEL_be17a3e9f49d4cb4974d844ca645f215"
      ],
      "layout": "IPY_MODEL_a972ff589aa2424b86734ae5b2df8729"
     }
    },
    "63fd6f14d7464054b9e9a7373d704e09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "652527ee92284905b74ff7a3598e96f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_120b8879445642cc96088a69434a0d6f",
      "max": 1617902,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_61f0a00a29da408e899a4951dda9eadf",
      "value": 1617902
     }
    },
    "6886877033984300ba85c8cc9cf80a36": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e3231cced70478ba374b03aeed1593c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ed58b1c767e4fe286dbd1f9fc2582af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7471235d72e244e1aadc0db68d5d4555",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_2bf1a0ea3d0147e2abc18d728e3355f7",
      "value": "pytorch_model.bin:â€‡100%"
     }
    },
    "701211ffa6134025a064ac529786b0bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72631d13f3f64ce0a6986e47a658639a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d007579803b946c086b8e28c991f78a5",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_01984d65864948d4b5781f1875bc3244",
      "value": "tokenizer_config.json:â€‡100%"
     }
    },
    "735a7fbffae44fb598b5018da962b69d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_401cceb11b65497d883218572a1d5c2f",
       "IPY_MODEL_767719c2b4254361b800a91e76d4b4ec",
       "IPY_MODEL_c1acf167c2cd42f5aa6f0200bfb57611"
      ],
      "layout": "IPY_MODEL_5a7174c9e9f641a39d8f6c35a332ba6a"
     }
    },
    "7471235d72e244e1aadc0db68d5d4555": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "767719c2b4254361b800a91e76d4b4ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d85e875ff7264026be65b1bee81e82c1",
      "max": 806530,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38a8aa8b5d1942e28269ac3548b1cf9b",
      "value": 806530
     }
    },
    "77fb294192f74b379d85d3e03535e04c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78ff9f405bc941988926e75e213eafc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a580bab682140ccb95d14989ee36656": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e8bd7fc18ce4fda8c656f813decfade": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7f89f703aa3e4b1ea8fa85a8d355dc81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "802201dfcb1b406483ff0ee25dc48077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "818a654426b64d30b84615b82706a0a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "831f8d7e65734a36b34f868dfa806be0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83b4e3a41f5f4430ac859258a6291e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83e7a4ef31ee4d6790f36df22c2d63c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "857723b3a7bb4c66add7e9337ce0fb35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8594e9dd2cd34c4a8c31f072df490669": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abbad1e8e7414c349d078b94ba06ab68",
      "max": 62127,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7e8bd7fc18ce4fda8c656f813decfade",
      "value": 62127
     }
    },
    "88f9eed6736d47dcac2c451c348b044a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c414892a5aa471dbd4a35dc9145b304": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6158858af342430383786e6cd636fbe3",
       "IPY_MODEL_8594e9dd2cd34c4a8c31f072df490669",
       "IPY_MODEL_48d92342685b4651a5935bc2d2e368b4"
      ],
      "layout": "IPY_MODEL_8d98b199711a46f08bbbb0796cfd2a2c"
     }
    },
    "8c61b2f275164c52abbf489f8d15f5f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d98b199711a46f08bbbb0796cfd2a2c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d9cf65834c54890acf2b7bb8740e94c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e43d373ea134eab904d34a1ce9b4250": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16615c82c9a24594a6e6c2f1a4f6607b",
       "IPY_MODEL_e88e154e6a584d8c9d9479fbd9c5e5d3",
       "IPY_MODEL_e6f0712640e746dfb683989dea1b3e94"
      ],
      "layout": "IPY_MODEL_952a003defdc41ac8bcac503dd6f8276"
     }
    },
    "8f1bf5a49f454b13b89b5bfc24cf6e54": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ffe3f3740b64302b13d81dd43d6556b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e3231cced70478ba374b03aeed1593c",
      "max": 44,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3cdcca994e07414fae64f8d0f0f72e2f",
      "value": 44
     }
    },
    "93772eacec864960baf965dd294c8950": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d475dc6dca8d4ae1aac02ada6d4db047",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_152fdef31ed24c8999a47da9c59f29c0",
      "value": "â€‡44.0/44.0â€‡[00:00&lt;00:00,â€‡2.87kB/s]"
     }
    },
    "952a003defdc41ac8bcac503dd6f8276": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "95bc4a700ad14fc6a6960c7ba0a2d4c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b6e3ba5fdfc24a2c825f5d55705fff62",
       "IPY_MODEL_3a87c9a7ae41446b8b8838ba779bf91d",
       "IPY_MODEL_5fe98b6ebe014d9b938734756e653454"
      ],
      "layout": "IPY_MODEL_e18f852ed61148a789b1d2beef31f3d9"
     }
    },
    "9dcd0da6d499448780f40554d01e7fa5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9dd2263ce2214add88ba9ab67697ede7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e99377214854fe0a36f287b92e00a77": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5e3a27ac4f9f486da839aae8021fd882",
      "max": 313,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dad4dfd22e194e7fb2e392aae936b9d0",
      "value": 313
     }
    },
    "a0520bda70e44cc3b88aebf070c2922b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a17c733e4a9a4f36b333bd516e6079b2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a37c52f3b6bf45e68c439112107c9f34": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a972ff589aa2424b86734ae5b2df8729": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab3af3f0d47b468fb0f56c06f20c2fb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abbad1e8e7414c349d078b94ba06ab68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "acdc71ee6b3543ad8adeaa0e98acbdef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b46f7bdc00d54530b114d0bd17061118": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6e3ba5fdfc24a2c825f5d55705fff62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ff1e2eb9ce6461693bad31e61b13961",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_dfdabe6671904d35b50a396d6f222105",
      "value": "vocab.txt:â€‡100%"
     }
    },
    "bd93adebc80045c19104e9d54dd75e03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be17a3e9f49d4cb4974d844ca645f215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_831f8d7e65734a36b34f868dfa806be0",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_3c1b7a7407fb400c91a40650d530be60",
      "value": "â€‡436M/436Mâ€‡[00:02&lt;00:00,â€‡217MB/s]"
     }
    },
    "c1acf167c2cd42f5aa6f0200bfb57611": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d9cf65834c54890acf2b7bb8740e94c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_037f94547f06499385df8c8b164621f7",
      "value": "â€‡807k/807kâ€‡[00:00&lt;00:00,â€‡3.13MB/s]"
     }
    },
    "c3af7379713f40f08c95a46b731f384b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c3fef1be4ee046fdbae4af568134826a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d7ea464cfd44e42b3e1fefb117b7c26",
       "IPY_MODEL_ff668eb8873745cf8fd398bf96634bab",
       "IPY_MODEL_30c32c353ef143fcaed4a0f4735d8a0c"
      ],
      "layout": "IPY_MODEL_77fb294192f74b379d85d3e03535e04c"
     }
    },
    "c436dcfb3e00413eafb14476b181acf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c43d985d8c84423a9c289a10fc6f57bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "caee605432454911958820c92284189f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cfb5729666104005ac27bc82750535bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b46f7bdc00d54530b114d0bd17061118",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_701211ffa6134025a064ac529786b0bb",
      "value": "â€‡313/313â€‡[00:00&lt;00:00,â€‡1.74kB/s]"
     }
    },
    "cfb8bb86bb404556a51703afeecded59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d007579803b946c086b8e28c991f78a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d475dc6dca8d4ae1aac02ada6d4db047": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6e9e2ac999f44e29c6ade68e880197f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7086884374547acaecc2eef00bf8e2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e6475bb1208e4fd6816e0d8e08384715",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_087dd94f47e34ac58a3472c78d451ddd",
      "value": "config.json:â€‡100%"
     }
    },
    "d85e875ff7264026be65b1bee81e82c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d91d27dced67459c864d8f7e61882cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2e579baccd5d41ff8047169045e0e792",
       "IPY_MODEL_307705e08963408f9829446ac1ca6a51",
       "IPY_MODEL_3dd95a8045704c0699df8d0590e2fe2d"
      ],
      "layout": "IPY_MODEL_7a580bab682140ccb95d14989ee36656"
     }
    },
    "d91fd827015d484cad80a56efda1caba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dad4dfd22e194e7fb2e392aae936b9d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ddf8a8eea4374c00a983e47a04cbc175": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d7086884374547acaecc2eef00bf8e2f",
       "IPY_MODEL_9e99377214854fe0a36f287b92e00a77",
       "IPY_MODEL_cfb5729666104005ac27bc82750535bc"
      ],
      "layout": "IPY_MODEL_1357a8e92e074d39bf7c4c5f56cec350"
     }
    },
    "dfdabe6671904d35b50a396d6f222105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0895dbce7ac4bca8456c97826b1e6f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e18f852ed61148a789b1d2beef31f3d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e39679e6c0a1498da70bee68f8911a73": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6475bb1208e4fd6816e0d8e08384715": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6f0712640e746dfb683989dea1b3e94": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_408cabacdcf14384b7bdbbc674493139",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_16d2fdc3436a4344a936e8493850b220",
      "value": "â€‡62127/62127â€‡[00:30&lt;00:00,â€‡1423.19â€‡examples/s]"
     }
    },
    "e845ba792d1942a49b9073099793b9ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_35fa7eb0c57443d9ac549e63575a303c",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_63fd6f14d7464054b9e9a7373d704e09",
      "value": "pytorch_model.bin:â€‡100%"
     }
    },
    "e88e154e6a584d8c9d9479fbd9c5e5d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16bea5c39ea24cb989e69e7c2d6066c3",
      "max": 62127,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_83b4e3a41f5f4430ac859258a6291e6c",
      "value": 62127
     }
    },
    "ede7604fe1974c22b440419faeeebb76": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ee41e199eb4c4fb0b531ff5204897027": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4774bdc3fe742b4b15aab882aeec53f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7ffdefb657b4a9ea36d05d1b1e8a125": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_09bdde5f48974a0d98e909bf69d045f6",
       "IPY_MODEL_3928bdd3a1e24b7bbbdad923aa63b08d",
       "IPY_MODEL_4851647895334f498e8c54b01892b95d"
      ],
      "layout": "IPY_MODEL_108bfcf9372a4d9ca362529c2781a895"
     }
    },
    "fd975f2d4a3945c99cc296b4b15fad58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fde1feff2d58464ca349ed4ae7b3b321": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ff668eb8873745cf8fd398bf96634bab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88f9eed6736d47dcac2c451c348b044a",
      "max": 804677,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_802201dfcb1b406483ff0ee25dc48077",
      "value": 804677
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
