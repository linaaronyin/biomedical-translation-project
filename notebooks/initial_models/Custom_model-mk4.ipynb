{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1wer6lSMEHx6zEHKJoFn2k2uebLwPDCbu","timestamp":1733539827056},{"file_id":"1KfCiYgiXEsi8u40xzA_V1ADOaLGJxNbW","timestamp":1733525415899},{"file_id":"16a2vyMpbdmZbN7syh-7cKxcQEG8VZ9RR","timestamp":1733470113903}],"gpuType":"L4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5e7bed37dc7840ac9aca04956b102225":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9a587c32242414897997219cdd96a6e","IPY_MODEL_65a6a3e8a0f248ab9ccfd0e9afa04344","IPY_MODEL_c27d26e5af744a949fbd606a62d42bdf"],"layout":"IPY_MODEL_c02941ae182d4fe49bbd1ba25ae95164"}},"c9a587c32242414897997219cdd96a6e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0cf6d71679b9455abe9fcf1c5af9bc14","placeholder":"​","style":"IPY_MODEL_9bd884e34ed24df487ab76fd779f28e3","value":"pytorch_model.bin: 100%"}},"65a6a3e8a0f248ab9ccfd0e9afa04344":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb911dfe12ba4b768c559d23ccd3578c","max":312087009,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0119c1939c141c0a1a2496caf0c45bc","value":312087009}},"c27d26e5af744a949fbd606a62d42bdf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e643631fb8eb41c9815b15741d5757f8","placeholder":"​","style":"IPY_MODEL_0b493e8e06ed41eca08af056051b5e17","value":" 312M/312M [00:02&lt;00:00, 167MB/s]"}},"c02941ae182d4fe49bbd1ba25ae95164":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0cf6d71679b9455abe9fcf1c5af9bc14":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9bd884e34ed24df487ab76fd779f28e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb911dfe12ba4b768c559d23ccd3578c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0119c1939c141c0a1a2496caf0c45bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e643631fb8eb41c9815b15741d5757f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b493e8e06ed41eca08af056051b5e17":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc3a32a931fe4d419305d5664eefe20b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dafad109950a4f8888b1ec7cdc8686ee","IPY_MODEL_655211ca35374bbd9adf321c8b6e2071","IPY_MODEL_ec9d31dfc4ee4a1e8a217f9b9c69cd08"],"layout":"IPY_MODEL_ce718bcc23de43769c357aba68f29364"}},"dafad109950a4f8888b1ec7cdc8686ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_115d8a2ed797422b80887e914603fda1","placeholder":"​","style":"IPY_MODEL_c1bb058f96fd4318947281aa83b0f65e","value":"generation_config.json: 100%"}},"655211ca35374bbd9adf321c8b6e2071":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e91803e09574ccbba94421b7b86f7b5","max":293,"min":0,"orientation":"horizontal","style":"IPY_MODEL_44f584a15d044c218cc3199be074df31","value":293}},"ec9d31dfc4ee4a1e8a217f9b9c69cd08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94df5f20e3ce493481193d82e3027066","placeholder":"​","style":"IPY_MODEL_b5b8806543344f8c94d0b092fa0b0a68","value":" 293/293 [00:00&lt;00:00, 26.7kB/s]"}},"ce718bcc23de43769c357aba68f29364":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"115d8a2ed797422b80887e914603fda1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1bb058f96fd4318947281aa83b0f65e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e91803e09574ccbba94421b7b86f7b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44f584a15d044c218cc3199be074df31":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"94df5f20e3ce493481193d82e3027066":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5b8806543344f8c94d0b092fa0b0a68":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a0911763c5c4d44a0684c92bf0cbe7b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b4cfdbdfa3cc462cb3b6e4a98d652581","IPY_MODEL_41055821770c4fa9a194625b0d87b35e","IPY_MODEL_1a52da55e2d34e0ea0bac79ab5c0d3cc"],"layout":"IPY_MODEL_2b0bb118cbe14882868fac872b9c82d2"}},"b4cfdbdfa3cc462cb3b6e4a98d652581":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95339a3a37314640b9a565835d304881","placeholder":"​","style":"IPY_MODEL_41b3c82016354b179f3c10bfad3f9e50","value":"Generating train split: "}},"41055821770c4fa9a194625b0d87b35e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b2499a010964770b40abcf8f0af0bf9","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_de46f41a18fb43c39c1a829980b59a85","value":1}},"1a52da55e2d34e0ea0bac79ab5c0d3cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_035fd615b1794439b47493edb3d3d42a","placeholder":"​","style":"IPY_MODEL_7f21e3e0410c45978038f69cfdb12a93","value":" 62127/0 [00:00&lt;00:00, 88556.22 examples/s]"}},"2b0bb118cbe14882868fac872b9c82d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95339a3a37314640b9a565835d304881":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"41b3c82016354b179f3c10bfad3f9e50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7b2499a010964770b40abcf8f0af0bf9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"de46f41a18fb43c39c1a829980b59a85":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"035fd615b1794439b47493edb3d3d42a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f21e3e0410c45978038f69cfdb12a93":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c246115026534c1993d74fbae691ab72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5ffe85a0fdd64dafba38461d09eeca5f","IPY_MODEL_0e9f5224d98c47bbaf431a77fd645f7f","IPY_MODEL_30d0cfd3b0124939ae1a584d26b93d62"],"layout":"IPY_MODEL_c6f9e7d9c3a641b9a1672e1628d44720"}},"5ffe85a0fdd64dafba38461d09eeca5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_48fee50e87d049468334d2f1de187b5c","placeholder":"​","style":"IPY_MODEL_ac10a8e8099e428ea200a3d6c8713f34","value":"Map (num_proc=4):   0%"}},"0e9f5224d98c47bbaf431a77fd645f7f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_536b349799bc4d21b2df7c3c002933b5","max":62127,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aab05dc8c15440c4aed7464583a3d6f7","value":0}},"30d0cfd3b0124939ae1a584d26b93d62":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30efa121df6b498a802b932415d65a0a","placeholder":"​","style":"IPY_MODEL_050942c9e83b424eb2e1a6e7a8633aa1","value":" 0/62127 [02:55&lt;?, ? examples/s]"}},"c6f9e7d9c3a641b9a1672e1628d44720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48fee50e87d049468334d2f1de187b5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac10a8e8099e428ea200a3d6c8713f34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"536b349799bc4d21b2df7c3c002933b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aab05dc8c15440c4aed7464583a3d6f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30efa121df6b498a802b932415d65a0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"050942c9e83b424eb2e1a6e7a8633aa1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"87d6a4ce5db146ef969790b340dbe36c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3a796f48a2b444d98969e65b8505fc3","IPY_MODEL_7360715ba23c43cbbba7ef0294307154","IPY_MODEL_0b8de23d10ec480c9b71a55503e94703"],"layout":"IPY_MODEL_749452168d7d421eb487e0e5ec528723"}},"b3a796f48a2b444d98969e65b8505fc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_677d76b949d34ff0a4cc8a34cdd87ec9","placeholder":"​","style":"IPY_MODEL_f5674635e8874ccf8e87b0c58d067875","value":"Map: 100%"}},"7360715ba23c43cbbba7ef0294307154":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c3aab5216cc42918b32cfc95279a1e3","max":62127,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3f64e84934e42cf92c751ea270f2bc6","value":62127}},"0b8de23d10ec480c9b71a55503e94703":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5ee40ba5670454aa774c31895b2d905","placeholder":"​","style":"IPY_MODEL_100c5f964fe24ddcac02e806cc8154b5","value":" 62127/62127 [01:03&lt;00:00, 984.96 examples/s]"}},"749452168d7d421eb487e0e5ec528723":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"677d76b949d34ff0a4cc8a34cdd87ec9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f5674635e8874ccf8e87b0c58d067875":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c3aab5216cc42918b32cfc95279a1e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3f64e84934e42cf92c751ea270f2bc6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5ee40ba5670454aa774c31895b2d905":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"100c5f964fe24ddcac02e806cc8154b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"62bca58f81684dc887173e740707a620":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_da1fcade97c74398bd00a36f42c3f050","IPY_MODEL_d38694d5eb8f485a9753ba175136a0a0","IPY_MODEL_4c7f8f00d25c43d4ae6468c349c2c207"],"layout":"IPY_MODEL_6d29dc02a6b6489aa2df35c1dff1300d"}},"da1fcade97c74398bd00a36f42c3f050":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d7a54983fba41f29d8b8412c7b1d26c","placeholder":"​","style":"IPY_MODEL_1031a3d5d61f43d6bb9df81aceb318aa","value":"Creating parquet from Arrow format: 100%"}},"d38694d5eb8f485a9753ba175136a0a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76cd0a6abd254b0eaeea673ecf59d831","max":63,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09d1dbebcf744bc5903fcb724201e6e0","value":63}},"4c7f8f00d25c43d4ae6468c349c2c207":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9400be8d7134ae69258010035d470ce","placeholder":"​","style":"IPY_MODEL_f2ea8e3de68d46a1accdde0bd18351b8","value":" 63/63 [00:01&lt;00:00, 32.28ba/s]"}},"6d29dc02a6b6489aa2df35c1dff1300d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d7a54983fba41f29d8b8412c7b1d26c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1031a3d5d61f43d6bb9df81aceb318aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76cd0a6abd254b0eaeea673ecf59d831":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09d1dbebcf744bc5903fcb724201e6e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b9400be8d7134ae69258010035d470ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2ea8e3de68d46a1accdde0bd18351b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpfUJY42OHfc","executionInfo":{"status":"ok","timestamp":1733549241778,"user_tz":480,"elapsed":6416,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"028bb2cd-35fa-4679-8d47-987bc353f91c","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting huggingface\n","  Downloading huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting bert_score\n","  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n","Collecting sacrebleu\n","  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m62.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.1+cu121)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n","Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.46.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.6)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.2)\n","Collecting portalocker (from sacrebleu)\n","  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.9.11)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n","Collecting colorama (from sacrebleu)\n","  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.26.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.55.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (11.0.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n","Downloading huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n","Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n","Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n","Building wheels for collected packages: rouge_score\n","  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=c5d298264e30d2e945994de52a1105d72d015f0dba4d82d21c047f388d7745d9\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge_score\n","Installing collected packages: huggingface, portalocker, colorama, sacrebleu, rouge_score, bert_score\n","Successfully installed bert_score-0.3.13 colorama-0.4.6 huggingface-0.0.1 portalocker-3.0.0 rouge_score-0.1.2 sacrebleu-2.4.3\n"]}],"source":["pip install huggingface rouge_score bert_score sacrebleu"]},{"cell_type":"code","source":["pip install datasets transformers evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OPNcQ2SBQYFp","executionInfo":{"status":"ok","timestamp":1733549246557,"user_tz":480,"elapsed":4781,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"d184157a-e337-4ce2-f14b-6103795c07c4","collapsed":true},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2024.10.0\n","    Uninstalling fsspec-2024.10.0:\n","      Successfully uninstalled fsspec-2024.10.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount your Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the data directory inside your Google Drive\n","# data_dir = \"/content/drive/My Drive/Colab Notebooks/corpora\"\n","data_dir = \"/content/drive/My Drive/266 Data Project/corpora\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9zdAKxG1Qwyf","executionInfo":{"status":"ok","timestamp":1733549264223,"user_tz":480,"elapsed":17700,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"61ec5eaa-a596-44fe-b22a-7013045ba5a5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","\n","# Set the environment variable\n","os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n","\n","# Import PyTorch after setting the environment variable\n","import torch\n","\n","# Optionally check if the variable is set correctly\n","print(\"PYTORCH_CUDA_ALLOC_CONF:\", os.environ.get('PYTORCH_CUDA_ALLOC_CONF'))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v_0QMR5TOfY8","executionInfo":{"status":"ok","timestamp":1733549267526,"user_tz":480,"elapsed":3314,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"b70f9fb6-5dc0-474c-e007-c5790b252d81"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n","import pandas as pd\n","from datasets import Dataset, load_dataset\n","import torch.nn.functional as F\n","import os\n","import json\n","os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n","os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\n","\n","def load_marian_with_biomedical_layer(model_name, hidden_size, special_tokens):\n","    # Load tokenizer and add special tokens\n","    tokenizer = MarianTokenizer.from_pretrained(model_name)\n","    tokenizer.add_special_tokens({\n","        'additional_special_tokens': list(set(special_tokens))\n","    })\n","\n","    # Load base model\n","    model = MarianMTModel.from_pretrained(model_name)\n","\n","    # Create custom model, CustomMarianMTModel will create a BiomedicalEncoder object in init()\n","    custom_model = CustomMarianMTModel(\n","        config=model.config,\n","        hidden_size=hidden_size,\n","        special_token_size=len(special_tokens),\n","    )\n","\n","    # Resize token embeddings\n","    custom_model.resize_token_embeddings(len(tokenizer))\n","\n","    return tokenizer, custom_model\n","\n","class BiomedicalEncoder(nn.Module):\n","    def __init__(self, hidden_size, special_token_size):\n","        super(BiomedicalEncoder, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.special_token_size = special_token_size\n","\n","        # Adjust the linear layer to match input dimensions\n","        self.linear = nn.Linear(special_token_size, hidden_size)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, entity_embeddings):\n","        # Reshape entity embeddings if necessary\n","        original_shape = entity_embeddings.shape\n","\n","        # Flatten the tensor if it has more than 2 dimensions\n","        if len(original_shape) > 2:\n","            entity_embeddings = entity_embeddings.view(-1, original_shape[-1])\n","\n","        # Ensure the input matches the expected dimension\n","        if entity_embeddings.size(1) != self.special_token_size:\n","            # If the input doesn't match, pad or truncate\n","            if entity_embeddings.size(1) < self.special_token_size:\n","                # Pad with zeros\n","                padding = torch.zeros(\n","                    entity_embeddings.size(0),\n","                    self.special_token_size - entity_embeddings.size(1),\n","                    device=entity_embeddings.device\n","                )\n","                entity_embeddings = torch.cat([entity_embeddings, padding], dim=1)\n","            else:\n","                # Truncate\n","                entity_embeddings = entity_embeddings[:, :self.special_token_size]\n","\n","        # Apply linear transformation and activation\n","        encoded = self.linear(entity_embeddings)\n","        return self.activation(encoded)\n","\n","class CustomMarianMTModel(MarianMTModel):\n","    def __init__(self, config, hidden_size=512, special_token_size=206573, biomedicalEncoder=None):\n","        super().__init__(config)\n","        self.hidden_size = hidden_size\n","        self.special_token_size = special_token_size\n","\n","        # Initialize biomedical encoder\n","        if biomedicalEncoder is None:\n","            self.biomedical_encoder = BiomedicalEncoder(hidden_size, special_token_size)\n","        else:\n","            self.biomedical_encoder = biomedicalEncoder\n","\n","        # Entity embedding layer\n","        self.entity_embedding = nn.Embedding(special_token_size + 1, hidden_size)  # +1 for padding token\n","\n","    def save_custom(self, save_directory):\n","        # Save the model and its configuration\n","        model_save_path = os.path.join(save_directory, \"model\")\n","        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n","        os.makedirs(model_save_path, exist_ok=True)\n","        os.makedirs(tokenizer_save_path, exist_ok=True)\n","\n","        self.save_pretrained(model_save_path)\n","        torch.save(self.biomedical_encoder.state_dict(), os.path.join(model_save_path, \"biomedical_encoder.pth\"))\n","\n","        # Save custom config in JSON\n","        custom_config = {\n","            \"hidden_size\": self.hidden_size,\n","            \"special_token_size\": self.special_token_size,\n","        }\n","        with open(os.path.join(model_save_path, \"custom_config.json\"), \"w\") as f:\n","            json.dump(custom_config, f)\n","\n","    @classmethod\n","    def from_custom(cls, save_directory):\n","        model_save_path = os.path.join(save_directory, \"model\")\n","        tokenizer_save_path = os.path.join(save_directory, \"tokenizer\")\n","\n","        # Load custom attributes from JSON\n","        custom_config_path = os.path.join(model_save_path, \"custom_config.json\")\n","        with open(custom_config_path, \"r\") as f:\n","            custom_config = json.load(f)\n","\n","        # Load base model and tokenizer\n","        model = MarianMTModel.from_pretrained(model_save_path)\n","        new_model = cls(config=model.config, hidden_size=custom_config[\"hidden_size\"], special_token_size=custom_config[\"special_token_size\"])\n","        biomedical_encoder_path = os.path.join(model_save_path, \"biomedical_encoder.pth\")\n","        biomedical_encoder_state_dict = torch.load(biomedical_encoder_path)\n","        new_model.biomedical_encoder.load_state_dict(biomedical_encoder_state_dict)\n","\n","        # Load the main model weights\n","        state_dict = model.state_dict()\n","        new_model_state_dict = new_model.state_dict()\n","        for key, value in state_dict.items():\n","            if key in new_model_state_dict:\n","                new_model_state_dict[key] = value\n","        new_model.load_state_dict(new_model_state_dict, strict=False)\n","\n","        # Load tokenizer\n","        tokenizer = MarianTokenizer.from_pretrained(tokenizer_save_path)\n","        return new_model, tokenizer\n","\n","    def forward(self, input_ids=None, attention_mask=None, labels=None, entity_ids=None, **kwargs):\n","      # Get token embeddings (from MarianMTModel's embeddings)\n","      token_embeddings = self.model.encoder.embed_tokens(input_ids)\n","\n","      # Manually align entity embeddings with token embeddings\n","      if entity_ids is not None:\n","          entity_embeddings = self.entity_embedding(entity_ids)  # (batch_size, max_entities, hidden_size)\n","          entity_features = self.biomedical_encoder(entity_embeddings)  # (batch_size, max_entities, hidden_size)\n","\n","          # Ensure entity_features match the sequence length of token embeddings\n","          batch_size, seq_len, _ = token_embeddings.size()\n","          if entity_features.size(1) != seq_len:\n","              # Adjust entity features to match sequence length (e.g., by repeating or slicing)\n","              entity_features = entity_features[:, :seq_len, :]  # Truncate to match sequence length\n","\n","          # Add the processed entity embeddings to the token embeddings\n","          token_embeddings += entity_features\n","\n","      # Forward the augmented embeddings to the encoder\n","      encoder_outputs = self.model.encoder(\n","          inputs_embeds=token_embeddings,\n","          attention_mask=attention_mask,\n","          return_dict=True\n","      )\n","\n","      # Pass encoder outputs to the decoder\n","      outputs = self.model.decoder(\n","          input_ids=None,\n","          attention_mask=attention_mask,\n","          encoder_hidden_states=encoder_outputs.last_hidden_state,\n","          encoder_attention_mask=attention_mask,\n","          labels=labels,\n","          return_dict=True\n","      )\n","\n","      return outputs\n","\n","\n","def preprocess_function(examples, tokenizer, max_entities=5, src_lang=\"chinese\", tgt_lang=\"english\"):\n","    # Extract sentences and entities\n","    src_sentences = examples[src_lang]\n","    tgt_sentences = examples[tgt_lang]\n","\n","    # Get entities or assign empty lists if not provided\n","    entities_list = examples.get(\"entities\", [None for _ in src_sentences])  # None if entities are missing\n","\n","    # Tokenize source and target sentences\n","    model_inputs = tokenizer(\n","        src_sentences,\n","        max_length=512,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    )\n","\n","    labels = tokenizer(\n","        tgt_sentences,\n","        max_length=512,\n","        truncation=True,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    )\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    model_inputs[\"labels\"][model_inputs[\"labels\"] == tokenizer.pad_token_id] = -100\n","\n","    # Process the entities for each sentence\n","    processed_entities = []\n","    for entities in entities_list:\n","        if entities is None:\n","            # If no entities, pad with zeros\n","            processed_entities.append([0] * max_entities)\n","        else:\n","            # Instead of aligning the entities manually, just get the entity IDs\n","            entity_ids = []\n","            for entity in entities:\n","                # Convert each entity to its corresponding token ID (special token handling)\n","                entity_token_ids = tokenizer.convert_tokens_to_ids([entity])\n","                entity_ids.extend(entity_token_ids)\n","\n","            # Pad or truncate entity list to match max_entities\n","            if len(entity_ids) < max_entities:\n","                # Pad with zeros if fewer than max_entities\n","                entity_ids += [0] * (max_entities - len(entity_ids))\n","            else:\n","                # Truncate if more than max_entities\n","                entity_ids = entity_ids[:max_entities]\n","\n","            processed_entities.append(entity_ids)\n","\n","    # Add entity IDs to the model inputs\n","    model_inputs[\"entity_ids\"] = torch.tensor(processed_entities, dtype=torch.long)\n","\n","    return model_inputs\n","\n","def prepare_dataset(dataset, tokenizer, max_entities=5, src_lang=\"chinese\", tgt_lang=\"english\"):\n","    # Apply the preprocessing function with batch processing and parallelism\n","    processed_dataset = dataset.map(\n","        lambda examples: preprocess_function(examples, tokenizer, max_entities, src_lang, tgt_lang),\n","        batched=True,\n","        remove_columns=dataset.column_names\n","    )\n","\n","    return processed_dataset\n","\n","\n","\n","def fine_tune_model(model, tokenizer, dataset, output_dir):\n","  # Split dataset\n","    dataset = dataset.train_test_split(test_size=0.1)\n","    training_args = Seq2SeqTrainingArguments(\n","        output_dir=output_dir,\n","        evaluation_strategy=\"epoch\",\n","        learning_rate=5e-5,\n","        per_device_train_batch_size=16,\n","        per_device_eval_batch_size=16,\n","        num_train_epochs=1.5,\n","        weight_decay=0.01,\n","    )\n","\n","    trainer = Seq2SeqTrainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=dataset[\"train\"],\n","        eval_dataset=dataset[\"test\"],\n","        tokenizer=tokenizer,\n","    )\n","\n","    trainer.train()\n","\n","\n"],"metadata":{"id":"NWQmaXceO-HR","executionInfo":{"status":"ok","timestamp":1733549280699,"user_tz":480,"elapsed":13189,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Load pretrained MarianMT model with custom biomedical encoder\n","output_dir = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/custom_models-mk4/\"\n","named_entities_df = pd.read_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/zh-entities.parquet\")\n","special_tokens = [\"\".join(x) for x in named_entities_df[\"tokens\"].tolist()]\n","hidden_size = 512\n","tokenizer, model = load_marian_with_biomedical_layer('Helsinki-NLP/opus-mt-zh-en', hidden_size, special_tokens)\n","\n","# Load dataset\n","dataset = load_dataset(\"parquet\", data_files=\"/content/drive/MyDrive/266 Data Project/corpora/nejm/nejm_train_entities.parquet\")[\"train\"]\n","\n","# Prepare the dataset\n","# prepared_dataset = prepare_dataset(dataset, tokenizer, src_lang=\"chinese\", tgt_lang=\"english\")\n","\n","# prepared_dataset.to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/mk4_tokenized.parquet\")\n","\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# custom_model = custom_model.to(device)\n","# torch.cuda.empty_cache()  # Clear GPU memory before training\n","# # Fine-tune the model\n","# fine_tune_model(model, tokenizer, prepared_dataset)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":798,"referenced_widgets":["5e7bed37dc7840ac9aca04956b102225","c9a587c32242414897997219cdd96a6e","65a6a3e8a0f248ab9ccfd0e9afa04344","c27d26e5af744a949fbd606a62d42bdf","c02941ae182d4fe49bbd1ba25ae95164","0cf6d71679b9455abe9fcf1c5af9bc14","9bd884e34ed24df487ab76fd779f28e3","fb911dfe12ba4b768c559d23ccd3578c","b0119c1939c141c0a1a2496caf0c45bc","e643631fb8eb41c9815b15741d5757f8","0b493e8e06ed41eca08af056051b5e17","bc3a32a931fe4d419305d5664eefe20b","dafad109950a4f8888b1ec7cdc8686ee","655211ca35374bbd9adf321c8b6e2071","ec9d31dfc4ee4a1e8a217f9b9c69cd08","ce718bcc23de43769c357aba68f29364","115d8a2ed797422b80887e914603fda1","c1bb058f96fd4318947281aa83b0f65e","2e91803e09574ccbba94421b7b86f7b5","44f584a15d044c218cc3199be074df31","94df5f20e3ce493481193d82e3027066","b5b8806543344f8c94d0b092fa0b0a68","6a0911763c5c4d44a0684c92bf0cbe7b","b4cfdbdfa3cc462cb3b6e4a98d652581","41055821770c4fa9a194625b0d87b35e","1a52da55e2d34e0ea0bac79ab5c0d3cc","2b0bb118cbe14882868fac872b9c82d2","95339a3a37314640b9a565835d304881","41b3c82016354b179f3c10bfad3f9e50","7b2499a010964770b40abcf8f0af0bf9","de46f41a18fb43c39c1a829980b59a85","035fd615b1794439b47493edb3d3d42a","7f21e3e0410c45978038f69cfdb12a93","c246115026534c1993d74fbae691ab72","5ffe85a0fdd64dafba38461d09eeca5f","0e9f5224d98c47bbaf431a77fd645f7f","30d0cfd3b0124939ae1a584d26b93d62","c6f9e7d9c3a641b9a1672e1628d44720","48fee50e87d049468334d2f1de187b5c","ac10a8e8099e428ea200a3d6c8713f34","536b349799bc4d21b2df7c3c002933b5","aab05dc8c15440c4aed7464583a3d6f7","30efa121df6b498a802b932415d65a0a","050942c9e83b424eb2e1a6e7a8633aa1"]},"id":"s-GFiJ-VNRii","executionInfo":{"status":"error","timestamp":1733548434046,"user_tz":480,"elapsed":966171,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"0084b53b-9c59-4d95-b011-832ad976934d"},"execution_count":6,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n","  warnings.warn(\"Recommended: pip install sacremoses.\")\n"]},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e7bed37dc7840ac9aca04956b102225"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3a32a931fe4d419305d5664eefe20b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"]},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a0911763c5c4d44a0684c92bf0cbe7b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Parameter 'function'=<function prepare_dataset.<locals>.preprocess_function at 0x7f51bb9e17e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","WARNING:datasets.fingerprint:Parameter 'function'=<function prepare_dataset.<locals>.preprocess_function at 0x7f51bb9e17e0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"]},{"output_type":"display_data","data":{"text/plain":["Map (num_proc=4):   0%|          | 0/62127 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c246115026534c1993d74fbae691ab72"}},"metadata":{}},{"output_type":"error","ename":"TimeoutError","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<string>\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/managers.py\u001b[0m in \u001b[0;36m_callmethod\u001b[0;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethodname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-aa243c21a2ac>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Prepare the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprepared_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"chinese\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_lang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprepared_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/266 Data Project/corpora/nejm/mk4_tokenized.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-bc080df99ebe>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(dataset, tokenizer, src_lang, tgt_lang, max_entities)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;31m# Apply the preprocessing function with batch processing and parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     processed_dataset = dataset.map(\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0mpreprocess_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m         }\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3145\u001b[0m                         \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mf\" (num_proc={num_proc})\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m                     ) as pbar:\n\u001b[0;32m-> 3147\u001b[0;31m                         for rank, done, content in iflatmap_unordered(\n\u001b[0m\u001b[1;32m   3148\u001b[0m                             \u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_iterable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs_per_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m                         ):\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    716\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpool_changed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;31m# we get the result in case there's an error to raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0masync_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0masync_result\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_results\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTimeoutError\u001b[0m: "]}]},{"cell_type":"code","source":["model_save_path = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training-mk4\"\n","# Loading custom model\n","custom_model.save_custom(model_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W2QFcAvBlR2M","executionInfo":{"status":"ok","timestamp":1733549217418,"user_tz":480,"elapsed":7415,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"748e22d9-dfc7-4f85-a52c-5bb81c082b00"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[65000]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["model_save_path = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training-mk4\"\n","custom_model, tokenizer = CustomMarianMTModel.from_custom(model_save_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":438},"id":"b2Ojr8yqlvXz","executionInfo":{"status":"error","timestamp":1733549330984,"user_tz":480,"elapsed":18516,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"d29bc5cc-ebb8-4d26-d046-719d491c29f1"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-bd3b275344b2>:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  biomedical_encoder_state_dict = torch.load(biomedical_encoder_path)\n"]},{"output_type":"error","ename":"OSError","evalue":"Can't load tokenizer for '/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training-mk4/tokenizer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training-mk4/tokenizer' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-570ff48950b2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training-mk4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcustom_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomMarianMTModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_custom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-bd3b275344b2>\u001b[0m in \u001b[0;36mfrom_custom\u001b[0;34m(cls, save_directory)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;31m# Load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMarianTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;31m# loaded directly from the GGUF file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgguf_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m             raise EnvironmentError(\n\u001b[0m\u001b[1;32m   2198\u001b[0m                 \u001b[0;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m                 \u001b[0;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for '/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training-mk4/tokenizer'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/drive/MyDrive/266 Data Project/corpora/nejm/before-training-mk4/tokenizer' is the correct path to a directory containing all relevant files for a MarianTokenizer tokenizer."]}]},{"cell_type":"code","source":["# prepared_dataset = prepare_dataset(dataset, tokenizer, src_lang=\"chinese\", tgt_lang=\"english\")\n","\n","# prepared_dataset.to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/mk4_tokenized.parquet\")\n","\n","path = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/mk4_tokenized.parquet\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","custom_model = model\n","custom_model = custom_model.to(device)\n","torch.cuda.empty_cache()  # Clear GPU memory before training\n","# Fine-tune the model\n","# fine_tune_model(model, tokenizer, prepared_dataset, output_dir)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449,"referenced_widgets":["87d6a4ce5db146ef969790b340dbe36c","b3a796f48a2b444d98969e65b8505fc3","7360715ba23c43cbbba7ef0294307154","0b8de23d10ec480c9b71a55503e94703","749452168d7d421eb487e0e5ec528723","677d76b949d34ff0a4cc8a34cdd87ec9","f5674635e8874ccf8e87b0c58d067875","3c3aab5216cc42918b32cfc95279a1e3","e3f64e84934e42cf92c751ea270f2bc6","d5ee40ba5670454aa774c31895b2d905","100c5f964fe24ddcac02e806cc8154b5","62bca58f81684dc887173e740707a620","da1fcade97c74398bd00a36f42c3f050","d38694d5eb8f485a9753ba175136a0a0","4c7f8f00d25c43d4ae6468c349c2c207","6d29dc02a6b6489aa2df35c1dff1300d","2d7a54983fba41f29d8b8412c7b1d26c","1031a3d5d61f43d6bb9df81aceb318aa","76cd0a6abd254b0eaeea673ecf59d831","09d1dbebcf744bc5903fcb724201e6e0","b9400be8d7134ae69258010035d470ce","f2ea8e3de68d46a1accdde0bd18351b8"]},"id":"p_dlfJKdiiYF","executionInfo":{"status":"error","timestamp":1733548868301,"user_tz":480,"elapsed":70488,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"bbf99b44-3b4c-408e-9c7f-5de0126384f1"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/62127 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87d6a4ce5db146ef969790b340dbe36c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Creating parquet from Arrow format:   0%|          | 0/63 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62bca58f81684dc887173e740707a620"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n"]},{"output_type":"error","ename":"KeyError","evalue":"\"Column train not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels', 'entity_ids']\"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-5497be52280d>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Clear GPU memory before training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Fine-tune the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepared_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-57d65f0edbdb>\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model, tokenizer, dataset, output_dir)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m         \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: F811\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2761\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2762\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2744\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2746\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2747\u001b[0m         formatted_output = format_table(\n\u001b[1;32m   2748\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0m_raise_bad_key_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0m_check_valid_column_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_check_valid_column_key\u001b[0;34m(key, columns)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_valid_column_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column {key} not in the dataset. Current columns in the dataset: {columns}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"Column train not in the dataset. Current columns in the dataset: ['input_ids', 'attention_mask', 'labels', 'entity_ids']\""]}]},{"cell_type":"code","source":["fine_tune_model(model, tokenizer, prepared_dataset, output_dir)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":385},"id":"6YFT936KkXQ6","executionInfo":{"status":"error","timestamp":1733549115137,"user_tz":480,"elapsed":757,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}},"outputId":"5cf49e70-8a25-43dd-bf99-8d2612590d5f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-20-bd3b275344b2>:253: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (16) must match the size of tensor b (80) at non-singleton dimension 0","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-232c5a25b48b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfine_tune_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprepared_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-20-bd3b275344b2>\u001b[0m in \u001b[0;36mfine_tune_model\u001b[0;34m(model, tokenizer, dataset, output_dir)\u001b[0m\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2479\u001b[0m                     )\n\u001b[1;32m   2480\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m                     if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3578\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3579\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3581\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3634\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-bc080df99ebe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, labels, entity_ids, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# We just append entity embeddings to token embeddings directly, no manual expansion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mentity_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Adding entity features to token embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# Forward the augmented embeddings to the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (80) at non-singleton dimension 0"]}]},{"cell_type":"code","source":["# Saving custom model\n","model_save_path = \"/content/drive/MyDrive/266 Data Project/corpora/nejm/after-training-mk4\"\n","custom_model.save_custom(model_save_path)\n"],"metadata":{"id":"wQx4-HR_RYTP","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":16,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from evaluate import load\n","import torch\n","\n","def translate_tokenized_dataset(model, tokenizer, tokenized_dataset, batch_size=32):\n","    translations = []\n","\n","    model.eval()\n","\n","    for i in range(0, len(tokenized_dataset), batch_size):\n","        # Extract batch data\n","        input_ids = tokenized_dataset[\"input_ids\"][i:i + batch_size]\n","        attention_mask = tokenized_dataset[\"attention_mask\"][i:i + batch_size]\n","        entity_ids = tokenized_dataset[\"entity_ids\"][i:i + batch_size]\n","\n","        # Convert to tensors with explicit type and device handling\n","        input_ids = torch.tensor(input_ids, dtype=torch.long).to(model.device)\n","        attention_mask = torch.tensor(attention_mask, dtype=torch.long).to(model.device)\n","        entity_ids = torch.tensor(entity_ids, dtype=torch.long).to(model.device)\n","\n","        # Debug print statements\n","        print(f\"Batch {i//batch_size + 1}:\")\n","        print(f\"Input IDs shape: {input_ids.shape}\")\n","        print(f\"Attention Mask shape: {attention_mask.shape}\")\n","        print(f\"Entity IDs shape: {entity_ids.shape}\")\n","        print(f\"Entity IDs min: {entity_ids.min()}, max: {entity_ids.max()}\")\n","        print(f\"Model entity embedding size: {model.entity_embedding.num_embeddings}\")\n","\n","        # Validate entity_ids before generation\n","        try:\n","            # Check if all entity IDs are within the valid range\n","            assert torch.all(entity_ids >= 0), \"Negative entity IDs found\"\n","            assert torch.all(entity_ids < model.entity_embedding.num_embeddings), \"Out-of-bound entity IDs\"\n","        except AssertionError as e:\n","            print(f\"Entity ID validation error: {e}\")\n","            # Skip this batch or handle the error as needed\n","            continue\n","\n","        # Generate translations\n","        try:\n","            with torch.no_grad():\n","                outputs = model.generate(\n","                    input_ids=input_ids,\n","                    attention_mask=attention_mask,\n","                    entity_ids=entity_ids,\n","                    max_length=50,  # Increase this to the desired total length\n","                    num_beams=5,  # Optional: For beam search\n","                    do_sample=False  # Optional: To control randomness\n","                )\n","\n","        except Exception as e:\n","            print(f\"Generation error in batch {i//batch_size + 1}: {e}\")\n","            continue\n","\n","        # Decode translations\n","        translated_batch = [tokenizer.decode(t, skip_special_tokens=True) for t in outputs]\n","        translations.extend(translated_batch)\n","\n","    return translations\n","\n","\n","# Define evaluation metrics\n","def evaluate_model_metrics(predictions, references, save_path=None):\n","    # Load the evaluation metrics\n","    bleu_metric = load(\"bleu\")\n","    rouge_metric = load(\"rouge\")\n","    bertscore_metric = load(\"bertscore\")\n","    ter_metric = load(\"ter\")\n","\n","    # Format references for metric calculation\n","    references = [[ref] for ref in references]\n","\n","    # Evaluate BLEU score\n","    bleu_result = bleu_metric.compute(predictions=predictions, references=references)\n","\n","    # Evaluate ROUGE score\n","    rouge_result = rouge_metric.compute(predictions=predictions, references=references)\n","\n","    # Evaluate BERTScore\n","    bertscore_result = bertscore_metric.compute(predictions=predictions, references=references, lang=\"en\")\n","\n","    # Evaluate TER (Translation Edit Rate)\n","    ter_result = ter_metric.compute(predictions=predictions, references=references)\n","\n","    # Extract summary statistics for BERTScore\n","    bertscore_summary = {\n","        \"mean\": sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]),\n","        \"median\": sorted(bertscore_result[\"f1\"])[len(bertscore_result[\"f1\"]) // 2],\n","        \"std\": (sum((x - sum(bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]))**2 for x in bertscore_result[\"f1\"]) / len(bertscore_result[\"f1\"]))**0.5\n","    }\n","\n","    # Consolidate results\n","    results = {\n","        \"BLEU\": bleu_result,\n","        \"ROUGE\": rouge_result,\n","        \"BERTScore\": bertscore_summary,\n","        \"TER\": ter_result,\n","    }\n","\n","    return results"],"metadata":{"id":"5fS1Yk4GpnT_","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":15,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(examples, tokenizer, src_lang=\"chinese\", max_entities=5):\n","    src_sentences = examples[src_lang]\n","    entities_list = examples.get(\"entities\", [[] for _ in src_sentences])\n","\n","    processed_src = []\n","    processed_entities = []\n","\n","    for sentence, entities in zip(src_sentences, entities_list):\n","        processed_src.append(sentence)\n","\n","        # Convert entities to token IDs (if in vocab)\n","        entity_ids = [\n","            tokenizer.convert_tokens_to_ids(entity)\n","            if entity in tokenizer.get_vocab() else 0\n","            for entity in entities\n","        ]\n","\n","        # Pad or truncate entity_ids\n","        entity_ids = entity_ids[:max_entities]\n","        entity_ids += [0] * (max_entities - len(entity_ids))  # Pad with zeros\n","        processed_entities.append(entity_ids)\n","\n","    # Tokenize the processed source sentences\n","    model_inputs = tokenizer(\n","        processed_src,\n","        max_length=512,\n","        truncation=True,\n","        padding=True,\n","        return_tensors=\"pt\"\n","    )\n","\n","    # Add entity_ids as a tensor to the inputs\n","    model_inputs[\"entity_ids\"] = torch.tensor(processed_entities, dtype=torch.long)\n","\n","    return model_inputs\n","\n","def preprocess_test_data(test_dataset, tokenizer, src_lang=\"chinese\", max_entities=5):\n","    \"\"\"\n","    Preprocess test data to tokenize inputs and add entity_ids for entity-based embeddings.\n","    \"\"\"\n","    # Wrap preprocess_function with fixed arguments\n","    def wrapped_preprocess_function(examples):\n","        return preprocess_function(\n","            examples, tokenizer=tokenizer, src_lang=src_lang, max_entities=max_entities\n","        )\n","\n","    # Apply the preprocessing function to the test dataset\n","    processed_test_dataset = test_dataset.map(\n","        wrapped_preprocess_function,\n","        batched=True,\n","        remove_columns=test_dataset.column_names,\n","    )\n","\n","    return processed_test_dataset\n","\n","def evaluate_model(model, tokenized_test_dataset, tokenizer, batch_size=16, max_length=50, num_beams=5, do_sample=False):\n","    \"\"\"\n","    Evaluate the model on the tokenized test dataset.\n","\n","    Args:\n","        model: The model to evaluate.\n","        tokenized_test_dataset: Tokenized test dataset.\n","        tokenizer: The tokenizer used for tokenization.\n","        batch_size: Batch size for evaluation.\n","        max_length: The maximum total sequence length (input + output).\n","        num_beams: Number of beams for beam search.\n","        do_sample: Whether to sample during generation.\n","\n","    Returns:\n","        List of predictions.\n","    \"\"\"\n","    # Prepare DataLoader for test data\n","    test_loader = torch.utils.data.DataLoader(\n","        tokenized_test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        collate_fn=lambda batch: tokenizer.pad(batch, return_tensors=\"pt\")\n","    )\n","\n","    model.eval()\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            # Move inputs to GPU if available\n","            inputs = {key: val.to(model.device) for key, val in batch.items() if key != \"labels\"}\n","\n","            # Generate predictions with specified parameters\n","            outputs = model.generate(\n","                **inputs,\n","                max_length=max_length,\n","                num_beams=num_beams,\n","                do_sample=do_sample\n","            )\n","            predictions.extend(outputs)\n","\n","    return predictions"],"metadata":{"id":"mDdTQpxEpyuj","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":15,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load your test dataset\n","test_dataset = load_dataset(\"parquet\", data_files={\"test\": \"/content/drive/MyDrive/266 Data Project/corpora/nejm/nejm_test_entities.parquet\"})[\"test\"]\n","\n","# Preprocess and tokenize test data\n","tokenized_test_dataset = preprocess_test_data(test_dataset, tokenizer)\n","\n","# # Evaluate model on test data\n","# predictions = evaluate_model(custom_model, tokenized_test_dataset, tokenizer)\n","# predictions[0]"],"metadata":{"id":"BLXoJhm3p1Qb","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":15,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evaluate model on test data\n","predictions = evaluate_model(custom_model, tokenized_test_dataset, tokenizer)\n","predictions[0]"],"metadata":{"id":"1lLGN2Lessqw","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":15,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["temp = tokenizer.convert_ids_to_tokens(predictions[0])\n","temp"],"metadata":{"id":"DBP8JLTfu1Wr","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":15,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(data={\"predicted_english_tokens\": [x.cpu().numpy() for x in predictions]}).to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/mk4_post_training_predictions.parquet\")"],"metadata":{"id":"p1IrWQLhvEc_","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":15,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Decode predictions to text\n","decoded_predictions = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predictions]\n","print(\"Predictions:\", decoded_predictions[0:5])"],"metadata":{"id":"gMynXYNnvZ7b","executionInfo":{"status":"aborted","timestamp":1733548868312,"user_tz":480,"elapsed":15,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pd.DataFrame(data={\"predicted_english\": decoded_predictions}).to_parquet(\"/content/drive/MyDrive/266 Data Project/corpora/nejm/mk4_post_training_predictions_detokenized.parquet\")"],"metadata":{"id":"RXzSKR8owB_W","executionInfo":{"status":"aborted","timestamp":1733548868313,"user_tz":480,"elapsed":16,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = evaluate_model_metrics(decoded_predictions, test_dataset[\"english\"])\n","results"],"metadata":{"id":"V5XO_d4s5xb6","executionInfo":{"status":"aborted","timestamp":1733548868313,"user_tz":480,"elapsed":64245,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JO5YQfR2L7Jd","executionInfo":{"status":"aborted","timestamp":1733548868313,"user_tz":480,"elapsed":64097,"user":{"displayName":"Aaron Lin","userId":"11593436174117725897"}}},"execution_count":null,"outputs":[]}]}